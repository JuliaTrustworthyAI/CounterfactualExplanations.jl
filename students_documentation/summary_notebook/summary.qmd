# Added Features to CounterfactualExplanations.jl

In this notebook, we document the new features that we have added to the CounterfactualExplanations.jl package. These features include adding new counterfactual generators into the package, increasing the scope of predictive models that are compatible with the package, adding unit testing and documentation, and improving the package infrastructure. We hope that this notebook will serve as a useful reference for both ourselves and the course staff.

# Setup

```{julia}
using Pkg
Pkg.activate("students_documentation/summary_notebook")
    
# Dependencies
using Revise
using CounterfactualExplanations
using Images
using MLDatasets
using Tidier
using PythonCall
using Random
```

## Must haves

### Added Datasets

#### Statlog German credit dataset

Added Statlog German credit as a benchmark dataset.

```{julia}
german_credit_data = CounterfactualExplanations.Data.load_german_credit()
german_credit_data_x = german_credit_data.X
german_credit_data_y = german_credit_data.output_encoder.y
```

##### Post Hoc benchmarking with Statlog German credit dataset

```{julia}
# Train a model
M = fit_model(german_credit_data, :Linear)

# Factual and target labels:
target = 1
factual = 0
```

```{julia}
n_individuals = 5

# Choose random factual individuals
ids = rand(findall(predict_label(M, german_credit_data) .== factual), n_individuals)
xs = select_factual(german_credit_data, ids)

generator = GenericGenerator()

generated_counterfactuals_german_credit_data = generate_counterfactual(
    xs,
    target,
    german_credit_data,
    M, 
    generator; 
    num_counterfactuals=1
)
```

```{julia}
benchmark_german_credit_data = CounterfactualExplanations.Evaluation.benchmark(generated_counterfactuals_german_credit_data)

@chain benchmark_german_credit_data() begin
    @filter(variable == "distance")
    @select(sample, variable, value)
end
```

```{julia}
factual_datapoint = first(xs)[1]
counterfactual_datapoint = generated_counterfactuals_german_credit_data[1].sâ€²

df = DataFrame(Vector1 = vec(factual_datapoint), Vector2 = vec(counterfactual_datapoint))
```

### Added models

#### Interface to Python models

The interface supports generating counterfactuals for any neural network that has been previously defined and trained using PyTorch, regardless of the specific architectural details of the model.

An important detail to note is that generating counterfactuals for Python models is only supported for Julia versions 1.8 and above. For an in-depth discussion of why this is the case, refer to `students_documentation\PyTorch_and_R_models_report.md`.

##### Load the data

```{julia}
# Load data
Random.seed!(42);
N = 100

counterfactual_data = CounterfactualExplanations.Data.load_moons(N)
```

##### Load the saved PyTorch model

Since the package provides support for PyTorch models predefined by the user rather than for training PyTorch models inside the package as was requested by the client, the user has to provide a path to the model definition and a path to the model weights as input. This can be done through the following method:

```{julia}
model_loaded = CounterfactualExplanations.Models.pytorch_model_loader(
    "$(pwd())/students_documentation/summary_notebook/miscellaneous",
    "neural_network_class",
    "NeuralNetwork",
    "$(pwd())/students_documentation/summary_notebook/miscellaneous/pretrained_model.pt"
)

model_pytorch = CounterfactualExplanations.Models.PyTorchModel(model_loaded, :classification_multi)
```

The four arguments of `pytorch_model_loader()` are the following:
1. `model_path`: the path to the folder with a `.py` file where the PyTorch model is defined
2. `model_file`: the name of the `.py` file where the PyTorch model is defined
3. `class_name`: The name of the class of the PyTorch model
4. `pickle_path`: The path to the Pickle file that holds the model weights

After importing the model in this way, counterfactuals can be created for a PyTorch model the same way as for any other model.

##### Select a factual and generate counterfactuals for it like with any other model

```{julia}
Random.seed!(42)

target = 0
factual = 1

@info "Choose factual's index"
y_chosen_factual_idx = rand(findall(CounterfactualExplanations.Models.predict_label(model_pytorch, counterfactual_data) .== factual))

@info "Select factual"
x_random_factual = select_factual(counterfactual_data, y_chosen_factual_idx)
```

```{julia}
# Define a generator:
generator = CounterfactualExplanations.Generators.GenericGenerator()

@info "Generate counterfactual_pytorch"
counterfactual_pytorch = CounterfactualExplanations.generate_counterfactual(
    x_random_factual, 
    target, 
    counterfactual_data, 
    model_pytorch, 
    generator;
    max_iter=1000
)

CounterfactualExplanations.plot(counterfactual_pytorch; title = "model_pytorch path")
```

#### Interface to R models

The interface supports generating counterfactuals for any neural network that has been previously defined and trained using torch library in R, regardless of the specific architectural details of the model.

An important detail to note is that generating counterfactuals for R torch models is the only untested feature of the package. For an in-depth discussion of why this is the case, refer to `students_documentation\PyTorch_and_R_models_report.md`.

Please note that executing the code provided below requires the presence of both the R language and the torch library on the machine where it is being run.

##### Load the data

```{julia}
# Load data
Random.seed!(42);
N = 100

counterfactual_data = CounterfactualExplanations.Data.load_moons(N)
```

##### Load the saved R model

Since the package provides support for R models predefined by the user rather than for training R models inside the package as was requested by the client, the user has to provide a path to the pickle file with the trained model. This can be done through the following method:

```{julia}
model_loaded = CounterfactualExplanations.Models.rtorch_model_loader("$(pwd())/students_documentation/summary_notebook/miscellaneous/r_model.pt")

model_R = CounterfactualExplanations.Models.RTorchModel(model_loaded, :classification_multi)
```

The only argument of `rtorch_model_loader()` is the following:
1. `pickle_path`: The path to the Pickle file that holds the trained model

After importing the model in this way, counterfactuals can be created for an RTorch model the same way as for any other model.

##### Select a factual and generate counterfactuals for it like with any other model

```{julia}
Random.seed!(42)

target = 0
factual = 1

@info "Choose factual's index"
y_chosen_factual_idx = rand(findall(CounterfactualExplanations.Models.predict_label(model_R, counterfactual_data) .== factual))

@info "Select factual"
x_random_factual = select_factual(counterfactual_data, y_chosen_factual_idx)
```

```{julia}
# Define a generator:
generator = CounterfactualExplanations.Generators.GenericGenerator()

@info "Generate counterfactual_R"
counterfactual_R = CounterfactualExplanations.generate_counterfactual(
    x_random_factual, 
    target, 
    counterfactual_data, 
    model_R, 
    generator;
    max_iter=1000
)

CounterfactualExplanations.plot(counterfactual_R; title = "model_R path")
```

#### MLJ models

We have integrated three models from the [MLJ model registry](https://alan-turing-institute.github.io/MLJ.jl/dev/list_of_supported_models/) into our package. The list of supported MLJ models can be accessed as follows:

```{julia}
mlj_models_catalogue
```

##### The `DecisionTreeClassifier` and `RandomForestClassifier` models

Added a basic interface that allows the user to load and use the `DecisionTreeClassifier` and the `RandomForestClassifier` from the MLJ package.

```{julia}
n = 500
counterfactual_data = CounterfactualExplanations.Data.load_moons(n)
```

The decision tree can be fitted as follows:

```{julia}
decision_tree = CounterfactualExplanations.Models.fit_model(counterfactual_data, :DecisionTree)
```

The random forest model can be fitted as follows:

```{julia}
forest = CounterfactualExplanations.Models.fit_model(counterfactual_data, :RandomForest)
```

However, it's also possible to tune the DecisionTreeClassifier's parameters. This can be done using the keyword arguments when calling `fit_model()` as follows:

```{julia}
decision_tree = CounterfactualExplanations.Models.fit_model(counterfactual_data, :DecisionTree; max_depth=2, min_samples_leaf=3)
```

For all supported MLJ models, every tunable parameter they have is supported as a keyword argument. The tunable parameters for the `DecisionTreeModel` and the `RandomForestModel` can be found from the [documentation of the `DecisionTree.jl` package](https://docs.juliahub.com/DecisionTree/pEDeB/0.10.11/) under the Decision Tree Classifier and Random Forest Classifier sections.

Counterfactuals can be generated for this model using the Feature Tweak generator. Note that other generators are not applicable to these two models: both decision trees and random forests are non-differentiable tree-based models and thus, gradient-based generators don't apply for them.

##### The `EvoTreeClassifier` model

Added a basic interface that allows the user to load and use the `EvoTreeClassifier` from the MLJ package.

```{julia}
n = 500
counterfactual_data = CounterfactualExplanations.Data.load_moons(n)
M = CounterfactualExplanations.Models.fit_model(counterfactual_data, :EvoTree)
```

The model parameters can be defined using keyword arguments. For example, we can define another EvoTreeClassifier in the following way:

```{julia}
evotree = CounterfactualExplanations.Models.fit_model(counterfactual_data, :EvoTree; lambda=0.1, max_depth=3)
```

The tunable parameters for the `EvoTreeModel` can be found from the [documentation of the `EvoTrees.jl` package](https://evovest.github.io/EvoTrees.jl/stable/) under the EvoTreeClassifier section.

Now, the EvoTree can be used as part of the package like any other model:

```{julia}
x = float32.([1, -0.5])
CounterfactualExplanations.Models.probs(M, x)
```

However, please note that creating counterfactuals for this model is not supported yet.


### Added generators

#### Feature Tweak

This section describes the Feature Tweak generator, introduced in the paper ["Interpretable Predictions of Tree-based Ensembles via Actionable Feature Tweaking"](https://arxiv.org/pdf/1706.06691.pdf) by Tolomei et al. This is an important addition to the package, as all previously implemented generators were only applicable to differentiable models. In contrast, this generator is specifically tailored for generating counterfactuals for non-differentiable tree-based models such as decision trees and random forests and thus expands the scope of models for which counterfactual explanations can be generated using the package.

This specialization to tree-based models comes at a cost: the Feature Tweak generator is incompatible with any model that does not have a tree structure, thus being incompatible with many models in the package. However, we believe that it's worth having such specialized generators in the package when that allows us to expand the scope of supported models, as Feature Tweak does.

The first step when generating counterfactuals using Feature Tweak is importing a dataset and fitting a tree-based model to that dataset:

```{julia}
n = 500
counterfactual_data = CounterfactualExplanations.Data.load_moons(n)
decision_tree = CounterfactualExplanations.Models.fit_model(counterfactual_data, :DecisionTree)
```

An important thing to note is that the Feature Tweak generator can only generate counterfactuals for models trained on binary classification datasets. This limitation was also noted in the paper by Tolomei et al. and follow-up research might make it feasible to expand the method to multi-class classification problems.

The next step is to initialize the Feature Tweak generator:

```{julia}
generator = FeatureTweakGenerator(Ïµ=0.1)
```

Now, let's take a point that the model believes is in class 1:

```{julia}
x = float32.([1, -0.5])
Models.predict_label(decision_tree, x)
```

Let's choose 2 as the target class. We can now generate the counterfactual as usual:

```{julia}
target = 2
ce = generate_counterfactual(x, target, counterfactual_data, decision_tree, generator)
```

Let's verify that the counterfactual is indeed in the target class:

```{julia}
Models.predict_label(decision_tree, ce.sâ€²)
```

Finally, we can visualize the generated counterfactual and the decision boundaries of the decision tree:

```{julia}
X = counterfactual_data.X
y = counterfactual_data.y
plt = plot()
scatter!(counterfactual_data)

scatter!(plt, [x[1]], [x[2]], color="red", markersize=10, label="x_old")
scatter!(plt, [ce.x[1]], [ce.x[2]], color="dark blue", markersize=10, label="x_new")

hline!(plt, [-0.007127], linestyle=:dash, linecolor=:black, label=nothing)
vline!(plt, [-0.5219], linestyle=:dash, linecolor=:black, label=nothing)
vline!(plt, [1.475], linestyle=:dash, linecolor=:black, label=nothing)
```

The visualization will be improved as part of a separate pull request.


### <feature_name>

<feature_description>

```{julia}
print("Hello world!")
```

## Should haves

### Added Datasets

#### CIFAR10 dataset

Added CIFAR10 from MLDatasets.jl as a benchmark dataset.

```{julia}
cifar_10_data = CounterfactualExplanations.Data.load_cifar_10()
cifar_10_x = cifar_10_data.X
cifar_10_y = cifar_10_data.output_encoder.y
```

```{julia}
images = []

for i in 0:9
    ys = findall(cifar_10_y .== i)
    for j in 0:9
        x = cifar_10_x[:,rand(ys)]

        x = clamp.((x .+ 1.0) ./ 2.0, 0.0, 1.0) |>
            x -> reshape(x, 32, 32, 3) |>
            x -> convert2image(CIFAR10, x)
        push!(images, x)
    end
end

mosaic(images..., ncol=10) |> display
```

#### UCI Adult dataset

Added the UCI adult dataset as a benchmark dataset.

```{julia}
adult_data = CounterfactualExplanations.Data.load_uci_adult()
adult_data_x = adult_data.X
adult_data_y = adult_data.output_encoder.y
```

```{julia}
# Train a model
M = fit_model(adult_data, :Linear)

# Factual and target labels:
target = 1
factual = 0
```

```{julia}
n = 10

# Choose random factual datapoints
ids = rand(findall(predict_label(M, adult_data) .== factual), n)
xs = select_factual(adult_data, ids)

generator = GenericGenerator()

adult_counterfactual_data = generate_counterfactual(
    xs,
    target,
    adult_data,
    M, 
    generator; 
    num_counterfactuals=1
)
```

### <feature_name>

<feature_description>

```{julia}
print("Hello world!")
```

## Could haves

### <feature_name>

<feature_description>

```{julia}
print("Hello world!")
```
