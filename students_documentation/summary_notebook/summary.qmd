# Added Features to CounterfactualExplanations.jl

In this notebook, we document the new features that we have added to the CounterfactualExplanations.jl package. These features include adding new counterfactual generators into the package, increasing the scope of predictive models that are compatible with the package, adding unit testing and documentation, and improving the package infrastructure. We hope that this notebook will serve as a useful reference for both ourselves and the course staff.

## Setup

```{julia}
using Pkg
Pkg.activate("students_documentation/summary_notebook")
    
# Dependencies
using Revise
using CounterfactualExplanations
using Images
using MLDatasets
using Tidier
using PythonCall
using Random
using CounterfactualExplanations.Data
using CounterfactualExplanations.Models
using CounterfactualExplanations.Generators
```

## Added models and generators

### Interface to Python models

The interface supports generating counterfactuals for any neural network that has been previously defined and trained using PyTorch, regardless of the specific architectural details of the model.

An important detail to note is that generating counterfactuals for Python models is only supported for Julia versions 1.8 and above. For an in-depth discussion of why this is the case, refer to `students_documentation\PyTorch_and_R_models_report.md`.

#### Load the data

```{julia}
# Load data
Random.seed!(42);
N = 100

counterfactual_data = CounterfactualExplanations.Data.load_moons(N)
```

#### Load the saved PyTorch model

Since the package provides support for PyTorch models predefined by the user rather than for training PyTorch models inside the package as was requested by the client, the user has to provide a path to the model definition and a path to the model weights as input. This can be done through the following method:

```{julia}
model_loaded = CounterfactualExplanations.Models.pytorch_model_loader(
    "$(pwd())/students_documentation/summary_notebook/miscellaneous",
    "neural_network_class",
    "NeuralNetwork",
    "$(pwd())/students_documentation/summary_notebook/miscellaneous/pretrained_model.pt"
)

model_pytorch = CounterfactualExplanations.Models.PyTorchModel(model_loaded, :classification_binary)
```

The four arguments of `pytorch_model_loader()` are the following:
1. `model_path`: the path to the folder with a `.py` file where the PyTorch model is defined
2. `model_file`: the name of the `.py` file where the PyTorch model is defined
3. `class_name`: The name of the class of the PyTorch model
4. `pickle_path`: The path to the Pickle file that holds the model weights

### PROBE

Now that we have loaded the Python model, it is time to generate counterfactuals with it. A good choice for that is the PROBE generator that we added, introduced by Pawelczyk et al. (2022).

#### Load the generator

```{julia}
target = 2
factual = 1
chosen = rand(findall(predict_label(M, counterfactual_data) .== factual))
x = select_factual(counterfactual_data, chosen)
# Search:
generator = ProbeGenerator()
```

```{julia}
blobs_counterfactual = generate_counterfactual(
    x,
    target,
    counterfactual_data,
    M,
    generator;
    converge_when=:invalidation_rate,
    max_iter=10000,
    invalidation_rate=0.1,
    learning_rate=0.1,
)
CounterfactualExplanations.plot(blobs_counterfactual; title = "PROBE counterfactuals on a PyTorch model")
```

Let's now try a wildly different invalidation rate for the same model and same generator:

```{julia}
blobs_counterfactual = generate_counterfactual(
    x,
    target,
    counterfactual_data,
    M,
    generator;
    converge_when=:invalidation_rate,
    max_iter=10000,
    invalidation_rate=0.99,
    learning_rate=0.1,
)
CounterfactualExplanations.plot(blobs_counterfactual)
```

Finally, let's combine ideas from the PROBE, Greedy and REVISE generators, using the builder design pattern:

```{julia}
generator = Generators.Generator()

@chain generator begin
    @objective logitbinarycrossentropy + 0.1distance_l1     # PROBE
    @with_optimiser JSMADescent(η=0.5)
    @search_latent_space                                    # REVISE
end
```

```{julia}
blobs_counterfactual = generate_counterfactual(
    x,
    target,
    counterfactual_data,
    M,
    generator;
    converge_when=:invalidation_rate,
    max_iter=10000,
    invalidation_rate=0.9,
    learning_rate=0.1,
)
CounterfactualExplanations.plot(blobs_counterfactual; title = "PROBE + REVISE counterfactuals on a PyTorch model, IR=0.9")
```

### MLJ models

We have integrated three models from the [MLJ model registry](https://alan-turing-institute.github.io/MLJ.jl/dev/list_of_supported_models/) into our package. The list of supported MLJ models can be accessed as follows:

```{julia}
Models.mlj_models_catalogue
```

#### The `DecisionTreeClassifier` and `RandomForestClassifier` models

First, we added a basic interface that allows the user to load and use the `DecisionTreeClassifier` and the `RandomForestClassifier` from the MLJ package.

```{julia}
n = 500
counterfactual_data = Data.load_moons(n)
```

The decision tree can be fitted as follows:

```{julia}
decision_tree = Models.fit_model(counterfactual_data, :DecisionTree)
```

The random forest model can be fitted as follows:

```{julia}
forest = Models.fit_model(counterfactual_data, :RandomForest)
```

However, it's also possible to tune the DecisionTreeClassifier's parameters. This can be done using the keyword arguments when calling `fit_model()` as follows:

```{julia}
decision_tree = CounterfactualExplanations.Models.fit_model(counterfactual_data, :DecisionTree; max_depth=2, min_samples_leaf=3)
```

For all supported MLJ models, every tunable parameter they have is supported as a keyword argument. The tunable parameters for the `DecisionTreeModel` and the `RandomForestModel` can be found from the [documentation of the `DecisionTree.jl` package](https://docs.juliahub.com/DecisionTree/pEDeB/0.10.11/) under the Decision Tree Classifier and Random Forest Classifier sections.

Counterfactuals can be generated for this model using the Feature Tweak generator. Note that other generators are not applicable to these two models: both decision trees and random forests are non-differentiable tree-based models and thus, gradient-based generators don't apply for them.

### Feature Tweak

Since decision trees and random forests have non-differentiable decision boundaries, they were incompatible with all of the generators implemented previously by our client, as those generators perform gradient descent in the feature space to find the optimal feature values for the counterfactual. Such gradient descent cannot be performed with respect to non-differentiable decision boundaries.

This section describes the Feature Tweak generator, introduced by Tolomei et al (2017). This generator is specifically tailored for generating counterfactuals for non-differentiable tree-based models such as decision trees and random forests and thus expands the scope of models for which counterfactual explanations can be generated using the package.

This specialization to tree-based models comes at a cost: the Feature Tweak generator is incompatible with any model that does not have a tree structure, thus being incompatible with many models in the package. However, we believe that it's worth having such specialized generators in the package when that allows us to expand the scope of supported models, as Feature Tweak does.

Another important limitation is that the Feature Tweak generator can only generate counterfactuals for models trained on binary classification datasets. This limitation was also noted in the paper by Tolomei et al. and follow-up research might make it feasible to expand the method to multi-class classification problems.

#### Generating counterfactuals using Feature Tweak

We have already set up the decision tree and forest models in the previous section. The next step is to initialize the Feature Tweak generator:

```{julia}
generator = FeatureTweakGenerator(ϵ=0.1)
```

Now, let's take a point that the model believes is in class 1:

```{julia}
x = float32.([1, -0.5])
Models.predict_label(forest, x)
```

This leaves us with 0 as the target class. We can now generate the counterfactual as usual. Let's create a counterfactual for both the decision tree and the forest:

```{julia}
target = 0
tree_counterfactual = generate_counterfactual(x, target, counterfactual_data, decision_tree, generator)
forest_counterfactual = generate_counterfactual(x, target, counterfactual_data, forest, generator)
```

Finally, we can visualize the generated counterfactual and the decision boundaries of the decision tree and the forest:

```{julia}
CounterfactualExplanations.plot(tree_counterfactual; colorbar=false)
```

```{julia}
CounterfactualExplanations.plot(forest_counterfactual)
```

### CLUE

Added the CLUE generator, proposed by Antorán et al. (2023).

The CLUE Generator leverages differentiable probabilistic models, such as Bayesian Neural Networks (BNNs), to estimate uncertainty in predictions. In contrast to other generators in this package, CLUE is not meant for moving the input in the feature space to change the model's prediction of it: rather, it is meant to find a more robust datapoint of the same class. The generated result will be of the same class as the original input, but the model's uncertainty when classifying it will be lower.

CLUE works best for BNNs, as these models provide reliable uncertainty quantification. The CLUEGenerator can also be used with any other differentiable probabilistic model, but the results may not be as good as with BNNs. Thus, let's first train a Deep Ensemble, a neural network ensemble designed to approximate a Bayesian model:

```{julia}
counterfactual_data = Data.load_moons(N)
M = Models.fit_model(counterfactual_data, :DeepEnsemble)
```

```{julia}
generator = CLUEGenerator()
clue_counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator, converge_when=:early_stopping)
CounterfactualExplanations.plot(clue_counterfactual)
```

As seen here, the generator does not move the input to another class, but rather to a less uncertain spot in the feature space.

The CLUE generator can also be used upon already having achieved a counterfactual with a different generator. In this case you can use CLUE and make the counterfactual more robust.

## Added datasets

### Statlog German credit dataset

Added Statlog German credit as a benchmark dataset.

```{julia}
german_credit_data = CounterfactualExplanations.Data.load_german_credit()
german_credit_data_x = german_credit_data.X
german_credit_data_y = german_credit_data.output_encoder.y
```

#### Post Hoc benchmarking with Statlog German credit dataset

```{julia}
# Train a model
M = fit_model(german_credit_data, :Linear)

# Factual and target labels:
target = 1
factual = 0
```

```{julia}
n_individuals = 5

# Choose random factual individuals
ids = rand(findall(predict_label(M, german_credit_data) .== factual), n_individuals)
xs = select_factual(german_credit_data, ids)

generator = GenericGenerator()

generated_counterfactuals_german_credit_data = generate_counterfactual(
    xs,
    target,
    german_credit_data,
    M,
    generator; 
    num_counterfactuals=1
)
```

```{julia}
benchmark_german_credit_data = CounterfactualExplanations.Evaluation.benchmark(generated_counterfactuals_german_credit_data)

@chain benchmark_german_credit_data() begin
    @filter(variable == "distance")
    @select(sample, variable, value)
end
```

```{julia}
factual_datapoint = first(xs)[1]
counterfactual_datapoint = generated_counterfactuals_german_credit_data[1].s′

df = DataFrame(Vector1 = vec(factual_datapoint), Vector2 = vec(counterfactual_datapoint))
```

### CIFAR10 dataset

Added CIFAR10 from MLDatasets.jl as a benchmark dataset.

```{julia}
cifar_10_data = CounterfactualExplanations.Data.load_cifar_10()
cifar_10_x = cifar_10_data.X
cifar_10_y = cifar_10_data.output_encoder.y
```

```{julia}
images = []

for i in 0:9
    ys = findall(cifar_10_y .== i)
    for j in 0:9
        x = cifar_10_x[:,rand(ys)]

        x = clamp.((x .+ 1.0) ./ 2.0, 0.0, 1.0) |>
            x -> reshape(x, 32, 32, 3) |>
            x -> convert2image(CIFAR10, x)
        push!(images, x)
    end
end

mosaic(images..., ncol=10) |> display
```

#### UCI Adult dataset

Added the UCI adult dataset as a benchmark dataset.

```{julia}
adult_data = CounterfactualExplanations.Data.load_uci_adult()
adult_data_x = adult_data.X
adult_data_y = adult_data.output_encoder.y
```

```{julia}
# Train a model
M = fit_model(adult_data, :Linear)

# Factual and target labels:
target = 1
factual = 0
```

```{julia}
n = 10

# Choose random factual datapoints
ids = rand(findall(predict_label(M, adult_data) .== factual), n)
xs = select_factual(adult_data, ids)

generator = GenericGenerator()

adult_counterfactual_data = generate_counterfactual(
    xs,
    target,
    adult_data,
    M, 
    generator; 
    num_counterfactuals=1
)
```

#### References

M. Pawelczyk, T. Datta, J. van den Heuvel, G. Kasneci, and H. Lakkaraju, *Probabilistically robust recourse: Navigating the trade-offs between costs and robustness in algorithmic recourse*, 2022. [Online]. Available: https://arxiv.org/abs/2203.06768.

G. Tolomei, F. Silvestri, A. Haines, and M. Lalmas, “Interpretable predictions of tree-based ensembles via actionable feature tweaking,” in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, Aug. 2017. [Online]. Available: https://doi.org/10.1145%5C%2F3097983.3098039.

J. Antoran, U. Bhatt, T. Adel, A. Weller, and J. M. Hernandez-Lobato, “Getting a CLUE: A method for explaining uncertainty estimates,” in International Conference on Learning Representations, Published as a conference paper at ICLR 2021, 2021.
