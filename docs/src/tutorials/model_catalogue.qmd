```@meta
CurrentModule = CounterfactualExplanations 
```

```{julia}
#| echo: false
include("docs/setup_docs.jl")
eval(setup_docs)
```

# Model Catalogue

While in general it is assumed that users will use this package to explain their pre-trained models, we provide out-of-the-box functionality to train various simple default models. In this tutorial, we will see how these models can be fitted to `CounterfactualData`.

## Available Models

The `standard_models_catalogue` can be used to inspect the available default models: 

```{julia}
#| output: true
standard_models_catalogue
```

As with the [`data_catalogue`](@ref), the dictionary keys correspond to the model names. In this case, the dictionary values are constructors that can be used called on instances of type `CounterfactualData` to fit the corresponding model. In most cases, users will find it most convenient to use the [`fit_model`](@ref) API call instead.

Models from the standard model catalogue are a core part of the package and thus compatible with all offered counterfactual generators and functionalities.

The `all_models_catalogue` can be used to inspect all models offered by the package:

```{julia}
all_models_catalogue
```

However, when using models not included in the `standard_models_catalogue`, additional caution is advised: they might not be supported by all counterfactual generators or they might not be models native to Julia. Thus, a more thorough reading of their documentation may be necessary to make sure that they are used correctly.

## Fitting Flux Models

First, let's load one of the synthetic datasets:

```{julia}
n = 500
counterfactual_data = load_multi_class(n)
```

We could use a Deep Ensemble [@lakshminarayanan2016simple] as follows:

```{julia}
M = fit_model(counterfactual_data, :DeepEnsemble)
```

The returned object is an instance of type `FluxEnsemble <: AbstractFittedModel` and can be used in downstream tasks without further ado. For example, the resulting fit can be visualised using the generic `plot()` method as:

```{julia}
#| output: true

plts = []
for target in counterfactual_data.y_levels
    plt = plot(M, counterfactual_data; target=target, title="p(y=$(target)|x,θ)")
    plts = [plts..., plt]
end
plot(plts...)
```

## Tuning Flux Models

By default, model architectures are very simple. Through optional arguments, users have some control over the neural network architecture and can choose to impose regularization through dropout. Let's tackle a more challenging dataset: MNIST [@lecun1998mnist]. 

```{julia}
counterfactual_data = load_mnist(10000)
train_data, test_data = 
    CounterfactualExplanations.DataPreprocessing.train_test_split(counterfactual_data)
```

```{julia}
#| echo: false
#| output: true

img_size = 200
n_images = 5
input_dim = size(train_data.X, 1)
plts = []
for i in 1:n_images
    x = counterfactual_data.X[:,rand(1:end)]
    img = convert2image(MNIST, reshape(x,Int(√(input_dim)),Int(√(input_dim))))
    plt = plot(img, axis=nothing)
    plts = [plts..., plt]
end
plot(plts..., size=(n_images * img_size, img_size), layout=(1, n_images))
```

In this case, we will use a Multi-Layer Perceptron (MLP) but we will adjust the model and training hyperparameters. Parameters related to training of `Flux.jl` models are currently stored in a mutable container:

```{julia}
#| output: true

flux_training_params
```

In cases like this one, where model training can be expected to take a few moments, it can be useful to activate verbosity, so let's set the corresponding field value to `true`. We'll also impose mini-batch training:

```{julia}
flux_training_params.verbose = true
flux_training_params.batchsize = round(size(train_data.X,2)/10)
```

To account for the fact that this is a slightly more challenging task, we will use an appropriate number of hidden neurons per layer. We will also activate dropout regularization. To scale networks up further, it is also possible to adjust the number of hidden layers, which we will not do here.

```{julia}
model_params = (
    n_hidden = 32,
    dropout = true
)
```

The `model_params` can be supplied to the familiar API call:

```{julia}
#| output: true
M = fit_model(train_data, :MLP; model_params...)
```

The model performance on our test set can be evaluated as follows:

```{julia}
#| output: true
model_evaluation(M, test_data)
```

Finally, let's restore the default training parameters:

```{julia}
CounterfactualExplanations.reset!(flux_training_params)
```

## Fitting and tuning MLJ models

Among models from the MLJ library, only importing the `EvoTreeClassifier` is supported thus far. Generating counterfactuals is not supported for any MLJ model yet, but integration with gradient-based generators will be provided soon. For that reason, a short overview of setting up MLJ models will be provided.

Tuning the EvoTreeClassifier is very simple. As the first step, let's reload the dataset:

```{julia}
n = 500
counterfactual_data = CounterfactualExplanations.Data.load_moons(n)
```

Using the usual procedure for fitting models, we can call the following method:

```{julia}
evotree = CounterfactualExplanations.Models.fit_model(counterfactual_data, :EvoTree)
```

However, it's also possible to tune the EvoTreeClassifier's parameters. This can be done using the keyword arguments when calling `fit_model()` as follows:

```{julia}
evotree = CounterfactualExplanations.Models.fit_model(counterfactual_data, :EvoTree; lambda=0.1, max_depth=3)
```

All tunable parameters of the `EvoTreeClassifier` are supported as keyword arguments. The tunable parameters can be found from the [documentation of the `EvoTrees.jl` package](https://evovest.github.io/EvoTrees.jl/stable/).

Please note again that generating counterfactuals for the `EvoTreeClassifier` is not supported yet.

## References




