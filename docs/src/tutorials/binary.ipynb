{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```@meta\n",
    "CurrentModule = AlgorithmicRecourse \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recourse for binary targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogLevel(1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Flux, Random, Plots, PlotThemes, AlgorithmicRecourse\n",
    "theme(:wong)\n",
    "using Logging\n",
    "disable_logging(Logging.Info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the core functionality of AlgorithmicRecourse.jl we will look at two example use cases of the `generate_recourse` function. This function takes a structure of type `Generator` as its main argument. Users can utilize one of the [default generators](#default-generators): `GenericGenerator <: Generator`, `GreedyGenerator <: Generator`. Alternatively, users can also create their own [custom generator](#custom-generators). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default generators\n",
    "\n",
    "### `GenericGenerator`\n",
    "\n",
    "Let $t\\in\\{0,1\\}$ denote the target label, $M$ the model (classifier) and $\\underline{x}\\in\\mathbb{R}^D$ the vector of counterfactual features. In order to generate recourse the `GenericGenerator` optimizes the following objective function through steepest descent\n",
    "\n",
    "```math\n",
    "\\underline{x} = \\arg \\min_{\\underline{x}}  \\ell(M(\\underline{x}),t) + \\lambda h(\\underline{x})\n",
    "```\n",
    "\n",
    "where $\\ell$ denotes some loss function targeting the deviation between the target label and the predicted label and $h(\\cdot)$ as a complexity penality generally addressing the *realism* or *cost* of the proposed counterfactual. \n",
    "\n",
    "Let's generate some toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random data:\n",
    "Random.seed!(1234);\n",
    "N = 25\n",
    "w = [1.0 1.0]# true coefficients\n",
    "b = 0\n",
    "x, y = toy_data_linear(N)\n",
    "X = hcat(x...)\n",
    "plt = plot()\n",
    "plt = plot_data!(plt,X',y);\n",
    "savefig(plt, \"www/binary_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](www/binary_samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this toy data we will now implement algorithmic recourse as follows:\n",
    "\n",
    "- Use the coefficients `w` and `b` to define our model using `AlgorithmicRecourse.Models.LogisticModel(w, b)`.\n",
    "- Define our `GenericGenerator`.\n",
    "- Generate recourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "using AlgorithmicRecourse.Models: LogisticModel, logits, probs # import functions in order to extend\n",
    "# Logit model:\n",
    "ð‘´ = LogisticModel(w, [b])\n",
    "# Randomly selected factual:\n",
    "Random.seed!(1234);\n",
    "xÌ… = X[:,rand(1:size(X)[2])]\n",
    "yÌ… = round(probs(ð‘´, xÌ…)[1])\n",
    "target = ifelse(yÌ…==1.0,0.0,1.0) # opposite label as target\n",
    "Î³ = ifelse(target==1.0,0.75,0.25); # desired threshold based on target\n",
    "# Define Generator:\n",
    "generator = GenericGenerator(0.1,0.1,1e-5,:logitbinarycrossentropy,nothing)\n",
    "# Generate recourse:\n",
    "recourse = generate_recourse(generator, xÌ…, ð‘´, target, Î³); # generate recourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the resulting counterfactual path in the 2-D feature space (left) and the predicted probability (right):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = size(recourse.path)[1]\n",
    "yÌ‚ = probs(recourse.ð‘´, recourse.path')\n",
    "p1 = plot_contour(X',y,ð‘´;clegend=false, title=\"Posterior predictive - Plugin\")\n",
    "anim = @animate for t in 1:T\n",
    "    scatter!(p1, [recourse.path[t,1]], [recourse.path[t,2]], ms=5, color=Int(yÌ…))\n",
    "    p2 = plot(1:t, yÌ‚[1:t], xlim=(0,T), ylim=(0, 1), label=\"p(yÌ²=1)\", title=\"Validity\")\n",
    "    Plots.abline!(p2,0,Î³,label=\"threshold Î³\") # decision boundary\n",
    "    plot(p1,p2,size=(800,400))\n",
    "end\n",
    "gif(anim, \"www/binary_generic_recourse.gif\", fps=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](www/binary_generic_recourse.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GreedyGenerator`\n",
    "\n",
    "Next we will repeat the exercise above, but instead use the `GreedyGenerator` in the context of a Bayesian classifier. This generator is greedy in the sense that it simply chooses the most salient feature $\\underline{x}[d]$ where $d=\\arg\\max_{d \\in [1,D]} \\nabla_{\\underline{x}} \\ell(M(\\underline{x}),t)$ and perturbs it by a fixed amount $\\delta$. In other words, optimization is penalty-free. This is possible in the Bayesian context, because maximizing the predictive probability $\\gamma$ corresponds to minimizing the predictive uncertainty: by construction the generated counterfactual will therefore be *realistic* (low epistemic uncertainty) and *unambiguous* (low aleotoric uncertainty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "Î£ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix\n",
    "Î¼ = hcat(b, w)\n",
    "ð‘´ = AlgorithmicRecourse.Models.BayesianLogisticModel(Î¼, Î£);\n",
    "generator = GreedyGenerator(0.5,12,:logitbinarycrossentropy,nothing)\n",
    "recourse = generate_recourse(generator, xÌ…, ð‘´, target, Î³); # generate recourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we plot the resulting counterfactual path (left) and changes in the predicted probability (right). For the Bayesian classifier predicted probabilities splash out: uncertainty increases in regions with few samples. Note how the greedy approach selects the same most salient feature over and over again until its exhausted (i.e. it has been chosen `GreedyGenerator.n` times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = size(recourse.path)[1]\n",
    "yÌ‚ = probs(recourse.ð‘´, recourse.path')\n",
    "p1 = plot_contour(X',y,ð‘´;clegend=false, title=\"Posterior predictive - Laplace\")\n",
    "anim = @animate for t in 1:T\n",
    "    scatter!(p1, [recourse.path[t,1]], [recourse.path[t,2]], ms=5, color=Int(yÌ…))\n",
    "    p2 = plot(1:t, yÌ‚[1:t], xlim=(0,T), ylim=(0, 1), label=\"p(yÌ²=1)\", title=\"Validity\")\n",
    "    Plots.abline!(p2,0,Î³,label=\"threshold Î³\") # decision boundary\n",
    "    plot(p1,p2,size=(800,400))\n",
    "end\n",
    "gif(anim, \"www/binary_greedy_recourse.gif\", fps=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](www/binary_greedy_recourse.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.4",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
