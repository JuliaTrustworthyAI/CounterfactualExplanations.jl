``` @meta
CurrentModule = CounterfactualExplanations 
```

# Recourse for binary targets

``` julia
using Flux, Random, Plots, PlotThemes, CounterfactualExplanations
theme(:wong)
using Logging
disable_logging(Logging.Info)
include("dev/utils.jl") # some plotting functions
www_folder = "docs/src/tutorials/www"
```

To understand the core functionality of CounterfactualExplanations.jl we will look at two example use cases of the `generate_counterfactual` function. This function takes a structure of type `AbstractGenerator` as its main argument. Users can utilize one of the [default generators](#default-generators): `GenericGenerator <: AbstractGenerator`, `GreedyGenerator <: AbstractGenerator`. Alternatively, users can also create their own [custom generator](#custom-generators).

## Default generators

### `GenericGenerator`

Let *t*â€„âˆˆâ€„{0,â€†1} denote the target label, *M* the model (classifier) and xÌƒ âˆˆ â„á´° the vector of counterfactual features. In order to generate recourse the `GenericGenerator` optimizes the following objective function through steepest descent

``` math
\tilde{x} = \arg \min_{\tilde{x}}  \ell(M(\tilde{x}),t) + \lambda h(\tilde{x})
```

where â„“ denotes some loss function targeting the deviation between the target label and the predicted label and *h*(â‹…) as a complexity penalty generally addressing the *realism* or *cost* of the proposed counterfactual.

Letâ€™s generate some toy data:

``` julia
# Some random data:
using CounterfactualExplanations.Data
Random.seed!(1234);
N = 25
w = [1.0 1.0]# true coefficients
b = 0
x, y = toy_data_linear(N)
X = hcat(x...)
counterfactual_data = CounterfactualData(X,y')
plt = plot()
plt = plot_data!(plt,X',y);
savefig(plt, joinpath(www_folder, "binary_samples.png"))
```

![](www/binary_samples.png)

For this toy data we will now implement algorithmic recourse as follows:

-   Use the coefficients `w` and `b` to define our model using `CounterfactualExplanations.Models.LogisticModel(w, b)`.
-   Define our `GenericGenerator`.
-   Generate recourse.

``` julia
using CounterfactualExplanations.Models: LogisticModel, probs 
# Logit model:
ğ‘´ = LogisticModel(w, [b])
# Randomly selected factual:
Random.seed!(123);
x = select_factual(counterfactual_data,rand(1:size(X)[2]))
y = round(probs(ğ‘´, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target
```

``` julia
plt = plot_contour(X',y,ğ‘´;title="Posterior predictive - Plugin")
savefig(plt, joinpath(www_folder, "binary_contour.png"))
```

![](www/binary_contour.png)

``` julia
# Define AbstractGenerator:
generator = GenericGenerator()
# Generate recourse:
counterfactual = generate_counterfactual(x, target, counterfactual_data, ğ‘´, generator); # generate recourse
```

Now letâ€™s plot the resulting counterfactual path in the 2-D feature space (left) and the predicted probability (right):

``` julia
import CounterfactualExplanations.Counterfactuals: target_probs
T = total_steps(counterfactual)
X_path = reduce(hcat,path(counterfactual))
yÌ‚ = target_probs(counterfactual,X_path)
p1 = plot_contour(X',y,ğ‘´;clegend=false, title="Posterior predictive - Plugin")
anim = @animate for t in 1:T
    scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label="")
    p2 = plot(1:t, yÌ‚[1:t], xlim=(0,T), ylim=(0, 1), label="p(yÌƒ=" * string(target) * ")", title="Validity", lc=:black)
    Plots.abline!(p2,0,counterfactual.params[:Î³],label="threshold Î³", ls=:dash) # decision boundary
    plot(p1,p2,size=(800,400))
end
gif(anim, joinpath(www_folder, "binary_generic_recourse.gif"), fps=25)
```

![](www/binary_generic_recourse.gif)

### `GreedyGenerator`

Next we will repeat the exercise above, but instead use the `GreedyGenerator` in the context of a Bayesian classifier. This generator is greedy in the sense that it simply chooses the most salient feature {xÌƒ}áµˆ where

``` math
d=\arg\max_{d \in [1,D]} \nabla_{\tilde{x}} \ell(M(\tilde{x}),t)
```

and perturbs it by a fixed amount *Î´*. In other words, optimization is penalty-free. This is possible in the Bayesian context, because maximizing the predictive probability *Î³* corresponds to minimizing the predictive uncertainty: by construction the generated counterfactual will therefore be *realistic* (low epistemic uncertainty) and *unambiguous* (low aleotoric uncertainty).

``` julia
using LinearAlgebra
Î£ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix
Î¼ = hcat(b, w)
ğ‘´ = CounterfactualExplanations.Models.BayesianLogisticModel(Î¼, Î£);
generator = GreedyGenerator(Dict(:Î´=>0.1,:n=>25))
counterfactual = generate_counterfactual(x, target, counterfactual_data, ğ‘´, generator); # generate counterfactual
```

Once again we plot the resulting counterfactual path (left) and changes in the predicted probability (right). For the Bayesian classifier predicted probabilities splash out: uncertainty increases in regions with few samples. Note how the greedy approach selects the same most salient feature over and over again until its exhausted (i.e.Â it has been chosen `GreedyGenerator.n` times).

``` julia
import CounterfactualExplanations.Counterfactuals: target_probs
T = total_steps(counterfactual)
X_path = reduce(hcat,path(counterfactual))
yÌ‚ = target_probs(counterfactual,X_path)
p1 = plot_contour(X',y,ğ‘´;clegend=false, title="Posterior predictive - Plugin")
anim = @animate for t in 1:T
    scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label="")
    p2 = plot(1:t, yÌ‚[1:t], xlim=(0,T), ylim=(0, 1), label="p(yÌƒ=" * string(target) * ")", title="Validity", lc=:black)
    Plots.abline!(p2,0,counterfactual.params[:Î³],label="threshold Î³", ls=:dash) # decision boundary
    plot(p1,p2,size=(800,400))
end
gif(anim, joinpath(www_folder, "binary_greedy_recourse.gif"), fps=25);
```

![](www/binary_greedy_recourse.gif)
