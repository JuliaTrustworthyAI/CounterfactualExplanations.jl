{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```@meta\n",
        "CurrentModule = CounterfactualExplanations \n",
        "```\n"
      ],
      "id": "edc4ceb1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "include(\"$(pwd())/docs/setup_docs.jl\")\n",
        "eval(setup_docs)"
      ],
      "id": "6e7b72aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# `REVISEGenerator`\n",
        "\n",
        "REVISE is a Latent Space generator introduced by @joshi2019realistic.\n",
        "\n",
        "## Description\n",
        "\n",
        "The current consensus in the literature is that Counterfactual Explanations should be realistic: the generated counterfactuals should look like they were generated by the data-generating process (DGP) that governs the problem at hand. With respect to Algorithmic Recourse, it is certainly true that counterfactuals should be realistic in order to be actionable for individuals.^[In general, we believe that there may be a trade-off between creating counterfactuals that respect the DGP vs. counterfactuals reflect the behaviour of the black-model in question - both accurately and complete.] To address this need, researchers have come up with various approaches in recent years. Among the most popular approaches is **Latent Space Search**, which was first proposed in @joshi2019realistic: instead of traversing the feature space directly, this approach relies on a separate generative model that learns a latent space representation of the DGP. Assuming the generative model is well-specified, access to the learned latent embeddings of the data comes with two **advantages**:\n",
        "\n",
        "1. Since the learned DGP is encoded in the latent space, the generated counterfactuals will respect the learned representation of the data. In practice, this means that counterfactuals will be realistic.\n",
        "2. The latent space is typically a compressed (i.e. lower dimensional) version of the feature space. This makes the counterfactual search less costly.\n",
        "\n",
        "There are also certain **disadvantages** though:\n",
        "\n",
        "1. Learning generative models is (typically) an expensive task, which may well outweigh the benefits associated with utlimately traversing a lower dimensional space. \n",
        "2. If the generative model is poorly specified, this will affect the quality of the counterfactuals.^[We believe that there is another potentially crucial disadvantage of relying on a separate generative model: it reallocates the task of learning realistic explanations for the data from the black-box model to the generative model.]\n",
        "\n",
        "Anyway, traversing latent embeddings is a powerful idea that may be very useful depending on the specific context. This tutorial introduces the concept and how it is implemented in this package. \n",
        "\n",
        "## Usage\n",
        "\n",
        "The approach can be used in our package as follows:\n"
      ],
      "id": "a9dad717"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "generator = REVISEGenerator()\n",
        "ce = generate_counterfactual(x, target, counterfactual_data, M, generator)\n",
        "plot(ce)"
      ],
      "id": "ce5d2b87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Worked 2D Examples\n",
        "\n",
        "Below we load 2D data and train a VAE on it and plot the original samples against their reconstructions.\n"
      ],
      "id": "67bafd1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# output: true\n",
        "\n",
        "counterfactual_data = load_overlapping()\n",
        "X = counterfactual_data.X\n",
        "y = counterfactual_data.y\n",
        "input_dim = size(X, 1)\n",
        "using CounterfactualExplanations.GenerativeModels: VAE, train!, reconstruct\n",
        "vae = VAE(input_dim; nll=Flux.Losses.mse, epochs=100, λ=0.01, latent_dim=2, hidden_dim=32)\n",
        "flux_training_params.verbose = true\n",
        "train!(vae, X, y)\n",
        "X̂ = reconstruct(vae, X)[1]\n",
        "p0 = scatter(X[1, :], X[2, :], color=:blue, label=\"Original\", xlab=\"x₁\", ylab=\"x₂\")\n",
        "scatter!(X̂[1, :], X̂[2, :], color=:orange, label=\"Reconstructed\", xlab=\"x₁\", ylab=\"x₂\")\n",
        "p1 = scatter(X[1, :], X̂[1, :], color=:purple, label=\"\", xlab=\"x₁\", ylab=\"x̂₁\")\n",
        "p2 = scatter(X[2, :], X̂[2, :], color=:purple, label=\"\", xlab=\"x₂\", ylab=\"x̂₂\")\n",
        "plt2 = plot(p1,p2, layout=(1,2), size=(800, 400))\n",
        "plot(p0, plt2, layout=(2,1), size=(800, 600))"
      ],
      "id": "41cd5bb8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we train a simple MLP for the classification task. Then we determine a target and factual class for our counterfactual search and select a random factual instance to explain.\n"
      ],
      "id": "f5f18a33"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "M = fit_model(counterfactual_data, :MLP)\n",
        "target = 2\n",
        "factual = 1\n",
        "chosen = rand(findall(predict_label(M, counterfactual_data) .== factual))\n",
        "x = select_factual(counterfactual_data, chosen)"
      ],
      "id": "49f9383d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we generate and visualize the generated counterfactual:\n"
      ],
      "id": "fd469ef9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: true\n",
        "\n",
        "# Search:\n",
        "generator = REVISEGenerator()\n",
        "ce = generate_counterfactual(x, target, counterfactual_data, M, generator)\n",
        "plot(ce)"
      ],
      "id": "d92454dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3D Example\n",
        "\n",
        "To illustrate the notion of Latent Space search, let's look at an example involving 3-dimensional input data, which we can still visualize. The code chunk below loads the data and implements the counterfactual search. \n"
      ],
      "id": "2d435ab6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data and Classifier:\n",
        "counterfactual_data = load_blobs(k=3)\n",
        "X = counterfactual_data.X\n",
        "ys = counterfactual_data.output_encoder.labels.refs\n",
        "M = fit_model(counterfactual_data, :MLP)\n",
        "\n",
        "# Randomly selected factual:\n",
        "x = select_factual(counterfactual_data,rand(1:size(counterfactual_data.X,2)))\n",
        "y = predict_label(M, counterfactual_data, x)[1]\n",
        "target = counterfactual_data.y_levels[counterfactual_data.y_levels .!= y][1]\n",
        "\n",
        "# Generate recourse:\n",
        "ce = generate_counterfactual(x, target, counterfactual_data, M, generator)"
      ],
      "id": "b1282a4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The figure below demonstrates the idea of searching counterfactuals in a lower-dimensional latent space: on the left, we can see the counterfactual search in the 3-dimensional feature space, while on the right we can see the corresponding search in the latent space.\n"
      ],
      "id": "770f8b2b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "# Feature:\n",
        "p1 = scatter3d(X[1,:],X[2,:],X[3,:],group=vec(Int.(ys)), title=\"Feature space (3d)\", palette=:rainbow)\n",
        "X_path = reduce(hcat,path(ce))\n",
        "scatter3d!([X_path[1,end]],[X_path[2,end]],[X_path[3,end]],colour=:yellow,ms=10,label=\"Counterfactual\")\n",
        "scatter3d!(X_path[1,1:(end-1)],X_path[2,1:(end-1)],X_path[3,1:(end-1)],colour=:yellow,ms=5,label=\"\")\n",
        "\n",
        "# Latent:\n",
        "p2 = plot(ce.data.generative_model, X, ys; title=\"Latent space (2d)\")\n",
        "X_path = reduce(hcat,path(ce, feature_space=false))\n",
        "scatter!([X_path[1,end]],[X_path[2,end]],colour=:yellow,ms=10,label=\"Counterfactual\")\n",
        "scatter!([X_path[1,1:(end-1)]],[X_path[2,1:(end-1)]],colour=:yellow,ms=5,label=\"\")\n",
        "\n",
        "plot(p1,p2, legend=false, size=(800,350))"
      ],
      "id": "e36f02cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MNIST data\n",
        "\n",
        "Let's carry the ideas introduced above over to a more complex example. The code below loads MNIST data as well as a pre-trained classifier and generative model for the data.\n"
      ],
      "id": "e773cf18"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using CounterfactualExplanations.Models: load_mnist_mlp, load_mnist_ensemble, load_mnist_vae\n",
        "counterfactual_data = load_mnist()\n",
        "X, y = CounterfactualExplanations.DataPreprocessing.unpack_data(counterfactual_data)\n",
        "input_dim, n_obs = size(counterfactual_data.X)\n",
        "M = load_mnist_mlp()\n",
        "vae = load_mnist_vae()"
      ],
      "id": "022e3452",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "test_data = load_mnist_test()\n",
        "Markdown.parse(\n",
        "  \"\"\"\n",
        "  The F1-score of our pre-trained image classifier on test data is: $(round(model_evaluation(M, test_data)[1], digits=2))\n",
        "  \"\"\"\n",
        ")"
      ],
      "id": "283923dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before continuing, we supply the pre-trained generative model to our data container:\n"
      ],
      "id": "e5a8ea9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "counterfactual_data.generative_model = vae # assign generative model"
      ],
      "id": "55fd6435",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define a factual and target label:\n"
      ],
      "id": "693dc791"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Randomly selected factual:\n",
        "Random.seed!(2023)\n",
        "factual_label = 8\n",
        "x = reshape(X[:,rand(findall(predict_label(M, counterfactual_data).==factual_label))],input_dim,1)\n",
        "target = 3\n",
        "factual = predict_label(M, counterfactual_data, x)[1]"
      ],
      "id": "da6c0953",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "Markdown.parse(\n",
        "  \"\"\"\n",
        "  Using REVISE, we are going to turn a randomly drawn $(factual_label) into a $(target).\n",
        "  \"\"\"\n",
        ")"
      ],
      "id": "d061c49c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The API call is the same as always:\n"
      ],
      "id": "3172f9ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "γ = 0.95\n",
        "# Define generator:\n",
        "generator = REVISEGenerator(opt=Flux.Adam(0.5))\n",
        "# Generate recourse:\n",
        "ce = generate_counterfactual(x, target, counterfactual_data, M, generator; decision_threshold=γ)"
      ],
      "id": "b2c0c348",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The chart below shows the results:\n"
      ],
      "id": "17a681ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "\n",
        "image_size = 300\n",
        "p1 = plot(\n",
        "    convert2image(MNIST, reshape(x,28,28)),\n",
        "    axis=nothing, \n",
        "    size=(image_size, image_size),\n",
        "    title=\"Factual: $(factual_label)\"\n",
        ")\n",
        "savefig(p1, joinpath(www_path, \"mnist_factual.png\"))\n",
        "target_prob = round(target_probs(ce)[1],digits=2)\n",
        "ŷ = CounterfactualExplanations.counterfactual_label(ce)[1]\n",
        "p2 = plot(\n",
        "    convert2image(MNIST, reshape(CounterfactualExplanations.counterfactual(ce),28,28)),\n",
        "    axis=nothing, \n",
        "    size=(image_size, image_size),\n",
        "    title=\"Counterfactual: $(ŷ) (p=$(target_prob))\"\n",
        ")\n",
        "plot(p1,p2;size=(image_size * 2,image_size), layout=(1,2))"
      ],
      "id": "30bb8753",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References"
      ],
      "id": "c7f5390e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.10",
      "language": "julia",
      "display_name": "Julia 1.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}