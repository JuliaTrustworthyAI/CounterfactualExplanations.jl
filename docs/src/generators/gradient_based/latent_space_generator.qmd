```@meta
CurrentModule = CounterfactualExplanations 
```

```{julia}
#| echo: false
include("docs/setup_docs.jl")
eval(setup_docs)
```

# Latent Space Search

The current consensus in the literature is that Counterfactual Explanations should be realistic: the generated counterfactuals should look like they were generated by the data-generating process (DGP) that governs the problem at hand. With respect to Algorithmic Recourse, it is certainly true that counterfactuals should be realistic in order to be actionable for individuals.^[In general, we believe that there may be a trade-off between creating counterfactuals that respect the DGP vs. counterfactuals reflect the behaviour of the black-model in question - both accurately and complete.] To address this need, researchers have come up with various approaches in recent years. Among the most popular approaches is **Latent Space Search**, which was first proposed in @joshi2019realistic: instead of traversing the feature space directly, this approach relies on a separate generative model that learns a latent space representation of the DGP. Assuming the generative model is well-specified, access to the learned latent embeddings of the data comes with two **advantages**:

1. Since the learned DGP is encoded in the latent space, the generated counterfactuals will respect the learned representation of the data. In practice, this means that counterfactuals will be realistic.
2. The latent space is typically a compressed (i.e. lower dimensional) version of the feature space. This makes the counterfactual search less costly.

There are also certain **disadvantages** though:

1. Learning generative models is (typically) an expensive task, which may well outweigh the benefits associated with utlimately traversing a lower dimensional space. 
2. If the generative model is poorly specified, this will affect the quality of the counterfactuals.^[We believe that there is another potentially crucial disadvantage of relying on a separate generative model: it reallocates the task of learning realistic explanations for the data from the black-box model to the generative model.]

Anyway, traversing latent embeddings is a powerful idea that may be very useful depending on the specific context. This tutorial introduces the concept and how it is implemented in this package. 

## Synthetic data

We start by looking at synthetic data. 

### 2D Example

We then draw a random factual and generate two counterfactuals - one using generic search in the feature space and one using generic search in the latent space. The default generative model - a Variational Autoencoder (VAE) - is set up and trained under the hood. The resulting counterfactual paths are shown in the animation below: note how latent space search results in faster convergence to an optimum that sits right within the cluster of samples in the target class. For generic search in the feature space we instead end up just crossing the decision boundary before converging.  

```{julia}
# Data and Classifier:
counterfactual_data = load_linearly_separable(500)
M = fit_model(counterfactual_data, :Linear)

# Randomly selected factual:
x = select_factual(counterfactual_data,rand(1:size(counterfactual_data.X,2)))
y = predict_label(M, counterfactual_data, x)[1]
target = counterfactual_data.y_levels[counterfactual_data.y_levels .!= y][1]
```

```{julia}
opt = Descent(0.01)
# Define generator:
generator = GenericGenerator(opt=opt)
# Generate recourse:
ce = generate_counterfactual(x, target, counterfactual_data, M, generator)
```

```{julia}
generator = REVISEGenerator(opt=opt)
ce_latent = generate_counterfactual(x, target, counterfactual_data, M, generator)
```


```{julia}
#| echo: false
p1 = plot(ce; colorbar=false, title="Standard")
p2 = plot(ce_latent; colorbar=false, title="Latent")
plt = plot(p1,p2,size=(800,400))
savefig(plt, joinpath(www_path,"latent_2d.png"))
```

![Counterfactuals generated through generic search in feature space (left) and latent space (right).](../www/latent_2d.png)

### 3D Example

This second example is largely analogous to what we just saw above. The figure below demonstrates the idea of searching counterfactuals in a lower-dimensional latent space. 

```{julia}
# Data and Classifier:
counterfactual_data = load_blobs(k=3)
X = counterfactual_data.X
ys = counterfactual_data.output_encoder.labels.refs
M = fit_model(counterfactual_data, :MLP)

# Randomly selected factual:
x = select_factual(counterfactual_data,rand(1:size(counterfactual_data.X,2)))
y = predict_label(M, counterfactual_data, x)[1]
target = counterfactual_data.y_levels[counterfactual_data.y_levels .!= y][1]

# Generate recourse:
ce = generate_counterfactual(x, target, counterfactual_data, M, generator)
```

```{julia}
#| echo: false

# Feature:
p1 = scatter3d(X[1,:],X[2,:],X[3,:],group=vec(Int.(ys)), title="Feature space (3d)", palette=:rainbow)
X_path = reduce(hcat,path(ce))
scatter3d!([X_path[1,end]],[X_path[2,end]],[X_path[3,end]],colour=:yellow,ms=10,label="Counterfactual")
scatter3d!(X_path[1,1:(end-1)],X_path[2,1:(end-1)],X_path[3,1:(end-1)],colour=:yellow,ms=5,label="")

# Latent:
p2 = plot(ce.data.generative_model, X, ys; title="Latent space (2d)")
X_path = reduce(hcat,path(ce, feature_space=false))
scatter!([X_path[1,end]],[X_path[2,end]],colour=:yellow,ms=10,label="Counterfactual")
scatter!([X_path[1,1:(end-1)]],[X_path[2,1:(end-1)]],colour=:yellow,ms=5,label="")

plt = plot(p1,p2, legend=false, size=(800,400))
savefig(plt, joinpath(www_path, "example_3d.png"))
```

![](../www/example_3d.png)

## MNIST data

Finally, let's take the ideas introduced above to a more complex example. We first load the MNIST data and a simple pre-trained neural network. The test set accuracy is shown below.

```{julia}
using CounterfactualExplanations.Models: load_mnist_mlp, load_mnist_ensemble, load_mnist_vae
using Images
using MLDatasets
using MLDatasets: convert2image
counterfactual_data = load_mnist()
X, y = CounterfactualExplanations.DataPreprocessing.unpack_data(counterfactual_data)
input_dim, n_obs = size(counterfactual_data.X)
M = load_mnist_mlp()
M_ens = load_mnist_ensemble()
vae = load_mnist_vae()
```

### Training the VAE

```{julia}
X = counterfactual_data.X
ys = counterfactual_data.y
draw_mnist(i) = reshape(X[:,rand(findall(Flux.onecold(ys,0:9).==i))],input_dim,1)
reconstruct_mnist(x) = σ.(CounterfactualExplanations.GenerativeModels.reconstruct(vae,x)[1]) 
reshape_mnist(x) = reshape(x,28,28)
plt_list = []
for i in 0:9
  x = draw_mnist(i)
  x_rec = reconstruct_mnist(x)
  img = plot(convert2image(MNIST,reshape_mnist(x)), title="Original", axis=nothing)
  img_rec = plot(convert2image(MNIST,reshape_mnist(x_rec)), title="Rec", axis=nothing)
  plt = plot(img, img_rec)
  plt_list = vcat(plt_list, plt)
end
plt = plot(plt_list..., layout=(10,1), size=(200,1000))
savefig(plt, joinpath(www_path, "vae_reconstruct.png"))
```

... and supplied to an instance of type `CounterfactualData`. The figure below shows randomly selected MNIST images (left) and their reconstructions (right).

```{julia}
counterfactual_data.generative_model = vae # assign generative model
```

![Randomly selected MNIST images (left) and their reconstructions (right).](../www/vae_reconstruct.png)

### Counterfactual search

Since the image reconstructions are decent, we can expect the counterfactual search through the latent embedding to yield realistic counterfactuals. Below we put this to the test: we select a random nine (9) and use generic search in the latent space to generate a four (4). Note, that we have set the threshold probability to $90\%$ and we have chosen not to penalize the distance of the counterfactual from its factual. The result shown in the figure below is convincing. 

```{julia}
# Randomly selected factual:
n_factuals = 5
input_dim = size(X,1)
factual = 9
using Random
x = select_factual(counterfactual_data,rand(findall(predict_label(M, counterfactual_data, X).==factual), n_factuals))
target = 4
γ = 0.975
```

```{julia}
# Define generator:
generator = REVISEGenerator(
  opt = Descent(0.5),
  decision_threshold = γ
)
# Generate recourse:
ces = generate_counterfactual(x, target, counterfactual_data, M, generator)
```

```{julia}
using MLDatasets: convert2image
using Images
image_size = 300
function build_plot(x,ce)
    p1 = plot(convert2image(MNIST, reshape(x,28,28)),axis=nothing, title="Factual")
    target_prob = round(target_probs(ce)[1],digits=2)
    ŷ = CounterfactualExplanations.counterfactual_label(ce)[1]
    p2 = plot(
      convert2image(MNIST, reshape(CounterfactualExplanations.counterfactual(ce),28,28)),
      axis=nothing,title="ŷ=$(ŷ); p̂(y=$(target))=$(target_prob)"
    )
    plt = plot(p1,p2,size=(500,200))
    return plt
end
plts = []
for (i, _x) in enumerate(x)
    plt = build_plot(_x[1],ces[i])
    plts = [plts..., plt]
end
plt = plot(plts...,layout=(length(plts),1),size=(image_size * 2,image_size * length(plts)))
savefig(plt, joinpath(www_path, "mnist_$(factual)to$(target)_latent.png"))
```

![](../www/mnist_9to4_latent.png)

## References