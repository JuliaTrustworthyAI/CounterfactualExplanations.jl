<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Loss functions ¬∑ CounterfactualExplanations.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://pat-alt.github.io/CounterfactualExplanations.jl/contributing/loss/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CounterfactualExplanations.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../cats_dogs/">Motivating example</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/">Overview</a></li><li><a class="tocitem" href="../../tutorials/binary/">Binary target</a></li><li><a class="tocitem" href="../../tutorials/models/">Custom models</a></li><li><a class="tocitem" href="../../tutorials/multi/">Multi-class target</a></li><li><a class="tocitem" href="../../tutorials/generators/">Custom generators</a></li><li><a class="tocitem" href="../../tutorials/mutability/">Mutability constraints</a></li><li><a class="tocitem" href="../../tutorials/interop/">Interoperability</a></li></ul></li><li><span class="tocitem">Counterfactual Generators</span><ul><li><a class="tocitem" href="../../generators/gradient_based/latent_space_generator/">Latent Space Search</a></li></ul></li><li><span class="tocitem">More examples</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Image data</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/image/MNIST/">MNIST</a></li></ul></li></ul></li><li><span class="tocitem">Contributor&#39;s Guide</span><ul><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../interop/">Interoperability</a></li><li class="is-active"><a class="tocitem" href>Loss functions</a><ul class="internal"><li><a class="tocitem" href="#Classification"><span>Classification</span></a></li><li><a class="tocitem" href="#Regression"><span>Regression</span></a></li><li class="toplevel"><a class="tocitem" href="#Methodological-background"><span>Methodological background</span></a></li><li><a class="tocitem" href="#General-setup"><span>General setup</span></a></li><li><a class="tocitem" href="#Loss-function-‚Ñì"><span>Loss function ‚Ñì</span></a></li><li><a class="tocitem" href="#Example-in-2D"><span>Example in 2D</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Contributor&#39;s Guide</a></li><li class="is-active"><a href>Loss functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Loss functions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/pat-alt/CounterfactualExplanations.jl/blob/main/docs/src/contributing/loss.md#" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Loss-functions"><a class="docs-heading-anchor" href="#Loss-functions">Loss functions</a><a id="Loss-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-functions" title="Permalink"></a></h1><p>For the computation of loss functions and their gradients we leverage the functionality already implmented in <a href="https://fluxml.ai/">Flux</a>. All of the loss functions from Flux have been imported:</p><pre><code class="language-julia-repl hljs">julia&gt; names(CounterfactualExplanations.Losses)
19-element Vector{Symbol}:
 :Losses
 :binary_focal_loss
 :binarycrossentropy
 :crossentropy
 :ctc_loss
 :dice_coeff_loss
 :focal_loss
 :hinge_loss
 ‚ãÆ
 :logitbinarycrossentropy
 :logitcrossentropy
 :mae
 :mse
 :msle
 :poisson_loss
 :squared_hinge_loss
 :tversky_loss</code></pre><h2 id="Classification"><a class="docs-heading-anchor" href="#Classification">Classification</a><a id="Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Classification" title="Permalink"></a></h2><p>For most classification tasks the default <code>:logitbinarycrossentropy</code> (binary) and <code>:logitcrossentropy</code> should be sufficient. For both choices the package has been tested and works natively. When using other loss functions, some caution is recommended though:</p><div class="admonition is-warning"><header class="admonition-header">External loss functions</header><div class="admonition-body"><p>Some margin-based loss functions like hinge loss do not expect inputs in the domain <span>$\mathcal{Y}=\{0,1\}$</span>, but rather <span>$\mathcal{Y}=\{-1,1\}$</span>. In those case one needs to ensure that the training labels <span>$y$</span> are encoded accordingly. In order to use distance-based loss functions like mean squared error (MSE) loss needs to be computed with respect to probibilities rather than logits. This is currently not supported and we genenerally recommend not to use distance-based loss functions in the classification setting (more on this below).</p></div></div><h2 id="Regression"><a class="docs-heading-anchor" href="#Regression">Regression</a><a id="Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Regression" title="Permalink"></a></h2><p>At this point <code>CounterfactualExplanations.jl</code> is designed to be used with classification models, since the overwhelming majority of the existing literature on counterfactual explanations is set in this context. By default margin-based loss functions are used and computed with respect to logits (more on this below). To produce counterfactual explanations for regression problems users currently need to binarize the problem: let <em>t</em> denote some target value for the continuous dependent variable <em>y</em> in the regression context, then we could respecify the dependent variable as <em>tÃÉ</em>‚ÄÑ=‚ÄÑ0 for all <em>y</em>‚ÄÑ\&lt;‚ÄÑ<em>t</em> and <em>tÃÉ</em>‚ÄÑ=‚ÄÑ1 otherwise. In future work we want to add full support for regression problems.</p><h1 id="Methodological-background"><a class="docs-heading-anchor" href="#Methodological-background">Methodological background</a><a id="Methodological-background-1"></a><a class="docs-heading-anchor-permalink" href="#Methodological-background" title="Permalink"></a></h1><p>This is a short tutorial on loss functions and gradients typically involved in counterfactual search. It involves more maths than perhaps some of the other tutorials.</p><h2 id="General-setup"><a class="docs-heading-anchor" href="#General-setup">General setup</a><a id="General-setup-1"></a><a class="docs-heading-anchor-permalink" href="#General-setup" title="Permalink"></a></h2><p>We begin by restating the general setup for generic counterfactual search. Let <em>t</em>‚ÄÑ‚àà‚ÄÑ{0,‚ÄÜ1} denote the target label, <em>M</em> the model (classifier) and <em>x</em>‚Ä≤‚ÄÑ‚àà‚ÄÑ‚Ñù^(<em>D</em>) the vector of counterfactual features (we will assume all features are continuous). Then the differentiable optimization problem in algorithmic recourse is generally of the following form</p><p class="math-container">\[x\prime = \arg \min_{x\prime}  \ell(M(x\prime),t) + \lambda h(x\prime)\]</p><p>where ‚Ñì denotes some loss function targeting the deviation between the target label and the predicted label and <em>h</em>(‚ãÖ) acts as a complexity penality generally addressing the <em>realism</em> or <em>cost</em> of the proposed counterfactual.</p><h2 id="Loss-function-‚Ñì"><a class="docs-heading-anchor" href="#Loss-function-‚Ñì">Loss function ‚Ñì</a><a id="Loss-function-‚Ñì-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function-‚Ñì" title="Permalink"></a></h2><p>Different choices for ‚Ñì come to mind, each potentially leading to very different counterfactual outcomes. In practice, ‚Ñì is often implemented with respect to the <em>logits</em> <em>a</em>‚ÄÑ=‚ÄÑ<strong>w</strong>^(<em>T</em>)<em>x</em> rather than the probabilities <em>p</em>(<em>y</em>‚Ä≤=1|<em>x</em>‚Ä≤)‚ÄÑ=‚ÄÑ<em>œÉ</em>(<em>a</em>) predicted by the classifier. We follow this convention here, but as we shall see <em>depeding on the label domain this convention does not work well for every type of loss function</em>. Common choices for ‚Ñì in the literature include margin-based loss function like <strong>hinge</strong> loss and <strong>logit binary crossentropy</strong> (or <strong>log</strong>) loss. Some use distance-based loss such as <strong>mean squared error</strong> loss (MSE).</p><h3 id="Hinge-loss"><a class="docs-heading-anchor" href="#Hinge-loss">Hinge loss</a><a id="Hinge-loss-1"></a><a class="docs-heading-anchor-permalink" href="#Hinge-loss" title="Permalink"></a></h3><p>With respect to the logits <em>a</em>‚ÄÑ=‚ÄÑ<strong>w</strong>‚Ä≤<em>x</em> hinge loss can be defined as follows</p><p class="math-container">\[\ell(a,t^*)=(1-a\cdot t^*)_+=\max\{0,1-a\cdot t^*\}\]</p><p>where <em>t</em>^(*) is the target label in {‚ÄÖ‚àí‚ÄÖ1,‚ÄÜ1}. Since above we defined <em>t</em>‚ÄÑ‚àà‚ÄÑ{0,‚ÄÜ1} we need a mapping <em>h</em>‚ÄÑ:‚ÄÑ{0,‚ÄÜ1}‚ÄÑ‚Ü¶‚ÄÑ{‚ÄÖ‚àí‚ÄÖ1,‚ÄÜ1}. Specifically, we want to plug in <em>h</em>(<em>t</em>)‚ÄÑ=‚ÄÑ<em>t</em>^(*) where <em>h</em>(‚ãÖ) is just the following conditional:</p><p class="math-container">\[\begin{aligned}
h(t)&amp;=\begin{cases}
-1 &amp;&amp; \text{if} &amp;&amp; t=0 \\ 1 &amp;&amp; \text{if} &amp;&amp; t=1
\end{cases}
\end{aligned}\]</p><p>Then our loss function as function of <em>t</em> can restated as follows:</p><p class="math-container">\[\ell(a,t^*)=\ell(a,t)=(1-a\cdot h(t))_+=\max\{0,1-a\cdot h(t)\}\]</p><p>The first-order derivative of hinge loss with respect to the logits <em>a</em> is simply</p><p class="math-container">\[\begin{aligned}
\ell&#39;(a,t)&amp;=\begin{cases}
-h(t) &amp;&amp; \text{if} &amp;&amp; a \cdot h(t)&lt;=1 \\ 0 &amp;&amp; \text{otherwise.} 
\end{cases}
\end{aligned}\]</p><p>In the context of counterfactual search the gradient with respect to the feature vector is then:</p><p class="math-container">\[\begin{aligned}
&amp;&amp; \nabla_{x\prime} \ell(a,t)&amp;= \begin{cases}
-h(t)\mathbf{w} &amp;&amp; \text{if} &amp;&amp; h(t)\mathbf{w}^Tx\prime&lt;=1 \\ 0 &amp;&amp; \text{otherwise.} 
\end{cases}
\end{aligned}\]</p><p>In practice gradients are commonly computed through autodifferentiation. In this tutorial we use the <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a> package which is at the core of <a href="https://fluxml.ai/Flux.jl/stable/models/basics/">Flux.jl</a>, the main deep learning library for Julia.</p><p>The side-by-side plot below visualises the loss function and its derivative. The plot further below serves as a simple sanity check to verify that autodifferentiation indeed yields the same result as the closed-form solution for the gradient.</p><pre><code class="language-julia hljs">h(t) = ifelse(t==1,1,-1)
hinge(a,t) = max(0,1-a*h(t))</code></pre><pre><code class="language-julia hljs">default(size=(500,500))
a = -2:0.05:2
p1 = plot(a, [hinge(a,1) for a=a], title=&quot;Loss, t=1&quot;, xlab=&quot;logits&quot;)
p2 = plot(a, [gradient(hinge,a,1)[1] for a=a], title=&quot;Gradient, t=1&quot;, xlab=&quot;logits&quot;)
p3 = plot(a, [hinge(a,0) for a=a], title=&quot;Loss, t=0&quot;, xlab=&quot;logits&quot;)
p4 = plot(a, [gradient(hinge,a,0)[1] for a=a], title=&quot;Gradient, t=0&quot;, xlab=&quot;logits&quot;)
plot(p1, p2, p3, p4, layout = (2, 2), legend = false)
savefig(joinpath(www_path, &quot;loss_grad_hinge.png&quot;))</code></pre><p><img src="../www/loss_grad_hinge.png" alt/></p><pre><code class="language-julia hljs"># Just verifying that the formula for the gradient above indeed yields the same result.
function gradient_man(x,w,t)
    ùê† = ifelse(h(t)*w&#39;x&lt;=1, -h(t)*w, 0)
    return ùê†
end;
plot(a, [gradient_man(a,1,1) for a=a], legend=:bottomright, label=&quot;Manual&quot;, title=&quot;Gradient&quot;, xlab=&quot;logits&quot;)
scatter!(a, [gradient(hinge,a,1)[1] for a=a], label=&quot;Autodiff&quot;)
savefig(joinpath(www_path, &quot;loss_grad_hinge_test.png&quot;))</code></pre><p><img src="../www/loss_grad_hinge_test.png" alt/></p><h3 id="Logit-binary-crossentropy-loss"><a class="docs-heading-anchor" href="#Logit-binary-crossentropy-loss">Logit binary crossentropy loss</a><a id="Logit-binary-crossentropy-loss-1"></a><a class="docs-heading-anchor-permalink" href="#Logit-binary-crossentropy-loss" title="Permalink"></a></h3><p>Logit binary crossentropy loss loss (sometimes referred to as log loss) is defined as follows:</p><p class="math-container">\[\begin{aligned}
&amp;&amp; \ell(a,t)&amp;=- \left( t \cdot \log(\sigma(a)) + (1-t) \cdot \log (1-\sigma(a)) \right) \\
\end{aligned}\]</p><p>where <em>œÉ</em>(<em>a</em>) is the logit/sigmoid link function.</p><p>Once again for the purpose of counter factual search we are interested in the first-order derivative with respect to our feature vector <em>x</em>‚Ä≤. You can verify that the partial derivative with respect to feature <em>x</em>‚Ä≤_(<em>d</em>) is as follows:</p><p class="math-container">\[\begin{aligned}
&amp;&amp; \frac{\partial \ell(a,t)}{\partial x\prime_d}&amp;= (\sigma(a) - t) w_d \\
\end{aligned}\]</p><p>The gradient just corresponds to the stacked vector of partial derivatives:</p><p class="math-container">\[\begin{aligned}
&amp;&amp; \nabla_{x\prime} \ell(a,t)&amp;= (\sigma(a) - t) \mathbf{w} \\
\end{aligned}\]</p><p>As before implementation below is done through autodifferentiation. As before the side-by-side plot shows the resulting loss function and its gradient and the plot further below is a simple sanity check.</p><pre><code class="language-julia hljs"># sigmoid function:
function ùõî(a)
    trunc = 8.0 # truncation to avoid numerical over/underflow
    a = clamp.(a,-trunc,trunc)
    p = exp.(a)
    p = p ./ (1 .+ p)
    return p
end

# Logit binary crossentropy:
logitbinarycrossentropy(a, t) = - (t * log(ùõî(a)) + (1-t) * log(1-ùõî(a)))</code></pre><p><img src="../www/loss_grad_log.png" alt/></p><pre><code class="language-julia hljs">p1 = plot(a, [logitbinarycrossentropy(a,1) for a=a], title=&quot;Loss, t=1&quot;, xlab=&quot;logits&quot;)
p2 = plot(a, [gradient(logitbinarycrossentropy,a,1)[1] for a=a], title=&quot;Gradient, t=1&quot;, xlab=&quot;logits&quot;)
p3 = plot(a, [logitbinarycrossentropy(a,0) for a=a], title=&quot;Loss, t=0&quot;, xlab=&quot;logits&quot;)
p4 = plot(a, [gradient(logitbinarycrossentropy,a,0)[1] for a=a], title=&quot;Gradient, t=0&quot;, xlab=&quot;logits&quot;)
plot(p1, p2, p3, p4, layout = (2, 2), legend = false)
savefig(joinpath(www_path, &quot;loss_grad_log.png&quot;))</code></pre><p><img src="../www/loss_grad_log_test.png" alt/></p><pre><code class="language-julia hljs"># Just verifying that the formula for the gradient above indeed yields the same result.
function gradient_man(x,w,y)
    ùê† = (ùõî(w&#39;x) - y) .* w
    return ùê†
end;
plot(a, [gradient_man(a,1,1) for a=a], legend=:bottomright, label=&quot;Manual&quot;, title=&quot;Gradient&quot;, xlab=&quot;logits&quot;)
scatter!(a, [gradient(logitbinarycrossentropy,a,1)[1] for a=a], label=&quot;Autodiff&quot;)
savefig(joinpath(www_path, &quot;loss_grad_log_test.png&quot;))</code></pre><h3 id="Mean-squared-error"><a class="docs-heading-anchor" href="#Mean-squared-error">Mean squared error</a><a id="Mean-squared-error-1"></a><a class="docs-heading-anchor-permalink" href="#Mean-squared-error" title="Permalink"></a></h3><p>Some authors work with distance-based loss functions instead. Since in general we are interested in providing valid recourse, that is counterfactual explanations that indeed lead to the desired label switch, using one of the margin-based loss functions introduced above seems like a more natural choice. Nonetheless, we shall briefly introduce one of the common distance-based loss functions as well.</p><p>The mean squared error for counterfactual search implemented with respect to the logits is simply the squared ‚Ñì¬≤ norm between the target label and <em>a</em>‚ÄÑ=‚ÄÑ<strong>w</strong>^(<em>T</em>)<em>x</em>:</p><p class="math-container">\[\begin{aligned}
&amp;&amp; \ell(a,t)&amp;= ||t-a||^2
\end{aligned}\]</p><p>The gradient with respect to the vector of features is then:</p><p class="math-container">\[\begin{aligned}
&amp;&amp; \nabla_{x\prime} \ell(a,t)&amp;= 2(a - t) \mathbf{w} \\
\end{aligned}\]</p><p>As before implementation and visualizations follow below.</p><pre><code class="language-julia hljs">mse(a,t) = norm(t - a)^2</code></pre><p><strong>NOTE</strong>: I hinted above that the convention of taking derivatives with respect to logits can go wrong depending on the loss function we choose. The plot below demonstrates this point: for <em>t</em>‚ÄÑ=‚ÄÑ0 the global minimum of the MSE is of course also at 0. The implication for counterfactual search is that for <em>t</em>‚ÄÑ=‚ÄÑ0 the search stops when <strong>w</strong>^(<em>T</em>)<em>x</em>‚Ä≤‚ÄÑ=‚ÄÑ0. But at this point <em>œÉ</em>(<strong>w</strong>^(<em>T</em>)<em>x</em>‚Ä≤)‚ÄÑ=‚ÄÑ0.5, in other words we stop right at the decision boundary, but never cross it. We will see an example of this below. Key takeaway: carefully think about the choice of your loss function and <strong>DON‚ÄôT</strong> use distance-based loss functions when optimizing with respect to logits.</p><pre><code class="language-julia hljs">p1 = plot(a, [mse(a,1) for a=a], title=&quot;Loss, t=1&quot;, xlab=&quot;logits&quot;)
p2 = plot(a, [gradient(mse,a,1)[1] for a=a], title=&quot;Gradient, t=1&quot;, xlab=&quot;logits&quot;)
p3 = plot(a, [mse(a,0) for a=a], title=&quot;Loss, t=0&quot;, xlab=&quot;logits&quot;)
p4 = plot(a, [gradient(mse,a,0)[1] for a=a], title=&quot;Gradient, t=0&quot;, xlab=&quot;logits&quot;)
plot(p1, p2, p3, p4, layout = (2, 2), legend = false)
savefig(joinpath(www_path, &quot;loss_grad_mse.png&quot;))</code></pre><p><img src="../www/loss_grad_mse.png" alt/></p><pre><code class="language-julia hljs"># Just verifying that the formula for the gradient above indeed yields the same result.
function gradient_man(x,w,y)
    ùê† = 2*(w&#39;x - y) .* w
    return ùê†
end;
plot(a, [gradient_man(a,1,1) for a=a], legend=:bottomright, label=&quot;Manual&quot;, title=&quot;Gradient&quot;, xlab=&quot;logits&quot;)
scatter!(a, [gradient(mse,a,1)[1] for a=a], label=&quot;Autodiff&quot;)
savefig(joinpath(www_path, &quot;loss_grad_mse_test.png&quot;))</code></pre><p><img src="../www/loss_grad_mse_test.png" alt/></p><h2 id="Example-in-2D"><a class="docs-heading-anchor" href="#Example-in-2D">Example in 2D</a><a id="Example-in-2D-1"></a><a class="docs-heading-anchor-permalink" href="#Example-in-2D" title="Permalink"></a></h2><p>To understand the properties of the different loss functions we will now look at a toy example in 2D. The code below generates some random features and assigns labels based on a fixed vector of coefficients using the sigmoid function.</p><pre><code class="language-julia hljs"># Some random data:
using Flux, Random, CounterfactualExplanations.Data
Random.seed!(1234)
N = 25
w = [1.0 1.0]# true coefficients
b = 0
xs, ys = Data.toy_data_linear(N)
X = hcat(xs...)
counterfactual_data = CounterfactualData(X,ys&#39;)</code></pre><p>The plot below shows the samples coloured by label along with the decision boundary. You can think of this as representing the outcome of some automated decision making system. The highlighted sample was chosen to receive algorithmic recourse in the following: we will search for a counterfactual that leads to a label switch.</p><pre><code class="language-julia hljs">using CounterfactualExplanations
using CounterfactualExplanations.Models: LogisticModel
M = LogisticModel(w, [b])

Random.seed!(1234)
x = select_factual(counterfactual_data,rand(1:size(X)[2]))
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0)
Œ≥ = 0.75

# Plot with random sample chose for recourse
plt = plot_contour(X&#39;,ys,M)
scatter!(plt,[x[1]],[x[2]],ms=10,label=&quot;&quot;, color=Int(y))
Plots.abline!(plt,-w[2]/w[1],b,color=&quot;black&quot;,label=&quot;&quot;,lw=2)
savefig(joinpath(www_path, &quot;loss_example.png&quot;))</code></pre><p><img src="../www/loss_example.png" alt/></p><p>Now we instantiate different generators for our different loss functions and different choices of <em>Œª</em>. Finally we generate recourse for each of them:</p><pre><code class="language-julia hljs"># Generating recourse
Œõ = [0.0, 1.0, 5.0] # varying complexity penalties
losses = [:hinge_loss, :logitbinarycrossentropy, :mse]
counterfactuals = []
for loss in losses
    for Œª in Œõ
        generator = GenericGenerator(;loss=loss,Œª=Œª) 
        t = loss == :hinge_loss ? h(target) : target # mapping for hinge loss
        counterfactual = generate_counterfactual(x, t, counterfactual_data, M, generator; Œ≥=Œ≥, T=50)
        counterfactuals = vcat(counterfactuals, counterfactual)
    end
end</code></pre><p>The code below plots the resulting counterfactual paths.</p><ol><li><strong>Complexity penalty</strong> (<em>Œª</em>): has the expected effect of penalizing <em>long</em> counterfactual paths: as the distance between <em>x</em> and <em>x</em>‚Ä≤ the penalty exerts more and more pressure on the gradient in the opposite direction ‚àá‚Ñì. For large choices of <em>Œª</em> valid recourse is not attainable.</li><li><strong>Confidence threshold</strong> (<em>Œ≥</em>): note how for both log loss and hinge loss we overshoot a bit, that is we end up well beyond the decision boundary. This is because above we chose a confidence threshold of <em>Œ≥</em>‚ÄÑ=‚ÄÑ0.75. In the context of recourse this choice matters a lot: we have a longer distance to travel (=higher costs for the individual), but we can be more confident that recourse will remain valid. There is of course an interplay between <em>Œª</em> and <em>Œ≥</em>.</li><li><strong>The choice of the loss function matters</strong>: the distance-based MSE does <strong>NOT</strong> work without further ajustments when optimizing with respect to logits, as discussed above.</li></ol><pre><code class="language-julia hljs"># Plotting
k = length(counterfactuals)
function plot_recourse(counterfactual, t)
    l = string(counterfactual.generator.loss)
    l = l[1:minimum([5,length(l)])]
    Œª = string(counterfactual.generator.Œª)
    plt = plot_contour(X&#39;,ys,M;colorbar=false,title=&quot;Loss: $(l), Œª: $(Œª)&quot;)
    plt = plot(plt, size=(floor(‚àö(k)) * 350, ceil(‚àö(k)) * 350))
    Plots.abline!(plt,-w[2]/w[1],b,color=&quot;black&quot;,label=&quot;&quot;,lw=2)
    t = minimum([t, total_steps(counterfactual)])
    scatter!(plt, hcat(path(counterfactual)[1:t]...)[1,:], hcat(path(counterfactual)[1:t]...)[2,:], ms=10, color=Int(y), label=&quot;&quot;)
    return plt
end
max_path_length = maximum(map(counterfactual -&gt; total_steps(counterfactual), counterfactuals))
anim = @animate for i in 1:max_path_length
    plots = map(counterfactual -&gt; plot_recourse(counterfactual, i), counterfactuals)
    plot(plots..., layout = (Int(floor(‚àö(k))), Int(ceil(‚àö(k)))), legend = false, plot_title=&quot;Iteration: &quot; * string(i))
end
gif(anim, joinpath(www_path, &quot;loss_paths.gif&quot;), fps=5)</code></pre><p><img src="../www/loss_paths.gif" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../interop/">¬´ Interoperability</a><a class="docs-footer-nextpage" href="../../reference/">Reference ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Friday 24 June 2022 06:57">Friday 24 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
