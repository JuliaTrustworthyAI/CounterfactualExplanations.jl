---
title: "Explaining black-box models through counterfactuals"
author: Patrick Altmeyer
format: 
  revealjs:
    logo: www/afr.svg
    footer: "Explaining black-box models through counterfactuals"
    self-contained: true
    smaller: true
    scrollable: true
bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib
execute:
  eval: false
  echo: true
  fig-align: "center"
---

## Overview

- Intro
- Background and related work
- Counterfactual Explanations
- `CounterfactualExplanations.jl`: getting started
- Possible research questions
- Application to MNIST
- Discussion and Outlook

# Introduction

## Have you ever ...

:::{.incremental}
- ... received an automated rejection email? Why didn't you "mEet tHe sHoRtLisTiNg cRiTeRia"? ðŸ™ƒ
- ... used deep learning or some other black-box model? ðŸ”® Could you explain the model behvaiour intuitively? ðŸ‘€
- ... used a black-box model at ING to classify counterparties or clients? ðŸ¦
- ... worked for the belastingdienst? ðŸ« 
:::

## The need for explanations

- From human to data-driven decision-making:
  - Today, it is more likely than not that your digital loan or employment application will be handled by an algorithm, at least in the first instance.
- Black-box models create undesirable dynamics: 
  - Human operators in charge of the system have to rely on it blindy.
  - Those indviduals subject to it generally have no way to challenge an outcome.

> â€œYou cannot appeal to (algorithms). They do not listen. Nor do they bend.â€
>
> â€” Cathy O'Neil in [*Weapons of Math Destruction*](https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction), 2016

## Enter: counterfactual explanations 

#### From ðŸ± to ðŸ¶ 

We have fitted some black box classifier to divide cats and dogs. One ðŸ± is friends with a lot of cool ðŸ¶  and wants to remain part of that group. The counterfactual path below shows her how to fool the classifier:

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/www/recourse_laplace.gif){fig-align="center"}

## Limited software availability  

- Some of the existing approaches scattered across different GitHub repositories (ðŸ).
- Only one unifiying Python ðŸ library: CARLA [@pawelczyk2021carla].
    - Comprehensive and (somewhat) extensible ...
    - ... but not language-agnostic and some desirable functionality not supported.
- Both R and Julia lacking any kind of implementation. Until now ...

## Enter: `CounterfactualExplanations.jl` ðŸ“¦

[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://pat-alt.github.io/CounterfactualExplanations.jl/dev)
[![Build Status](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl)

- A unifying framework for generating counterfactual explanations and aglorithmic recourse.
- Built in Julia, but essentially language agnostic: supporting explanations for models built in Python, R, ...
- Designed to be easily extensible through multiple dispatch.
- Native support for differentiable models built and trained in Julia. 

> Julia is fast, transparent, beautiful and open ðŸ”´ðŸŸ¢ðŸŸ£

# Background and Related Work

## Explainable AI (XAI)

- *interpretable* = inherently interpretable model, no extra tools needed (GLM, decision trees, rules, ...)  [@rudin2019stop]
- *explainable* = inherently not interpretable model, but explainable through XAI

#### Post-hoc explainability:
- Local **surrogate explainers** like LIME and Shapley: useful and popular, but ... 
    - ... can be easily fooled [@slack2020fooling]
    - ... rely on reasonably interpretable features.
    - ... rely on the concept of fidelity.
- **Counterfactual explanations** explain how inputs into a system need to change for it to produce different decisions. 
    - Always full-fidelity, since no proxy involved. 
    - Intuitive interpretation and straight-forward implemenation.
    - Works well with Bayesian models. Clear link to Causal Inference. 
    - Does not rely on interpretable features.
- Realistic and actionable changes can be used for the purpose of **algorithmic recourse**.

# Counterfactual Explanations

## A framework for counterfactual explanations
 
- Objective originally proposed by @wachter2017counterfactual is as follows where $h$ relates to the complexity of the counterfactual and $M$ denotes the classifier:

$$
\min_{x\prime \in \mathcal{X}} h(x\prime) \ \ \ \mbox{s. t.} \ \ \ M(x\prime) = t
$$ {#eq-obj}

- Typically approximated through regularization:

$$
x\prime = \arg \min_{x\prime}  \ell(M(x\prime),t) + \lambda h(x\prime)
$$ {#eq-solution}

> So counterfactual search is just gradient descent in the feature space ðŸ’¡ Easy right?

## Not so fast ... 

> Effective counterfactuals should meet certain criteria âœ…

- **closeness**: the average distance between factual and counterfactual features should be small (@wachter2017counterfactual)
- **actionability**: the proposed feature perturbation should actually be actionable (@ustun2019actionable, @poyiadzi2020face)
- **plausibility**: the counterfactual explanation should be plausible to a human (@joshi2019towards)
- **unambiguity**: a human should have no trouble assigning a label to the counterfactual (@schut2021generating)
- **sparsity**: the counterfactual explanation should involve as few individual feature changes as possible (@schut2021generating)
- **robustness**: the counterfactual explanation should be robust to domain and model shifts (@upadhyay2021towards)
- **diversity**: ideally multiple diverse counterfactual explanations should be provided (@mothilal2020explaining)
- **causality**: counterfactual explanations reflect the structual causal model underlying the data generating process (@karimi2020algorithmic, @karimi2021algorithmic)

## The Bayesian approach - a catchall?

- @schut2021generating note that different approaches just work with different complexity functions ($h(x\prime)$ in @eq-obj)
- They show that for classifiers $\mathcal{\widetilde{M}}$ that incoporate predictive uncertainty we can drop the complexity penalty altogether:

$$
x\prime = \arg \min_{x\prime}  \ell(M(x\prime),t) \ \ , \ \  \forall M\in\mathcal{\widetilde{M}}
$$ {#eq-solution-bayes}

- See also @antoran2020getting.

# `CounterfactualExplanations.jl`: getting started

## Installation

1. Install Julia.
2. Install the package:

```{julia}
using Pkg
Pkg.add("CounterfactualExplanations")
```

3. Explain your black box ðŸ”®

## A simple generic generator

::: {.panel-tabset}

### Counterfactual path

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/tutorials/www/binary_generic_recourse.gif)

### Code 

```{.julia}
# Data:
using CounterfactualExplanations.Data
Random.seed!(1234)
N = 25
xs, ys = Data.toy_data_linear(N)
X = hcat(xs...)
counterfactual_data = CounterfactualData(X,ys')

# Model
using CounterfactualExplanations.Models: LogisticModel, probs 
# Logit model:
w = [1.0 1.0] # true coefficients
b = 0
M = LogisticModel(w, [b])

# Randomly selected factual:
Random.seed!(123)
x = select_factual(counterfactual_data,rand(1:size(X)[2]))
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target

# Counterfactual search:
generator = GenericGenerator()
counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)
```
:::


## A greedy generator

:::{.panel-tabset}

### Counterfactual path

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/tutorials/www/binary_greedy_recourse.gif)

### Code

```{.julia}
# Model:
using LinearAlgebra
Î£ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix
Î¼ = hcat(b, w)
M = CounterfactualExplanations.Models.BayesianLogisticModel(Î¼, Î£)

# Counterfactual search:
generator = GreedyGenerator(;Î´=0.1,n=25))
counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)
```

:::

## Custom models and interoperability

:::{.panel-tabset}

### Counterfactual path

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/tutorials/www/interop_r.gif)

### R

#### Subtyping and dispatch

```{.julia}
using Flux, RCall
using CounterfactualExplanations, CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs # import functions in order to extend

# Step 1)
struct TorchNetwork <: Models.AbstractFittedModel
    nn::Any
end

# Step 2)
function logits(M::TorchNetwork, X::AbstractArray)
  nn = M.nn
  yÌ‚ = rcopy(R"as_array($nn(torch_tensor(t($X))))")
  yÌ‚ = isa(yÌ‚, AbstractArray) ? yÌ‚ : [yÌ‚]
  return yÌ‚'
end
probs(M::TorchNetwork, X::AbstractArray)= Ïƒ.(logits(M, X))
M = TorchNetwork(R"model")
```

#### Gradient access

```{.julia}
import CounterfactualExplanations.Generators: âˆ‚â„“
using LinearAlgebra

# Countefactual loss:
function âˆ‚â„“(generator::AbstractGradientBasedGenerator, counterfactual_state::CounterfactualState) 
  M = counterfactual_state.M
  nn = M.nn
  xâ€² = counterfactual_state.xâ€²
  t = counterfactual_state.target_encoded
  R"""
  x <- torch_tensor($xâ€², requires_grad=TRUE)
  output <- $nn(x)
  obj_loss <- nnf_binary_cross_entropy_with_logits(output,$t)
  obj_loss$backward()
  """
  grad = rcopy(R"as_array(x$grad)")
  return grad
end
```

### Python

#### Subtyping and dispatch

```{.julia}
using Flux, PyCall
using CounterfactualExplanations, CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs # import functions in order to extend

# Step 1)
struct PyTorchNetwork <: Models.AbstractFittedModel
    nn::Any
end

# Step 2)
function logits(M::PyTorchNetwork, X::AbstractArray)
  nn = M.nn
  if !isa(X, Matrix)
    X = reshape(X, length(X), 1)
  end
  yÌ‚ = py"$nn(torch.Tensor($X).T).detach().numpy()"
  yÌ‚ = isa(yÌ‚, AbstractArray) ? yÌ‚ : [yÌ‚]
  return yÌ‚
end
probs(M::PyTorchNetwork, X::AbstractArray)= Ïƒ.(logits(M, X))
M = PyTorchNetwork(py"model")
```

#### Gradient access

```{.julia}
import CounterfactualExplanations.Generators: âˆ‚â„“
using LinearAlgebra

# Countefactual loss:
function âˆ‚â„“(generator::AbstractGradientBasedGenerator, counterfactual_state::CounterfactualState) 
  M = counterfactual_state.M
  nn = M.nn
  xâ€² = counterfactual_state.xâ€²
  t = counterfactual_state.target_encoded
  x = reshape(xâ€², 1, length(xâ€²))
  py"""
  x = torch.Tensor($x)
  x.requires_grad = True
  t = torch.Tensor($[t]).squeeze()
  output = $nn(x).squeeze()
  obj_loss = nn.BCEWithLogitsLoss()(output,t)
  obj_loss.backward()
  """
  grad = vec(py"x.grad.detach().numpy()")
  return grad
end
```

:::

## Custom generators

::: {.panel-tabset}

### Generic generator with dropout

![](www/dropout_recourse.gif)

### Code

#### Subtyping

```{.julia}
# Abstract suptype:
abstract type AbstractDropoutGenerator <: AbstractGradientBasedGenerator end

# Constructor:
struct DropoutGenerator <: AbstractDropoutGenerator
    loss::Symbol # loss function
    complexity::Function # complexity function
    mutability::Union{Nothing,Vector{Symbol}} # mutibility constraints 
    Î»::AbstractFloat # strength of penalty
    Ïµ::AbstractFloat # step size
    Ï„::AbstractFloat # tolerance for convergence
    p_dropout::AbstractFloat # dropout rate
end

# Instantiate:
using LinearAlgebra
generator = DropoutGenerator(
    :logitbinarycrossentropy,
    norm,
    nothing,
    0.1,
    0.1,
    1e-5,
    0.5
)
```

#### Dispatch

```{.julia}
import CounterfactualExplanations.Generators: generate_perturbations, âˆ‡
using StatsBase
function generate_perturbations(generator::AbstractDropoutGenerator, counterfactual_state::CounterfactualState)
    ð â‚œ = âˆ‡(generator, counterfactual_state) # gradient
    # Dropout:
    set_to_zero = sample(1:length(ð â‚œ),Int(round(generator.p_dropout*length(ð â‚œ))),replace=false)
    ð â‚œ[set_to_zero] .= 0
    Î”xâ€² = - (generator.Ïµ .* ð â‚œ) # gradient step
    return Î”xâ€²
end
```

:::

## Feature constraints

::: {.panel-tabset}

### Domain constraint

![](www/mutability_domain_2.gif)

### Code

Mutability constraints can be added at the preprocessing stage:

```{.julia}
counterfactual_data = CounterfactualData(X,ys';domain=[(-Inf,Inf),(-Inf,-0.5)])
```

:::

# Application to MNIST

## Counterfactuals for image data

> This looks nice ðŸ¤“

![Turning a nine (9) into a four (4).](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/examples/image/www/MNIST_9to4.png){fig-align="center"}

> And this ... ugh ðŸ¥´

![Turning a seven (7) into a (2).](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/examples/image/www/MNIST_7to2.png){fig-align="center"}

# Discussion and outlook

## The package ðŸ“¦

- To be submitted to [JuliaCon 2022](https://juliacon.org/2022/) (today ðŸ«£)
- Through the help of community contribution we hope to add:
    - native support for deep learning models (`Flux`, `torch`, `tensorflow`) and other differentiable models.
    - support for non-differentiable models.
    - more generators (DiCE [@mothilal2020explaining], ROAR [@upadhyay2021towards], MINT [@karimi2021algorithmic], CLUE [@antoran2020getting])

## Research topics (1) - student project

> What happens once AR has actually been implemented? ðŸ‘€

:::{.incremental}
- Towards robust AR: protection against exogenous domain and model shifts [@upadhyay2021towards]
- What about endogenous model shifts?
:::

![](www/bayesian.gif){fig-align="center" width=800px} 

## Research topics (2)

:::{.incremental}
- An effortless way to incorporate model uncertainty (w/o need for expensive generative model): *Laplace Redux*.
- Counterfactual explanations for time series data.
- Is CE really more intuitive? Could run a user-based study like in @kaur2020interpreting.
- More ideas form your side? ðŸ¤—
:::

## More resources

- Introductory [blog post: [[TDS](https://towardsdatascience.com/individual-recourse-for-black-box-models-5e9ed1e4b4cc), [homepage](https://www.paltmeyer.com/blog/posts/individual-recourse-for-black-box-models/)]
- Package [docs](https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/) with lots of examples
- Blog post on effortless Bayesian deep learning [[TDS]((https://towardsdatascience.com/go-deep-but-also-go-bayesian-ab25efa6f7b)), [homepage](https://www.paltmeyer.com/blog/posts/effortsless-bayesian-dl/)]
- [Get in touch](https://www.paltmeyer.com/)!

## References 