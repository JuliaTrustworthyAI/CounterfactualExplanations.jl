---
title: "Explaining black box models through counterfactuals"
author: Patrick Altmeyer
format: 
  revealjs:
    logo: www/afr.svg
    footer: "Explaining black box models through counterfactuals"
    self-contained: true
    smaller: true
    scrollable: true
bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib
execute:
  eval: false
  echo: true
---

## Overview

- Motivation
- Methodological background
- Examples in `CounterfactualExplanations.jl`
- Possible research questions

# Motivation

## An unfortunate reality

- From human to data-driven decision-making:
  - Today, it is more likely than not that your digital loan or employment application will be handled by an algorithm, at least in the first instance.
- Black-box models create undesirable dynamics: 
  - Human operators in charge of the system have to rely on it blindy.
  - Those indviduals subject to it generally have no way to challenge an outcome.

> â€œYou cannot appeal to (algorithms). They do not listen. Nor do they bend.â€
>
> â€” Cathy O'Neil in [*Weapons of Math Destruction*](https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction), 2016

## Explainable AI (xAI)

- *interpretable* = inherently interpretable model, no extra tools needed
- *explainable* = inherently not interpretable model, but explainable through xAI

#### Ad-hoc interpretability:
- Just use interpretable models ðŸ˜  ! (GLM, decision trees, rules, ...)  [@rudin2019stop]
- **Proxy methods** construct simple representations of complex models

#### Post-hoc explainability:
- Local **surrogate explainers** like LIME and Shapley: useful and popular, but can be easily fooled [@slack2020fooling]
- **Counterfactual explanations** explain how inputs into a system need to change for it to produce different decisions. 
- Realistic and actionable changes can be used for the purpose of **algorithmic recourse**.

## From ðŸ± to ðŸ¶ 

We have fitted some black box classifier to divide cats and dogs. One ðŸ± is friends with a lot of cool ðŸ¶  and wants to remain part of that group. The counterfactual path below shows her how to fool the classifier:

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/www/recourse_laplace.gif){fig-align="center"}

## On a more serious note ...

:::{.incremental}
- Ever received an automated rejection email? How was the feedback? ðŸ˜• 
- What about loan applications? ðŸ˜”
- Algorithms to evaluate redivisim risk (COMPAS)? ðŸ˜“
- ...
:::

# Methodology

## Generic counterfactual search 
 
- Objective originally proposed by @wachter2017counterfactual is as follows where $h$ relates to the complexity of the counterfactual and $M$ denotes the classifier:

$$
\min_{\tilde{x} \in \mathcal{X}} h(\tilde{x}) \ \ \ \mbox{s. t.} \ \ \ M(\tilde{x}) = t
$$ {#eq-obj}

- Typically approximated through regularization:

$$
\tilde{x} = \arg \min_{\tilde{x}}  \ell(M(\tilde{x}),t) + \lambda h(\tilde{x})
$$ {#eq-solution}

> So counterfactual search is just gradient descent in the feature space ðŸ’¡ Easy right?

## Not so fast ... 

> Effective counterfactuals should meet certain criteria âœ…

- **closeness**: the average distance between factual and counterfactual features should be small (@wachter2017counterfactual)
- **actionability**: the proposed feature perturbation should actually be actionable (@ustun2019actionable, @poyiadzi2020face)
- **plausibility**: the counterfactual explanation should be plausible to a human (@joshi2019towards)
- **unambiguity**: a human should have no trouble assigning a label to the counterfactual (@schut2021generating)
- **sparsity**: the counterfactual explanation should involve as few individual feature changes as possible (@schut2021generating)
- **robustness**: the counterfactual explanation should be robust to domain and model shifts (@upadhyay2021towards)
- **diversity**: ideally multiple diverse counterfactual explanations should be provided (@mothilal2020explaining)
- **causality**: counterfactual explanations reflect the structual causal model underlying the data generating process (@karimi2020algorithmic,@karimi2021algorithmic)

## The Bayesian approach - a catchall?

- @schut2021generating note that different approaches just work with different complexity functions ($h(\tilde{x})$ in @eq-obj)
- They show that for classifiers $\mathcal{\widetilde{M}}$ that incoporate predictive uncertainty we can drop the complexity penalty altogether:

$$
\tilde{x} = \arg \min_{\tilde{x}}  \ell(M(\tilde{x}),t) \ \ , \ \  \forall M\in\mathcal{\widetilde{M}}
$$ {#eq-solution-bayes}

# Examples in `CounterfactualExplanations.jl`

## Why Julia?

> Fast, transparent, beautiful ðŸ”´ðŸŸ¢ðŸŸ£

[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://pat-alt.github.io/CounterfactualExplanations.jl/dev)
[![Build Status](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl)

`CounterfactualExplanations.jl` is a package for generating counterfactual explanations and aglorithmic recourse.

#### Installation

```{julia}
using Pkg
Pkg.add("CounterfactualExplanations")
```

- To be submitted to upcoming JuliaCon 2022, if accepted then publish in proceedings.
- In Python ðŸ, use CARLA [@pawelczyk2021carla].

## Usage example

Using the package, generating counterfactuals is as easy as follows:

```{.julia code-line-numbers="8-16"}
# Some random example:
w = [1.0 -2.0] # true coefficients
b = [0] # true constant
x = [-1,0.5] # factual in class 0
target = 1.0 # target
Î³ = 0.9 # desired confidence

# Declare model:
using CounterfactualExplanations.Models
ð‘´ = LogisticModel(w, b)

# Counterfactual search:
generator = GenericGenerator(
  0.1,0.1,1e-5,:logitbinarycrossentropy,nothing)
recourse = generate_counterfactual(
  generator, x, ð‘´, target, Î³)
```

Designed to work with any custom model and generator through **multiple dispatch**.

## Generic search - plugin MLP

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/tutorials/www/multi_generic_recourse.gif)

## Greedy search - deep ensemble

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/tutorials/www/multi_greedy_recourse.gif)

## MNIST

> This looks nice ðŸ¤“

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/examples/image/www/MNIST_9to4.png){fig-align="center"}

> And this ... ugh ðŸ¥´

![](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/examples/image/www/MNIST_7to2.png){fig-align="center"}

# Research questions

## Dynamics of AR 

> What happens once AR has actually been implemented? ðŸ‘€

:::{.incremental}
- Towards robust AR: protection against exogenous domain and model shifts [@upadhyay2021towards]
- What about endogenous model shifts?
:::

## Endogenous shifts in AR

![](www/bayesian.gif){fig-align="center" width=800px} 

## Other questions

:::{.incremental}
- To what extent is the effectiveness of CE dependent on the quality of the classifier?
- Is CE really more intuitive? Could run a user-based study like in @kaur2020interpreting
- Counterfactual explanations for time series data? 
- More ideas form your side? ðŸ¤—
:::

## More resources

- Introductory [blog post: [[TDS](https://towardsdatascience.com/individual-recourse-for-black-box-models-5e9ed1e4b4cc),[homepage](https://www.paltmeyer.com/blog/posts/individual-recourse-for-black-box-models/)]
- Package [docs](https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/) with lots of examples
- Blog post on effortless Bayesian deep learning [[TDS]((https://towardsdatascience.com/go-deep-but-also-go-bayesian-ab25efa6f7b)), [homepage](https://www.paltmeyer.com/blog/posts/effortsless-bayesian-dl/)]
- [Get in touch](https://www.paltmeyer.com/)!

## References 