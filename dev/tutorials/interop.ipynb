{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "format:\n",
        "  commonmark:\n",
        "    variant: '-raw_html'\n",
        "    wrap: none\n",
        "    self-contained: true\n",
        "crossref:\n",
        "  fig-prefix: Figure\n",
        "  tbl-prefix: Table\n",
        "bibliography: 'https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib'\n",
        "execute:\n",
        "  echo: true\n",
        "  eval: false\n",
        "---"
      ],
      "id": "b511d131"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```@meta\n",
        "CurrentModule = CounterfactualExplanations \n",
        "```\n",
        "\n",
        "# Interoperability\n",
        "\n",
        "The Julia language offers unique support for programming language interoperability. For example, calling Python and R is made remarkably easy through `PyCall.jl` and `RCall.jl`. In this tutorial we will see how `CounterfactualExplanations.jl` leverages this functionality. In particular, we will see that through minimal extra effort the package can be used to explain models that were developed in train in Python or R.\n",
        "\n",
        "!!! warning \"Experimental feature\"\n",
        "    Our work on language interoperability is still in its early stages. What follows is a proof-of-concept."
      ],
      "id": "523be390"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "using Plots, PlotThemes\n",
        "theme(:wong)"
      ],
      "id": "e6ac1865",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get started we will first load some two-dimensional toy data:"
      ],
      "id": "130d3c9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Random\n",
        "# Some random data:\n",
        "Random.seed!(1234);\n",
        "N = 100\n",
        "using CounterfactualExplanations\n",
        "using CounterfactualExplanations.Data\n",
        "x, y = toy_data_non_linear(N)\n",
        "X = hcat(x...)"
      ],
      "id": "482c34a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `torch` model trained in R\n",
        "\n",
        "The code below builds a simple MLP in R:"
      ],
      "id": "a2f4c2f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using RCall\n",
        "R\"\"\"\n",
        "# Data\n",
        "library(torch)\n",
        "X <- torch_tensor(t($X))\n",
        "y <- torch_tensor($y)\n",
        "\n",
        "# Model:\n",
        "mlp <- nn_module(\n",
        "  initialize = function() {\n",
        "    self$layer1 <- nn_linear(2, 32)\n",
        "    self$layer2 <- nn_linear(32, 1)\n",
        "  },\n",
        "  forward = function(input) {\n",
        "    input <- self$layer1(input)\n",
        "    input <- nnf_sigmoid(input)\n",
        "    input <- self$layer2(input)\n",
        "    input\n",
        "  }\n",
        ")\n",
        "model <- mlp()\n",
        "optimizer <- optim_adam(model$parameters, lr = 0.1)\n",
        "loss_fun <- nnf_binary_cross_entropy_with_logits\n",
        "\"\"\""
      ],
      "id": "f2c9bdd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code trains the MLP for the binary prediction task at hand:"
      ],
      "id": "dcabdf93"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "R\"\"\"\n",
        "for (epoch in 1:100) {\n",
        "\n",
        "  model$train()  \n",
        "\n",
        "  # Compute prediction and loss:\n",
        "  output <- model(X)[,1]\n",
        "  loss <- loss_fun(output, y)\n",
        "\n",
        "  # Backpropagation:\n",
        "  optimizer$zero_grad()\n",
        "  loss$backward()\n",
        "  optimizer$step()\n",
        "  \n",
        "  cat(sprintf(\"Loss at epoch %d: %7f\\n\", epoch, loss$item()))\n",
        "}\n",
        "\"\"\""
      ],
      "id": "4c2d15f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Making the model compatible\n",
        "\n",
        "As always we need to extend the `logits` and `probs` functions to make the model compatible with `CounterfactualExplanations.jl`. As evident from the code below, this is actually quite straight-forward: the logits are returned by the `torch` model and copied form R into the Julia environment. Probabilities are then computed in Julia, by passing the logits through the sigmoid function."
      ],
      "id": "7f441a72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Flux\n",
        "using CounterfactualExplanations, CounterfactualExplanations.Models\n",
        "import CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n",
        "\n",
        "# Step 1)\n",
        "struct TorchNetwork <: Models.FittedModel\n",
        "    nn::Any\n",
        "end\n",
        "\n",
        "# Step 2)\n",
        "function logits(𝑴::TorchNetwork, X::AbstractArray)\n",
        "  nn = 𝑴.nn\n",
        "  ŷ = rcopy(R\"as_array($nn(torch_tensor(t($X))))\")\n",
        "  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]\n",
        "  return ŷ\n",
        "end\n",
        "probs(𝑴::TorchNetwork, X::AbstractArray)= σ.(logits(𝑴, X))\n",
        "𝑴 = TorchNetwork(R\"model\")"
      ],
      "id": "db70d75c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adapting the generator\n",
        "\n",
        "Next we need to do a tiny bit of work on the `Generator` side. By default methods underlying the `GenericGenerator` are desiged to work with models that have gradient access through `Zygote.jl`, one of Julia's main autodifferentiation packages. Of course, `Zygote.jl` cannot access the gradients of our `torch` model, so we need to adapt the code slightly. Fortunately, it turns out that all we need to do is extend the function that computes the gradient with respect to the loss function for the generic counterfactual search: `∂ℓ(generator::GenericGenerator, x̲, 𝑴, t)`. In particular, we will extend the function by a method that is specific to the `TorchNetwork` type we defined above. The code below implements this: our new method `∂ℓ` calls R in order to use `torch`'s autodifferentiation functionality for computing the gradient. "
      ],
      "id": "c7d21ffe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import CounterfactualExplanations.Generators: ∂ℓ\n",
        "using LinearAlgebra\n",
        "\n",
        "# Countefactual loss:\n",
        "function ∂ℓ(generator::GenericGenerator, x̲, 𝑴::TorchNetwork, t) \n",
        "  nn = 𝑴.nn\n",
        "  R\"\"\"\n",
        "  x <- torch_tensor($x̲, requires_grad=TRUE)\n",
        "  output <- $nn(x)\n",
        "  obj_loss <- nnf_binary_cross_entropy_with_logits(output,$t)\n",
        "  obj_loss$backward()\n",
        "  \"\"\"\n",
        "  grad = rcopy(R\"as_array(x$grad)\")\n",
        "  return grad\n",
        "end"
      ],
      "id": "83f6f65f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating counterfactuals\n",
        "\n",
        "From here on onwards we use the `CounterfactualExplanations.jl` functionality as always. Below we choose a random sample, define our generic generator and finally run the search:"
      ],
      "id": "e4a04395"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Randomly selected factual:\n",
        "Random.seed!(123)\n",
        "x̅ = x[rand(1:length(x))]\n",
        "y̅ = round(probs(𝑴, x̅)[1])\n",
        "target = ifelse(y̅==1.0,0.0,1.0) # opposite label as target\n",
        "γ = 0.75 # desired level of confidence\n",
        "# Define Generator:\n",
        "generator = GenericGenerator(0.5,0.1,1e-5,:logitbinarycrossentropy,nothing)\n",
        "# Generate recourse:\n",
        "counterfactual = generate_counterfactual(generator, x̅, 𝑴, target, γ)"
      ],
      "id": "d74ae9a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below just generates the animation that shows the counterfactual path."
      ],
      "id": "cb83aaf2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "include(\"docs/src/utils.jl\")\n",
        "using Plots\n",
        "T = size(counterfactual.path)[1]\n",
        "X_path = reduce(hcat,counterfactual.path)\n",
        "ŷ = CounterfactualExplanations.target_probs(probs(counterfactual.𝑴, X_path)',target)\n",
        "p1 = plot_contour(X',y,𝑴;clegend=false, title=\"Posterior predictive - Plugin\")\n",
        "# [scatter!(p1, [counterfactual.path[t][1]], [counterfactual.path[t][2]], ms=5, color=Int(y̅), label=\"\") for t in 1:T]\n",
        "# p1\n",
        "anim = @animate for t in 1:T\n",
        "    scatter!(p1, [counterfactual.path[t][1]], [counterfactual.path[t][2]], ms=5, color=Int(y̅), label=\"\")\n",
        "    p2 = plot(1:t, ŷ[1:t], xlim=(0,T), ylim=(0, 1), label=\"p(y̲=\" * string(target) * \")\", title=\"Validity\", lc=:black)\n",
        "    Plots.abline!(p2,0,γ,label=\"threshold γ\", ls=:dash) # decision boundary\n",
        "    plot(p1,p2,size=(800,400))\n",
        "end\n",
        "gif(anim, \"docs/src/tutorials/www/interop_r.gif\", fps=5)"
      ],
      "id": "3fcc7116",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](www/interop_r.gif)\n",
        "\n",
        "## Training a `torch` model in Python "
      ],
      "id": "c9b9ba6b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using PyCall\n",
        "py\"\"\"\n",
        "# Data\n",
        "import torch\n",
        "from torch import nn\n",
        "X = torch.Tensor($X).T\n",
        "y = torch.Tensor($y)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MLP, self).__init__()\n",
        "    self.model = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(2, 32),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(32, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = self.model(x)\n",
        "    return logits\n",
        "\n",
        "model = MLP()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "loss_fun = nn.BCEWithLogitsLoss()\n",
        "\"\"\""
      ],
      "id": "cc4c3bfb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "py\"\"\"\n",
        "for epoch in range(100):\n",
        "  # Compute prediction and loss:\n",
        "  output = model(X).squeeze()\n",
        "  loss = loss_fun(output, y)\n",
        "  \n",
        "  # Backpropagation:\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"Loss at epoch {epoch+1}: {loss.item():>7f}\")\n",
        "\"\"\""
      ],
      "id": "49078b6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Flux\n",
        "using CounterfactualExplanations, CounterfactualExplanations.Models\n",
        "import CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n",
        "\n",
        "# Step 1)\n",
        "struct PyTorchNetwork <: Models.FittedModel\n",
        "    nn::Any\n",
        "end\n",
        "\n",
        "# Step 2)\n",
        "function logits(𝑴::PyTorchNetwork, X::AbstractArray)\n",
        "  nn = 𝑴.nn\n",
        "  if !isa(X, Matrix)\n",
        "    X = reshape(X, length(X), 1)\n",
        "  end\n",
        "  ŷ = py\"$nn(torch.Tensor($X).T).detach().numpy()\"\n",
        "  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]\n",
        "  return ŷ\n",
        "end\n",
        "probs(𝑴::PyTorchNetwork, X::AbstractArray)= σ.(logits(𝑴, X))\n",
        "𝑴 = PyTorchNetwork(py\"model\")"
      ],
      "id": "f0557d5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import CounterfactualExplanations.Generators: ∂ℓ\n",
        "using LinearAlgebra\n",
        "\n",
        "# Countefactual loss:\n",
        "function ∂ℓ(generator::GenericGenerator, x̲, 𝑴::PyTorchNetwork, t) \n",
        "  nn = 𝑴.nn\n",
        "  x = reshape(x̲, 1, length(x̲))\n",
        "  py\"\"\"\n",
        "  x = torch.Tensor($x)\n",
        "  x.requires_grad = True\n",
        "  t = torch.Tensor($[t]).squeeze()\n",
        "  output = $nn(x).squeeze()\n",
        "  obj_loss = nn.BCEWithLogitsLoss()(output,t)\n",
        "  obj_loss.backward()\n",
        "  \"\"\"\n",
        "  grad = vec(py\"x.grad.detach().numpy()\")\n",
        "  return grad\n",
        "end"
      ],
      "id": "fcedd2fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define Generator:\n",
        "generator = GenericGenerator(0.5,0.1,1e-5,:logitbinarycrossentropy,nothing)\n",
        "# Generate recourse:\n",
        "counterfactual = generate_counterfactual(generator, x̅, 𝑴, target, γ)"
      ],
      "id": "36e54866",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "include(\"docs/src/utils.jl\")\n",
        "using Plots\n",
        "T = size(counterfactual.path)[1]\n",
        "X_path = reduce(hcat,counterfactual.path)\n",
        "ŷ = CounterfactualExplanations.target_probs(probs(counterfactual.𝑴, X_path)',target)\n",
        "p1 = plot_contour(X',y,𝑴;clegend=false, title=\"Posterior predictive - Plugin\")\n",
        "# [scatter!(p1, [counterfactual.path[t][1]], [counterfactual.path[t][2]], ms=5, color=Int(y̅), label=\"\") for t in 1:T]\n",
        "# p1\n",
        "anim = @animate for t in 1:T\n",
        "    scatter!(p1, [counterfactual.path[t][1]], [counterfactual.path[t][2]], ms=5, color=Int(y̅), label=\"\")\n",
        "    p2 = plot(1:t, ŷ[1:t], xlim=(0,T), ylim=(0, 1), label=\"p(y̲=\" * string(target) * \")\", title=\"Validity\", lc=:black)\n",
        "    Plots.abline!(p2,0,γ,label=\"threshold γ\", ls=:dash) # decision boundary\n",
        "    plot(p1,p2,size=(800,400))\n",
        "end\n",
        "gif(anim, \"docs/src/tutorials/www/interop_py.gif\", fps=5)"
      ],
      "id": "e601bdc7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.6",
      "language": "julia",
      "display_name": "Julia 1.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}