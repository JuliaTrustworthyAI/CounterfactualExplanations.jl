<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Model Catalogue · CounterfactualExplanations.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/tutorials/model_catalogue/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="CounterfactualExplanations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CounterfactualExplanations.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">🏠 Home</a></li><li><span class="tocitem">🫣 Tutorials</span><ul><li><a class="tocitem" href="../">Overview</a></li><li><a class="tocitem" href="../simple_example/">Simple Example</a></li><li><a class="tocitem" href="../whistle_stop/">Whiste-Stop Tour</a></li><li><a class="tocitem" href="../data_preprocessing/">Handling Data</a></li><li><a class="tocitem" href="../data_catalogue/">Data Catalogue</a></li><li><a class="tocitem" href="../models/">Handling Models</a></li><li class="is-active"><a class="tocitem" href>Model Catalogue</a><ul class="internal"><li><a class="tocitem" href="#Available-Models"><span>Available Models</span></a></li><li><a class="tocitem" href="#Fitting-Models"><span>Fitting Models</span></a></li><li><a class="tocitem" href="#Fitting-Flux-Models"><span>Fitting Flux Models</span></a></li><li><a class="tocitem" href="#Importing-PyTorch-models"><span>Importing PyTorch models</span></a></li><li><a class="tocitem" href="#Importing-R-torch-models"><span>Importing R torch models</span></a></li><li><a class="tocitem" href="#Tuning-Flux-Models"><span>Tuning Flux Models</span></a></li><li><a class="tocitem" href="#Fitting-and-tuning-MLJ-models"><span>Fitting and tuning MLJ models</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../generators/">Handling Generators</a></li><li><a class="tocitem" href="../evaluation/">Evaluating Explanations</a></li><li><a class="tocitem" href="../benchmarking/">Benchmarking Explanations</a></li><li><a class="tocitem" href="../parallelization/">Parallelization</a></li></ul></li><li><span class="tocitem">🤓 Explanation</span><ul><li><a class="tocitem" href="../../explanation/">Overview</a></li><li><a class="tocitem" href="../../explanation/architecture/">Package Architecture</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Generators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../explanation/generators/overview/">Overview</a></li><li><a class="tocitem" href="../../explanation/generators/generic/">Generic</a></li><li><a class="tocitem" href="../../explanation/generators/clap_roar/">ClaPROAR</a></li><li><a class="tocitem" href="../../explanation/generators/clue/">CLUE</a></li><li><a class="tocitem" href="../../explanation/generators/dice/">DiCE</a></li><li><a class="tocitem" href="../../explanation/generators/feature_tweak/">FeatureTweak</a></li><li><a class="tocitem" href="../../explanation/generators/gravitational/">Gravitational</a></li><li><a class="tocitem" href="../../explanation/generators/greedy/">Greedy</a></li><li><a class="tocitem" href="../../explanation/generators/growing_spheres/">GrowingSpheres</a></li><li><a class="tocitem" href="../../explanation/generators/probe/">PROBE</a></li><li><a class="tocitem" href="../../explanation/generators/revise/">REVISE</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Optimisers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../explanation/optimisers/overview/">Overview</a></li><li><a class="tocitem" href="../../explanation/optimisers/jsma/">JSMA</a></li></ul></li><li><a class="tocitem" href="../../explanation/categorical/">Categorical Features</a></li></ul></li><li><span class="tocitem">🫡 How-To ...</span><ul><li><a class="tocitem" href="../../how_to_guides/">Overview</a></li><li><a class="tocitem" href="../../how_to_guides/custom_generators/">... add custom generators</a></li><li><a class="tocitem" href="../../how_to_guides/custom_models/">... add custom models</a></li></ul></li><li><a class="tocitem" href="../../reference/">🧐 Reference</a></li><li><a class="tocitem" href="../../contribute/">🛠 Contribute</a></li><li><a class="tocitem" href="../../assets/resources/">📚 Additional Resources</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">🫣 Tutorials</a></li><li class="is-active"><a href>Model Catalogue</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Model Catalogue</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/blob/main/docs/src/tutorials/model_catalogue.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Model-Catalogue"><a class="docs-heading-anchor" href="#Model-Catalogue">Model Catalogue</a><a id="Model-Catalogue-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Catalogue" title="Permalink"></a></h1><p>While in general it is assumed that users will use this package to explain their pre-trained models, we provide out-of-the-box functionality to train various simple default models. In this tutorial, we will see how these models can be fitted to <code>CounterfactualData</code>.</p><h2 id="Available-Models"><a class="docs-heading-anchor" href="#Available-Models">Available Models</a><a id="Available-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Available-Models" title="Permalink"></a></h2><p>The <code>standard_models_catalogue</code> can be used to inspect the available default models:</p><pre><code class="language-julia hljs">standard_models_catalogue</code></pre><pre><code class="nohighlight hljs">Dict{Symbol, Any} with 3 entries:
  :Linear       =&gt; Linear
  :DeepEnsemble =&gt; FluxEnsemble
  :MLP          =&gt; FluxModel</code></pre><p>As with the <a href="tutorials/@ref"><code>data_catalogue</code></a>, the dictionary keys correspond to the model names. In this case, the dictionary values are constructors that can be used called on instances of type <code>CounterfactualData</code> to fit the corresponding model. In most cases, users will find it most convenient to use the <a href="tutorials/@ref"><code>fit_model</code></a> API call instead.</p><h2 id="Fitting-Models"><a class="docs-heading-anchor" href="#Fitting-Models">Fitting Models</a><a id="Fitting-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-Models" title="Permalink"></a></h2><p>Models from the standard model catalogue are a core part of the package and thus compatible with all offered counterfactual generators and functionalities.</p><p>The <code>all_models_catalogue</code> can be used to inspect all models offered by the package:</p><pre><code class="language-julia hljs">all_models_catalogue</code></pre><p>However, when using models not included in the <code>standard_models_catalogue</code>, additional caution is advised: they might not be supported by all counterfactual generators or they might not be models native to Julia. Thus, a more thorough reading of their documentation may be necessary to make sure that they are used correctly.</p><h2 id="Fitting-Flux-Models"><a class="docs-heading-anchor" href="#Fitting-Flux-Models">Fitting Flux Models</a><a id="Fitting-Flux-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-Flux-Models" title="Permalink"></a></h2><p>First, let’s load one of the synthetic datasets:</p><pre><code class="language-julia hljs">n = 500
counterfactual_data = load_multi_class(n)</code></pre><p>We could use a Deep Ensemble (Lakshminarayanan, Pritzel, and Blundell 2016) as follows:</p><pre><code class="language-julia hljs">M = fit_model(counterfactual_data, :DeepEnsemble)</code></pre><p>The returned object is an instance of type <code>FluxEnsemble &lt;: AbstractFittedModel</code> and can be used in downstream tasks without further ado. For example, the resulting fit can be visualised using the generic <code>plot()</code> method as:</p><pre><code class="language-julia hljs">plts = []
for target in counterfactual_data.y_levels
    plt = plot(M, counterfactual_data; target=target, title=&quot;p(y=$(target)|x,θ)&quot;)
    plts = [plts..., plt]
end
plot(plts...)</code></pre><p><img src="../model_catalogue_files/figure-commonmark/cell-7-output-1.svg" alt/></p><h2 id="Importing-PyTorch-models"><a class="docs-heading-anchor" href="#Importing-PyTorch-models">Importing PyTorch models</a><a id="Importing-PyTorch-models-1"></a><a class="docs-heading-anchor-permalink" href="#Importing-PyTorch-models" title="Permalink"></a></h2><p>The package supports generating counterfactuals for any neural network that has been previously defined and trained using PyTorch, regardless of the specific architectural details of the model. To generate counterfactuals for a PyTorch model, save the model inside a <code>.pt</code> file and call the following function:</p><pre><code class="language-julia hljs">model_loaded = CounterfactualExplanations.Models.pytorch_model_loader(
    &quot;$(pwd())/docs/src/tutorials/miscellaneous&quot;,
    &quot;neural_network_class&quot;,
    &quot;NeuralNetwork&quot;,
    &quot;$(pwd())/docs/src/tutorials/miscellaneous/pretrained_model.pt&quot;
)</code></pre><p>The method <code>pytorch_model_loader</code> requires four arguments:</p><ol><li>The path to the folder with a <code>.py</code> file where the PyTorch model is defined</li><li>The name of the file where the PyTorch model is defined</li><li>The name of the class of the PyTorch model</li><li>The path to the Pickle file that holds the model weights</li></ol><p>In the above case:</p><ol><li>The file defining the model is inside <code>$(pwd())/docs/src/tutorials/miscellaneous</code></li><li>The name of the <code>.py</code> file holding the model definition is <code>neural_network_class</code></li><li>The name of the model class is NeuralNetwork</li><li>The Pickle file is located at <code>$(pwd())/docs/src/tutorials/miscellaneous/pretrained_model.pt</code></li></ol><p>Though the model file and Pickle file are inside the same directory in this tutorial, this does not necessarily have to be the case.</p><p>The reason why the model file and Pickle file have to be provided separately is that the package expects an already trained PyTorch model as input. It is also possible to define new PyTorch models within the package, but since this is not the expected use of our package, special support is not offered for that. A guide for defining Python and PyTorch classes in Julia through <code>PythonCall.jl</code> can be found <a href="https://cjdoris.github.io/PythonCall.jl/stable/pythoncall-reference/#Create-classes">here</a>.</p><p>Once the PyTorch model has been loaded into the package, wrap it inside the PyTorchModel class:</p><pre><code class="language-julia hljs">model_pytorch = CounterfactualExplanations.Models.PyTorchModel(model_loaded, counterfactual_data.likelihood)</code></pre><p>This model can now be passed into the generators like any other.</p><p>Please note that the functionality for generating counterfactuals for Python models is only available if your Julia version is 1.8 or above. For Julia 1.7 users, we recommend upgrading the version to 1.8 or 1.9 before loading a PyTorch model into the package.</p><h2 id="Importing-R-torch-models"><a class="docs-heading-anchor" href="#Importing-R-torch-models">Importing R torch models</a><a id="Importing-R-torch-models-1"></a><a class="docs-heading-anchor-permalink" href="#Importing-R-torch-models" title="Permalink"></a></h2><div class="admonition is-warning"><header class="admonition-header">Not fully tested</header><div class="admonition-body"><p>Please note that due to the incompatibility between RCall and PythonCall, it is not feasible to test both PyTorch and RTorch implementations within the same pipeline. While the RTorch implementation has been manually tested, we cannot ensure its consistent functionality as it is inherently susceptible to bugs.</p></div></div><p>The CounterfactualExplanations package supports generating counterfactuals for neural networks that have been defined and trained using R torch. Regardless of the specific architectural details of the model, you can easily generate counterfactual explanations by following these steps.</p><h3 id="Saving-the-R-torch-model"><a class="docs-heading-anchor" href="#Saving-the-R-torch-model">Saving the R torch model</a><a id="Saving-the-R-torch-model-1"></a><a class="docs-heading-anchor-permalink" href="#Saving-the-R-torch-model" title="Permalink"></a></h3><p>First, save your trained R torch model as a <code>.pt</code> file using the <code>torch_save()</code> function provided by the R torch library. This function allows you to serialize the model and save it to a file. For example:</p><pre><code class="language-r hljs">torch_save(model, file = &quot;$(pwd())/docs/src/tutorials/miscellaneous/r_model.pt&quot;)</code></pre><p>Make sure to specify the correct file path where you want to save the model.</p><h3 id="Loading-the-R-torch-model"><a class="docs-heading-anchor" href="#Loading-the-R-torch-model">Loading the R torch model</a><a id="Loading-the-R-torch-model-1"></a><a class="docs-heading-anchor-permalink" href="#Loading-the-R-torch-model" title="Permalink"></a></h3><p>To import the R torch model into the CounterfactualExplanations package, use the <code>rtorch_model_loader()</code> function. This function loads the model from the previously saved <code>.pt</code> file. Here is an example of how to load the R torch model:</p><pre><code class="language-julia hljs">model_loaded = CounterfactualExplanations.Models.rtorch_model_loader(&quot;$(pwd())/docs/src/tutorials/miscellaneous/r_model.pt&quot;)</code></pre><p>The <code>rtorch_model_loader()</code> function requires only one argument:</p><ol><li><code>model_path</code>: The path to the <code>.pt</code> file that contains the trained R torch model.</li></ol><h3 id="Wrapping-the-R-torch-model"><a class="docs-heading-anchor" href="#Wrapping-the-R-torch-model">Wrapping the R torch model</a><a id="Wrapping-the-R-torch-model-1"></a><a class="docs-heading-anchor-permalink" href="#Wrapping-the-R-torch-model" title="Permalink"></a></h3><p>Once the R torch model has been loaded into the package, wrap it inside the <code>RTorchModel</code> class. This step prepares the model to be used by the counterfactual generators. Here is an example:</p><pre><code class="language-julia hljs">model_R = CounterfactualExplanations.Models.RTorchModel(model_loaded, counterfactual_data.likelihood)</code></pre><h3 id="Generating-counterfactuals-with-the-R-torch-model"><a class="docs-heading-anchor" href="#Generating-counterfactuals-with-the-R-torch-model">Generating counterfactuals with the R torch model</a><a id="Generating-counterfactuals-with-the-R-torch-model-1"></a><a class="docs-heading-anchor-permalink" href="#Generating-counterfactuals-with-the-R-torch-model" title="Permalink"></a></h3><p>Now that the R torch model has been wrapped inside the <code>RTorchModel</code> class, you can pass it into the counterfactual generators as you would with any other model.</p><p>Please note that RCall is not fully compatible with PythonCall. Therefore, it is advisable not to import both R torch and PyTorch models within the same Julia session. Additionally, it’s worth mentioning that the R torch integration is still untested in the CounterfactualExplanations package.</p><h2 id="Tuning-Flux-Models"><a class="docs-heading-anchor" href="#Tuning-Flux-Models">Tuning Flux Models</a><a id="Tuning-Flux-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Tuning-Flux-Models" title="Permalink"></a></h2><p>By default, model architectures are very simple. Through optional arguments, users have some control over the neural network architecture and can choose to impose regularization through dropout. Let’s tackle a more challenging dataset: MNIST (LeCun 1998).</p><pre><code class="language-julia hljs">counterfactual_data = load_mnist(10000)
train_data, test_data = 
    CounterfactualExplanations.DataPreprocessing.train_test_split(counterfactual_data)</code></pre><p><img src="../model_catalogue_files/figure-commonmark/cell-9-output-1.svg" alt/></p><p>In this case, we will use a Multi-Layer Perceptron (MLP) but we will adjust the model and training hyperparameters. Parameters related to training of <code>Flux.jl</code> models are currently stored in a mutable container:</p><pre><code class="language-julia hljs">flux_training_params</code></pre><pre><code class="nohighlight hljs">CounterfactualExplanations.FluxModelParams
  loss: Symbol logitbinarycrossentropy
  opt: Symbol Adam
  n_epochs: Int64 100
  batchsize: Int64 1
  verbose: Bool false</code></pre><p>In cases like this one, where model training can be expected to take a few moments, it can be useful to activate verbosity, so let’s set the corresponding field value to <code>true</code>. We’ll also impose mini-batch training:</p><pre><code class="language-julia hljs">flux_training_params.verbose = true
flux_training_params.batchsize = round(size(train_data.X,2)/10)</code></pre><p>To account for the fact that this is a slightly more challenging task, we will use an appropriate number of hidden neurons per layer. We will also activate dropout regularization. To scale networks up further, it is also possible to adjust the number of hidden layers, which we will not do here.</p><pre><code class="language-julia hljs">model_params = (
    n_hidden = 32,
    dropout = true
)</code></pre><p>The <code>model_params</code> can be supplied to the familiar API call:</p><pre><code class="language-julia hljs">M = fit_model(train_data, :MLP; model_params...)</code></pre><pre><code class="nohighlight hljs">FluxModel(Chain(Dense(784 =&gt; 32, relu), Dropout(0.25, active=false), Dense(32 =&gt; 10)), :classification_multi)</code></pre><p>The model performance on our test set can be evaluated as follows:</p><pre><code class="language-julia hljs">model_evaluation(M, test_data)</code></pre><pre><code class="nohighlight hljs">1-element Vector{Float64}:
 0.9136076495599659</code></pre><p>Finally, let’s restore the default training parameters:</p><pre><code class="language-julia hljs">CounterfactualExplanations.reset!(flux_training_params)</code></pre><h2 id="Fitting-and-tuning-MLJ-models"><a class="docs-heading-anchor" href="#Fitting-and-tuning-MLJ-models">Fitting and tuning MLJ models</a><a id="Fitting-and-tuning-MLJ-models-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-and-tuning-MLJ-models" title="Permalink"></a></h2><p>Among models from the MLJ library, three models are supported as of now:</p><pre><code class="language-julia hljs">mlj_models_catalogue</code></pre><p>From these models, the <code>DecisionTreeModel</code> and the <code>RandomForestModel</code> are compatible with the Feature Tweak generator. Support for other generators has not been implemented, as both decision trees and random forests are non-differentiable tree-based models and thus, gradient-based generators don’t apply for them. Support for generating counterfactuals for the <code>EvoTreeModel</code> has not been implemented yet.</p><p>Tuning MLJ models is very simple. As the first step, let’s reload the dataset:</p><pre><code class="language-julia hljs">n = 500
counterfactual_data = CounterfactualExplanations.Data.load_moons(n)</code></pre><p>Using the usual procedure for fitting models, we can call the following method:</p><pre><code class="language-julia hljs">tree = CounterfactualExplanations.Models.fit_model(counterfactual_data, :DecisionTree)</code></pre><p>However, it’s also possible to tune the DecisionTreeClassifier’s parameters. This can be done using the keyword arguments when calling <code>fit_model()</code> as follows:</p><pre><code class="language-julia hljs">tree = CounterfactualExplanations.Models.fit_model(counterfactual_data, :DecisionTree; max_depth=2, min_samples_leaf=3)</code></pre><p>For all supported MLJ models, every tunable parameter they have is supported as a keyword argument. The tunable parameters for the <code>DecisionTreeModel</code> and the <code>RandomForestModel</code> can be found from the <a href="https://docs.juliahub.com/DecisionTree/pEDeB/0.10.11/">documentation of the <code>DecisionTree.jl</code> package</a> under the Decision Tree Classifier and Random Forest Classifier sections. The tunable parameters for the <code>EvoTreeModel</code> can be found from the <a href="https://evovest.github.io/EvoTrees.jl/stable/">documentation of the <code>EvoTrees.jl</code> package</a> under the EvoTreeClassifier section.</p><p>Please note again that generating counterfactuals for the <code>EvoTreeModel</code> is not supported yet.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><p>Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2016. “Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles.” <a href="https://arxiv.org/abs/1612.01474">https://arxiv.org/abs/1612.01474</a>.</p><p>LeCun, Yann. 1998. “The MNIST Database of Handwritten Digits.”</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../models/">« Handling Models</a><a class="docs-footer-nextpage" href="../generators/">Handling Generators »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Tuesday 2 April 2024 14:07">Tuesday 2 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
