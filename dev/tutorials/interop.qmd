---
format: 
  commonmark:
    variant: -raw_html
    wrap: none
    self-contained: true
crossref:
  fig-prefix: Figure
  tbl-prefix: Table
bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib
output: asis
execute: 
  echo: false
  eval: false
jupyter: julia-1.6
---

```@meta
CurrentModule = CounterfactualExplanations 
```

# Interoperability


```{julia}
using Random
# Some random data:
Random.seed!(1234);
N = 100
w = [1.0 1.0]# true coefficients
b = 0
using CounterfactualExplanations
using CounterfactualExplanations.Data
x, y = toy_data_linear(N)
X = hcat(x...)'
```

## Training a `torch` model in R

The first code chunk below builds a simple MLP in R:

```{julia}
using RCall
R"""
# Data
library(torch)
X <- torch_tensor($X)
y <- torch_tensor($y)

# Model:
mlp <- nn_module(
  initialize = function() {
    self$layer1 <- nn_linear(2, 32)
    self$layer2 <- nn_linear(32, 1)
  },
  forward = function(input) {
    input <- self$layer1(input)
    input <- nnf_sigmoid(input)
    input <- self$layer2(input)
    input
  }
)
model <- mlp()
optimizer <- optim_adam(model$parameters, lr = 0.1)
loss_fun <- nnf_binary_cross_entropy_with_logits
"""
```

The following code trains the MLP for the binary prediction task at hand:

```{julia}
R"""
for (epoch in 1:10) {

  model$train()
  train_losses <- c()  

  optimizer$zero_grad()
  output <- model(X)
  loss <- loss_fun(output[,1], y)
  loss$backward()
  optimizer$step()
  train_losses <- c(train_losses, loss$item())
  
  cat(sprintf("Loss at epoch %d: %3f\n", epoch, mean(train_losses)))
}
"""
```


## Making the model compatible

```{julia}
using Flux
using CounterfactualExplanations, CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs # import functions in order to extend

# Step 1)
struct TorchNetwork <: Models.FittedModel
    nn::Any
end

# Step 2)
function logits(ğ‘´::TorchNetwork, X::AbstractArray)
  nn = ğ‘´.nn
  yÌ‚ = rcopy(R"as_array($nn(torch_tensor($X)))")
  yÌ‚ = isa(yÌ‚, AbstractArray) ? yÌ‚ : [yÌ‚]
  return yÌ‚
end
probs(ğ‘´::TorchNetwork, X::AbstractArray)= Ïƒ.(logits(ğ‘´, X))
ğ‘´ = TorchNetwork(R"model")
```

## Adapting the generator

```{julia}
import CounterfactualExplanations.Generators: âˆ‡
using LinearAlgebra
h(xÌ…, xÌ²) = norm(xÌ…-xÌ²) # complexity penalty
âˆ‚h(xÌ…, xÌ²) = gradient(() -> h(xÌ…, xÌ²), params(xÌ²))[xÌ²]
function âˆ‚â„“(xÌ², ğ‘´, t) 
  nn = ğ‘´.nn
  R"""
  x <- torch_tensor($xÌ², requires_grad=TRUE)
  obj_loss <- nnf_binary_cross_entropy_with_logits($nn(x),$t)
  obj_loss$backward()
  """
  grad = rcopy(R"as_array(x$grad)")
  return grad
end

âˆ‡(generator::GenericGenerator, xÌ², ğ‘´::TorchNetwork, t, xÌ…) = âˆ‚â„“(xÌ², ğ‘´, t) + generator.Î» * âˆ‚h(xÌ…, xÌ²)
```

```{julia}
# Randomly selected factual:
Random.seed!(123)
xÌ… = x[rand(1:length(x))]
yÌ… = round(probs(ğ‘´, xÌ…)[1])
target = ifelse(yÌ…==1.0,0.0,1.0) # opposite label as target
Î³ = 0.75 # desired level of confidence
# Define Generator:
generator = GenericGenerator(0.0,0.1,1e-5,:logitbinarycrossentropy,nothing)
# Generate recourse:
counterfactual = generate_counterfactual(generator, xÌ…, ğ‘´, target, Î³); # generate recourse
```


```{julia}
include("docs/src/utils.jl")
using Plots
T = size(counterfactual.path)[1]
X_path = reduce(hcat,counterfactual.path)'
yÌ‚ = CounterfactualExplanations.target_probs(probs(counterfactual.ğ‘´, X_path)',target)
p1 = plot_contour(X,y,ğ‘´;clegend=false, title="Posterior predictive - Plugin")
anim = @animate for t in 1:T
    scatter!(p1, [counterfactual.path[t][1]], [counterfactual.path[t][2]], ms=5, color=Int(yÌ…), label="")
    p2 = plot(1:t, yÌ‚[1:t], xlim=(0,T), ylim=(0, 1), label="p(yÌ²=" * string(target) * ")", title="Validity", lc=:black)
    Plots.abline!(p2,0,Î³,label="threshold Î³", ls=:dash) # decision boundary
    plot(p1,p2,size=(800,400))
end
gif(anim, "docs/src/tutorials/www/interop_r.gif", fps=25)
```


