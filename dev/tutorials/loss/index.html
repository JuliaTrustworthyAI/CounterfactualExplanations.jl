<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Loss functions and gradients ¬∑ AlgorithmicRecourse.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://pat-alt.github.io/AlgorithmicRecourse.jl/tutorials/loss/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">AlgorithmicRecourse.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Loss functions and gradients</a><ul class="internal"><li><a class="tocitem" href="#General-setup"><span>General setup</span></a></li><li><a class="tocitem" href="#Loss-function-\\ell"><span>Loss function <span>$\ell$</span></span></a></li><li><a class="tocitem" href="#Example-in-2D"><span>Example in 2D</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Loss functions and gradients</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Loss functions and gradients</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/pat-alt/AlgorithmicRecourse.jl/blob/master/docs/src/tutorials/loss.md#" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Loss-functions-and-gradients-in-algorithmic-recourse"><a class="docs-heading-anchor" href="#Loss-functions-and-gradients-in-algorithmic-recourse">Loss functions and gradients in algorithmic recourse</a><a id="Loss-functions-and-gradients-in-algorithmic-recourse-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-functions-and-gradients-in-algorithmic-recourse" title="Permalink"></a></h1><p>This is a short tutorial on gradients typically involved in optimization problems of algorithmic recourse.</p><pre><code class="language-julia hljs">using Zygote
using Plots
using PlotThemes
theme(:juno)
using LinearAlgebra</code></pre><h2 id="General-setup"><a class="docs-heading-anchor" href="#General-setup">General setup</a><a id="General-setup-1"></a><a class="docs-heading-anchor-permalink" href="#General-setup" title="Permalink"></a></h2><p>Let <span>$t\in\{0,1\}$</span> denote the target label, <span>$f\in\{0,1\}$</span> the predicted label and <span>$x&#39;\in\mathbb{R}^D$</span> the vector of counterfactual features. Then the differentiable optimization problem in algorithmic recourse is generally of the following form</p>$<p>x&#39; = \arg \min_{x&#39;} \max _{\lambda} \lambda \ell(f(x&#39;),t) + h(x&#39;) $</p><p>where <span>$\ell$</span> denotes some loss function targeting the deviation between the target label and the predicted label and <span>$h(\cdot)$</span> as a complexity penality generally addressing the <em>realism</em> or <em>cost</em> of the proposed counterfactual. </p><h2 id="Loss-function-\\ell"><a class="docs-heading-anchor" href="#Loss-function-\\ell">Loss function <span>$\ell$</span></a><a id="Loss-function-\\ell-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function-\\ell" title="Permalink"></a></h2><p>Different choices for <span>$\ell$</span> come to mind, each potentially leading to very different counterfactual outcomes. In practice, <span>$\ell$</span> is typically implemented with respect to the <em>logits</em> <span>$a=\mathbf{w}^Tx$</span> rather than the probabilities <span>$p(y&#39;=1|x&#39;)=\sigma(a)$</span> predicted by the classifier . We follow this convention here. Common choices for <span>$\ell$</span> in the literature have included <em>Hinge</em> loss, <em>cross-entropy</em> (or <em>log</em>) loss or <em>mean squared error</em> loss (MSE). We shall look at these in some more details here.</p><h3 id="Hinge-loss"><a class="docs-heading-anchor" href="#Hinge-loss">Hinge loss</a><a id="Hinge-loss-1"></a><a class="docs-heading-anchor-permalink" href="#Hinge-loss" title="Permalink"></a></h3><p>With respect to the logits <span>$a=\mathbf{w}&#39;x$</span> Hinge loss can be defined as follows</p>$<p>\ell(a,t)=(t-a)_+=\max{0,t-a} $</p><p>where <span>$t$</span> is the target class in as before (we have <span>$t\in\{0,1\}$</span> for a binary classification problem).</p><p><strong>NOTE</strong>: Hinge loss is generally defined for the target domain <span>$\{-1,1\}$</span>. Therefore in our context we have <span>$a=z \mathbf{w}^Tx&#39;$</span> where </p>$<p>\begin{aligned} z&amp;=\begin{cases} -1 &amp;&amp; \text{if} &amp;&amp; f=0 \ f &amp;&amp; \text{if} &amp;&amp; f=1 \end{cases} \end{aligned} $</p><p>The first-order derivative of Hinge loss with respect to the logits <span>$a$</span> is simply</p>$<p>\begin{aligned} \ell&#39;(a,t)&amp;=\begin{cases} -1 &amp;&amp; \text{if} &amp;&amp; a&lt;=1 \ 0 &amp;&amp; \text{otherwise.}  \end{cases} \end{aligned} $</p><p>In the context of counterfactual search the gradient with respect to the feature vector is as follows:</p>$<p>\begin{aligned} &amp;&amp; \nabla_{x&#39;} \ell(a,t)&amp;= \begin{cases} -z\mathbf{w} &amp;&amp; \text{if} &amp;&amp; z\mathbf{w}^Tx&#39;&lt;=1 \ 0 &amp;&amp; \text{otherwise.}  \end{cases} \end{aligned} $</p><p>In practice gradients are commonly computed through autodifferentiation. In this tutorial we use the <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a> package which is at the core of <a href="https://fluxml.ai/Flux.jl/stable/models/basics/">Flux.jl</a>, the main deep learning package for Julia.</p><p>The side-by-side plot below visualises the loss function and its derivative. The plot further below serves as a simple sanity check to verify that autodifferentiation indeed yields the same result as the closed-form solution for the gradient.</p><pre><code class="language-julia hljs">hinge(a,t) = max(0,t-a)</code></pre><pre><code class="nohighlight hljs">hinge (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">a = -2:0.1:2
p1 = plot(a, [hinge(a,1) for a=a], title=&quot;Loss&quot;)
p2 = plot(a, [gradient(hinge,a,1)[1] for a=a], title=&quot;Gradient&quot;)
plot(p1, p2, layout = (1, 2), legend = false)</code></pre><pre><code class="nohighlight hljs">‚îå Info: Precompiling GR_jll [d2c73de3-f751-5644-a686-071e5b155ba9]
‚îî @ Base loading.jl:1342</code></pre><p><img src="../loss_files/loss_6_1.svg" alt="svg"/></p><pre><code class="language-julia hljs"># Just verifying that the formula for the gradient above indeed yields the same result.
function gradient_man(x,w,y)
    ùê† = ifelse(w&#39;x&lt;=1, -w, 0)
    return ùê†
end;
plot(a, [gradient_man(a,1,1) for a=a], legend=:bottomright, label=&quot;Manual&quot;, title=&quot;Gradient&quot;)
scatter!(a, [gradient(hinge,a,1)[1] for a=a], label=&quot;Autodiff&quot;)</code></pre><p><img src="../loss_files/loss_7_0.svg" alt="svg"/></p><h3 id="Cross-entropy-loss-(binary)"><a class="docs-heading-anchor" href="#Cross-entropy-loss-(binary)">Cross-entropy loss (binary)</a><a id="Cross-entropy-loss-(binary)-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-entropy-loss-(binary)" title="Permalink"></a></h3><p>Binary cross-entropy loss or log loss is typically defined as follows:</p>$<p>\begin{aligned} &amp;&amp; \ell(a,t)&amp;=- \left( t \cdot \log(\sigma(a)) + (1-t) \cdot \log (1-\sigma(a)) \right) \
\end{aligned} $</p><p>where <span>$\sigma(a)$</span> is the logit function.</p><p>Once again for the purpose of counter factual search we are interested in the first-order derivative with respect to our feature vector <span>$x&#39;$</span>. You can verify that the partial derivative with respect to feature <span>$x&#39;_d$</span> is as follows:</p>$<p>\begin{aligned} &amp;&amp; \frac{\partial \ell(a,t)}{\partial x&#39;<em>d}&amp;= (\sigma(a) - t) w</em>d \
\end{aligned} $</p><p>The gradient just corresponds to the stacked vector of partial derivatives:</p>$<p>\begin{aligned} &amp;&amp; \nabla_{x&#39;} \ell(a,t)&amp;= (\sigma(a) - t) \mathbf{w} \
\end{aligned} $</p><p>As before implementation below is done through autodifferentiation. As before the side-by-side plot shows the resulting loss function and its gradient and the plot further below is a simple sanity check.</p><pre><code class="language-julia hljs"># logit function:
function ùõî(a)
    trunc = 8.0 # truncation to avoid numerical over/underflow
    a = clamp.(a,-trunc,trunc)
    p = exp.(a)
    p = p ./ (1 .+ p)
    return p
end

# Binary crossentropy:
crossentropy(a, t) = - (t * log(ùõî(a)) + (1-t) * log(1-ùõî(a)))</code></pre><pre><code class="nohighlight hljs">crossentropy (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">p1 = plot(a, [crossentropy(a,1) for a=a], title=&quot;Loss&quot;)
p2 = plot(a, [gradient(crossentropy,a,1)[1] for a=a], title=&quot;Gradient&quot;)
plot(p1, p2, layout = (1, 2), legend = false)</code></pre><p><img src="../loss_files/loss_10_0.svg" alt="svg"/></p><pre><code class="language-julia hljs"># Just verifying that the formula for the gradient above indeed yields the same result.
function gradient_man(x,w,y)
    ùê† = (ùõî(w&#39;x) - y) .* w
    return ùê†
end;
plot(a, [gradient_man(a,1,1) for a=a], legend=:bottomright, label=&quot;Manual&quot;, title=&quot;Gradient&quot;)
scatter!(a, [gradient(crossentropy,a,1)[1] for a=a], label=&quot;Autodiff&quot;)</code></pre><p><img src="../loss_files/loss_11_0.svg" alt="svg"/></p><h3 id="Mean-squared-error"><a class="docs-heading-anchor" href="#Mean-squared-error">Mean squared error</a><a id="Mean-squared-error-1"></a><a class="docs-heading-anchor-permalink" href="#Mean-squared-error" title="Permalink"></a></h3><p>Some authors work with distance-based loss functions instead. Since in general we are interested in providing valid recourse, that is counterfactual explanations that indeed lead to the desired label switch, using one of the margin-based loss functions introduced above seems like a more natural choice. Nonetheless, we shall briefly introduce one of the common distance-based loss functions as well. </p><p>The mean squared error for counterfactual search implemented with respect to the logits is simply the squared <span>$\ell 2$</span> distance between the target label and <span>$a=\mathbf{w}^Tx$</span>:</p>$<p>\begin{aligned} &amp;&amp; \ell(a,t)&amp;= ||t-a||^2 \end{aligned} $</p><p>The gradient with respect to the vector of features is then:</p>$<p>\begin{aligned} &amp;&amp; \nabla_{x&#39;} \ell(a,t)&amp;= 2(a - t) \mathbf{w} \
\end{aligned} $</p><p>As before implementation and visualizations follow below.</p><pre><code class="language-julia hljs">mse(a,t) = norm(t - a)^2</code></pre><pre><code class="nohighlight hljs">mse (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">p1 = plot(a, [mse(a,1) for a=a], title=&quot;Loss&quot;)
p2 = plot(a, [gradient(mse,a,1)[1] for a=a], title=&quot;Gradient&quot;)
plot(p1, p2, layout = (1, 2), legend = false)</code></pre><p><img src="../loss_files/loss_14_0.svg" alt="svg"/></p><pre><code class="language-julia hljs"># Just verifying that the formula for the gradient above indeed yields the same result.
function gradient_man(x,w,y)
    ùê† = 2*(w&#39;x - y) .* w
    return ùê†
end;
plot(a, [gradient_man(a,1,1) for a=a], legend=:bottomright, label=&quot;Manual&quot;, title=&quot;Gradient&quot;)
scatter!(a, [gradient(mse,a,1)[1] for a=a], label=&quot;Autodiff&quot;)</code></pre><p><img src="../loss_files/loss_15_0.svg" alt="svg"/></p><h2 id="Example-in-2D"><a class="docs-heading-anchor" href="#Example-in-2D">Example in 2D</a><a id="Example-in-2D-1"></a><a class="docs-heading-anchor-permalink" href="#Example-in-2D" title="Permalink"></a></h2><p>To understand the properties of the different loss functions we will now look at a tow example in 2D. The code below generates some random features and assigns labels based on a fixed vector of coefficients using the sigmoid function.</p><pre><code class="language-julia hljs"># Some random data:
using Flux
using Random
Random.seed!(1234);
N = 25
w = reshape([1.0,-2.0],2,1) # true coefficients
b = 0
X = reshape(randn(2*N),N,2) # random features
y = Int.(round.(Flux.œÉ.(X*w .+ b))); # label based on sigmoid</code></pre><pre><code class="nohighlight hljs">‚îå Info: Precompiling Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]
‚îî @ Base loading.jl:1342</code></pre><p>The plot below shows the samples coloured by label along with the decision boundary. You can think of this as representing the outcome of some automated decision making system. The highlighted sample was chosen to receive algorithmic recourse in the following: we will search for a counterfactual that leads to a label switch.</p><pre><code class="language-julia hljs"># Plot with random sample chose for recourse
x_prime = reshape(X[5,:],1,2)
scatter(X[:,1],X[:,2],legend=false,color=y) # features
Plots.abline!(-w[1]/w[2],0) # decision boundary
scatter!([x_prime[1]],[x_prime[2]],color=&quot;yellow&quot;,markersize=10)</code></pre><p><img src="../loss_files/loss_19_0.svg" alt="svg"/></p><p>Next we will generating recourse using the AlgorithmicRecourse.jl package.</p><pre><code class="language-julia hljs"># Generate recourse:
using AlgorithmicRecourse
ùìú = AlgorithmicRecourse.Models.LogisticModel(w, [b]);
generator = AlgorithmicRecourse.Generators.GenericGenerator(0.1,0.1,1e-5) # here we choose the generic generator
recourse = generate_recourse(generator, x_prime, ùìú, 0.0); # generate recourse</code></pre><pre><code class="nohighlight hljs">‚îå Info: Precompiling AlgorithmicRecourse [2f13d31b-18db-44c1-bc43-ebaf2cff0be0]
‚îî @ Base loading.jl:1342





AlgorithmicRecourse.Generators.GenericGenerator(0.1, 0.1, 1.0e-5)</code></pre><pre><code class="language-julia hljs">scatter(X[:,1],X[:,2],legend=false,color=y) # features
Plots.abline!(-w[1]/w[2],0) # decision boundary
scatter!([x_prime[1]],[x_prime[2]],color=&quot;yellow&quot;,markersize=10)
scatter!(recourse.path[:,1], recourse.path[:,2])</code></pre><p><img src="../loss_files/loss_22_0.svg" alt="svg"/></p><pre><code class="language-julia hljs">[recourse.path[:,1]]</code></pre><pre><code class="nohighlight hljs">1-element Vector{Vector{Float64}}:
 [0.8644013132535154, 0.7756078569143855, 0.6965199892217692, 0.6236031192969299, 0.5576876509635852, 0.49906589765446335, 0.4474893517555402, 0.40233009886252613, 0.3627869308575854, 0.32804342551286575  ‚Ä¶  -0.16184414930441315, -0.16184474581688937, -0.16184532958585093, -0.1618459008835757, -0.16184645997652264, -0.16184700712545624, -0.1618475425855683, -0.16184806660659695, -0.16184857943294337, -0.1618490813037859]</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">¬´ Home</a><a class="docs-footer-nextpage" href="../../reference/">Reference ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Wednesday 12 January 2022 14:48">Wednesday 12 January 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
