{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```@meta\n",
        "CurrentModule = CounterfactualExplanations \n",
        "```\n",
        "\n",
        "# Models\n"
      ],
      "id": "8f49fa58"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "using Flux, Plots, Random, PlotThemes, Statistics, CounterfactualExplanations\n",
        "theme(:wong)\n",
        "using Logging\n",
        "disable_logging(Logging.Info)\n",
        "include(\"dev/utils.jl\") # some plotting functions\n",
        "www_path = \"docs/src/tutorials/www\""
      ],
      "id": "b6941a6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Default models\n",
        "\n",
        "There are currently constructors for two default models, which mainly serve illustrative purposes (@fig-models below). Both take sets of estimated parameters at the point of instantiation: the constructors will not fit a model for you, but assume that you have already estimated the respective model yourself and have access to its parameter estimates. Based on the supplied parameters methods to predict logits and probabilities are already implemented and used in the counterfactual search. \n",
        "\n",
        "![Schematic overview of classes in `Models` module.](www/models_uml.png){#fig-models}\n",
        "\n",
        "For the simple logistic regression model logits are computed as $a=Xw + b$ and probabilities are simply $\\sigma(a)$. For the Bayesian logistic regression model logits are computed as $X\\mu$ and the predictive posterior is computed through Laplace and Probit approximation.\n",
        "\n",
        "## Custom models\n",
        "\n",
        "Apart from the default models you can use any arbitrary (differentiable) model and generate recourse in the same way as before. Only two steps are necessary to make your own Julia model compatible with this package:\n",
        "\n",
        "1. The model needs to be declared as a subtype of `CounterfactualExplanations.Models.AbstractFittedModel`.\n",
        "2. You need to extend the functions `CounterfactualExplanations.Models.logits` and `CounterfactualExplanations.Models.probs` to accept your custom model.\n",
        "\n",
        "Below we will go through a simple example to see how this can be done in practice. In one of the following sections we will also see how to make model built and trained in other programming languages compatible with this library.\n",
        "\n",
        "### Neural network\n",
        "\n",
        "In this example we will build a simple artificial neural network using [Flux](https://fluxml.ai/) for a binary classification task. First we generate some toy data below. The code that generates this data was borrowed from a great tutorial about Bayesian neural networks provided by [Turing.jl](https://turing.ml/dev/), which you may find [here](https://turing.ml/dev/tutorials/03-bayesian-neural-network/). \n"
      ],
      "id": "e532ebe3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Number of points to generate.\n",
        "N = 80\n",
        "M = round(Int, N / 4)\n",
        "Random.seed!(1234)\n",
        "\n",
        "using CounterfactualExplanations.Data\n",
        "xs, ys = Data.toy_data_non_linear(N)\n",
        "X = hcat(xs...)"
      ],
      "id": "a6469978",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot below shows the generated samples in the 2D feature space where colours indicate the associated labels. CounterfactualExplanationsly this data is not linearly separable and the default `LogisticModel` would be ill-suited for this classification task.\n"
      ],
      "id": "a5133609"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "plt = plot()\n",
        "plt = plot_data!(plt,X',ys)\n",
        "savefig(plt, joinpath(www_path, \"models_samples.png\"))"
      ],
      "id": "09f2e008",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](www/models_samples.png)\n",
        "\n",
        "#### Training the model\n",
        "\n",
        "Instead, we will build a simple artificial neural network `nn` with one hidden layer using a simple helper function `build_model`.^[Helper functions like this one are not part of our package functionality. They can be found [here](https://github.com/pat-alt/CounterfactualExplanations.jl/blob/main/dev/utils.jl).] For additional resources on how to do deep learning with [Flux](https://fluxml.ai/) just have a look at their documentation. \n"
      ],
      "id": "ebebc701"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nn = build_model(dropout=true,activation=Flux.σ)\n",
        "loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y)\n",
        "ps = Flux.params(nn)\n",
        "data = zip(xs,ys);"
      ],
      "id": "59baca68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below trains the neural network for the task at hand while keeping track of the training loss. Note that normally we would be interested in loss with respect to a validation data set. But since we are primarily interested in generating counterfactual explanations for a trained classifier here, we will just keep things very simple on the training side.\n"
      ],
      "id": "fe3ad04e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Flux.Optimise: update!, Adam\n",
        "opt = Adam()\n",
        "epochs = 100\n",
        "avg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\n",
        "show_every = epochs/10\n",
        "\n",
        "for epoch = 1:epochs\n",
        "  for d in data\n",
        "    gs = gradient(Flux.params(nn)) do\n",
        "      l = loss(d...)\n",
        "    end\n",
        "    update!(opt, Flux.params(nn), gs)\n",
        "  end\n",
        "  if epoch % show_every == 0\n",
        "    println(\"Epoch \" * string(epoch))\n",
        "    @show avg_loss(data)\n",
        "  end\n",
        "end"
      ],
      "id": "72d79c3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generating counterfactuals\n",
        "\n",
        "Now it's game time: we have a fitted model $M: \\mathcal{X} \\mapsto \\mathcal{Y}$ and are interested in generating recourse for some individual $x\\in\\mathcal{X}$. As mentioned above we need to do a bit more work to prepare the model for use with our package. \n",
        "\n",
        "The code below takes care of all of that: in step 1) it declares our model as a subtype of `Models.AbstractFittedModel` and in step 2) it just extends the two functions. \n"
      ],
      "id": "b1a33587"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using CounterfactualExplanations, CounterfactualExplanations.Models\n",
        "import CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n",
        "\n",
        "# Step 1)\n",
        "struct NeuralNetwork <: Models.AbstractDifferentiableModel\n",
        "    model::Any\n",
        "end\n",
        "\n",
        "# Step 2)\n",
        "logits(M::NeuralNetwork, X::AbstractArray) = M.model(X)\n",
        "probs(M::NeuralNetwork, X::AbstractArray)= σ.(logits(M, X))\n",
        "M = NeuralNetwork(nn)"
      ],
      "id": "6e451597",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot below shows the predicted probabilities in the feature domain. Evidently, our simple neural network is doing well on the training data.\n"
      ],
      "id": "9f04aca8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "# Plot the posterior distribution with a contour plot.\n",
        "plt = plot(M, counterfactual_data);\n",
        "savefig(plt, joinpath(www_path, \"models_contour.png\"))"
      ],
      "id": "b71d9ee9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](www/models_contour.png)\n",
        "\n",
        "To preprocess the data for use with our package we simply run the following:\n"
      ],
      "id": "fa6dd413"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "counterfactual_data = CounterfactualData(X,ys')"
      ],
      "id": "f65dcbae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we just select a random sample from our data and based on its current label we set as our target the opposite label.\n"
      ],
      "id": "78cf76cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Random\n",
        "Random.seed!(123)\n",
        "x = select_factual(counterfactual_data, rand(1:size(X)[2])) \n",
        "y = round(probs(M, x)[1])\n",
        "target = ifelse(y==1.0,0.0,1.0) # opposite label as target"
      ],
      "id": "98582ac4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then finally we use the `GenericGenerator` to generate counterfactual. The plot further below shows the resulting counterfactual path.\n"
      ],
      "id": "868bef68"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define generator:\n",
        "generator = GenericGenerator()\n",
        "# Generate recourse:\n",
        "counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)"
      ],
      "id": "3cf621b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "T = size(path(counterfactual))[1]\n",
        "X_path = reduce(hcat,path(counterfactual))\n",
        "ŷ = target_probs(counterfactual,X_path)\n",
        "p1 = plot_contour(X',ys,M;colorbar=false, title=\"MLP\")\n",
        "anim = @animate for t in 1:T\n",
        "    scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label=\"\")\n",
        "    p2 = plot(1:t, ŷ[1:t], xlim=(0,T), ylim=(0, 1), label=\"p(y′=\" * string(target) * \")\", title=\"Validity\", lc=:black)\n",
        "    Plots.abline!(p2,0,counterfactual.params[:γ],label=\"threshold γ\", ls=:dash) # decision boundary\n",
        "    plot(p1,p2,size=(800,400))\n",
        "end\n",
        "gif(anim, joinpath(www_path, \"models_generic_recourse.gif\"), fps=5)"
      ],
      "id": "e72b5366",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](www/models_generic_recourse.gif)\n",
        "\n",
        "### Ensemble of neural networks\n",
        "\n",
        "In the context of Bayesian classifiers the `GreedyGenerator` can be used since minimizing the predictive uncertainty acts as a proxy for *realism* and *unambiquity*. In other words, if we have a model that incorporates uncertainty, we can generate realistic counterfactuals without the need for a complexity penalty. \n",
        "\n",
        "One efficient way to produce uncertainty estimates in the context of deep learning is to simply use an ensemble of artificial neural networks, also referred to as *deep ensemble* [@lakshminarayanan2016simple]. To this end, we can use the `build_model` function from above repeatedly to compose an ensemble of $K$ neural networks:\n"
      ],
      "id": "a6cce0d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ensemble = build_ensemble(5;kw=(dropout=true,activation=Flux.σ))"
      ],
      "id": "79d5bb24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training this ensemble boils down to training each neural network separately:\n"
      ],
      "id": "b05accd0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ensemble, anim = forward(ensemble, data, opt, n_epochs=epochs, plot_every=show_every); # fit the ensemble\n",
        "gif(anim, joinpath(www_path, \"models_ensemble_loss.gif\"), fps=10)"
      ],
      "id": "529e3ce4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](www/models_ensemble_loss.gif)\n",
        "\n",
        "Once again it is straight-forward to make the model compatible with the package. Note that for an ensemble model the predicted logits and probabilities are just averages over predictions produced by all $K$ models.\n"
      ],
      "id": "35a6443e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1)\n",
        "struct FittedEnsemble <: Models.AbstractFittedModel\n",
        "    ensemble::AbstractArray\n",
        "end\n",
        "\n",
        "# Step 2)\n",
        "logits(M::FittedEnsemble, X::AbstractArray) = mean(Flux.flatten(Flux.stack([nn(X) for nn in M.ensemble],1)),dims=1)\n",
        "probs(M::FittedEnsemble, X::AbstractArray) = mean(Flux.flatten(Flux.stack([σ.(nn(X)) for nn in M.ensemble],1)),dims=1)\n",
        "\n",
        "M=FittedEnsemble(ensemble)"
      ],
      "id": "485e9882",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we plot the predicted probabilities in the feature domain. As expected the ensemble is more *conservative* because it incorporates uncertainty: the predicted probabilities splash out more than before, especially in regions that are not populated by samples.\n"
      ],
      "id": "beff529e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "plt = plot_contour(X',ys,M);\n",
        "savefig(plt, joinpath(www_path, \"models_ensemble_contour.png\"))"
      ],
      "id": "4639385b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](www/models_ensemble_contour.png)\n",
        "\n",
        "Finally, we use the `GreedyGenerator` for the counterfactual search. \n"
      ],
      "id": "1abc19f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "generator = GreedyGenerator(Dict(:δ=>0.1,:n=>30))\n",
        "counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)"
      ],
      "id": "12284c74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "T = size(path(counterfactual))[1]\n",
        "X_path = reduce(hcat,path(counterfactual))\n",
        "ŷ = target_probs(counterfactual,X_path)\n",
        "p1 = plot_contour(X',ys,M;colorbar=false, title=\"Deep ensemble\")\n",
        "anim = @animate for t in 1:T\n",
        "    scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label=\"\")\n",
        "    p2 = plot(1:t, ŷ[1:t], xlim=(0,T), ylim=(0, 1), label=\"p(y′=\" * string(target) * \")\", title=\"Validity\", lc=:black)\n",
        "    Plots.abline!(p2,0,counterfactual.params[:γ],label=\"threshold γ\", ls=:dash) # decision boundary\n",
        "    plot(p1,p2,size=(800,400))\n",
        "end\n",
        "gif(anim, joinpath(www_path, \"models_greedy_recourse.gif\"), fps=5)"
      ],
      "id": "1276d154",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](www/models_greedy_recourse.gif)\n",
        "\n",
        "# References\n"
      ],
      "id": "3f62ce7e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "language": "julia",
      "display_name": "Julia 1.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}