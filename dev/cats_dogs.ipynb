{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "format: \n",
    "  commonmark:\n",
    "    variant: -raw_html\n",
    "    wrap: none\n",
    "    self-contained: true\n",
    "crossref:\n",
    "  fig-prefix: Figure\n",
    "  tbl-prefix: Table\n",
    "bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib\n",
    "output: asis\n",
    "execute: \n",
    "  eval: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From üê± to üê∂ - a motivating example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a sample of cats and dogs with information about two features: height and tail length. Based on these two features we have trained two black box classifiers to distinguish between cats and dogs: firstly, an artificial neural network with weight regularization and secondly, that same neural network but its Bayesian counterpart (@fig-predictive below). One individual cat ‚Äì let‚Äôs call her Kitty üê± ‚Äì is friends with a lot of cool dogs and wants to be part of that group. Let's see how we can generate counterfactual paths for her."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "using Pkg.Artifacts\n",
    "img_path = joinpath(artifact_path(artifact_hash(\"cats_dogs_images\", \"../../Artifacts.toml\")), \"cats_dogs_images\")\n",
    "if !isdir(\"www\")\n",
    "    mkdir(\"www\")\n",
    "end\n",
    "[cp(joinpath(img_path, file), joinpath(\"www\", file), force=true) for file in readdir(img_path)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "using Images\n",
    "mlp = load(joinpath(img_path, \"predictive_mlp.png\"))\n",
    "laplace = load(joinpath(img_path, \"predictive_laplace.png\"))\n",
    "img = mosaic(mlp, laplace, ncol=2)\n",
    "save(\"www/predictive.png\",img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Classification for toy dataset of cats and dogs. The contour indicates confidence in predicted labels. Left: MLP with weight regularization. Right: That same MLP, but with Laplace approximation for posterior predictive.](www/predictive.png){#fig-predictive}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From basic principles ...\n",
    "\n",
    "Counterfactual search happens in the feature space: we are interested in understanding how we need to change üê±'s attributes in order to change the output of the black-box classifier. We will start with the first model, that relies on simple plugin estimates to produce its predictions. The model was pre-trained using Flux.jl and can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CounterfactualExplanations.Data: cats_dogs_model\n",
    "model = cats_dogs_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the Flux.jl model compatible with CounterfactualExplanations.jl we need to run the following (more on this in the [models tutorial](https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/tutorials/models/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n",
    "\n",
    "# Step 1)\n",
    "struct NeuralNetwork <: Models.FittedModel\n",
    "    model::Any\n",
    "end\n",
    "\n",
    "# Step 2)\n",
    "logits(ùë¥::NeuralNetwork, X::AbstractArray) = ùë¥.model(X)\n",
    "probs(ùë¥::NeuralNetwork, X::AbstractArray)= œÉ.(logits(ùë¥, X))\n",
    "ùë¥ = NeuralNetwork(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `xÃÖ` be the 2D-feature vector describing Kitty üê±. Based on those features she is currently labelled as `yÃÖ = 0.0`. We have set the target label to `1.0` and the desired confidence in the prediction to `Œ≥ = 0.75`.  Now we can use the `GenericGenerator` for our counterfactual search as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GenericGenerator(0.01,2,1e-5,:logitbinarycrossentropy,nothing)\n",
    "recourse = generate_counterfactual(generator, xÃÖ, ùë¥, target, Œ≥)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GenericGenerator` implements the search algorithm first proposed by @wachter2017counterfactual. The resulting counterfactual path is shown in @fig-recourse-mlp below. We can see that üê± travels through the feature space until she reaches a destination where the black-box model predicts that with a probability of >75% she is actually a dog. Her counterfactual self is in the target class so the algorithmic recourse objective is satisfied. We have also gained an intuitive understanding of how the black-model arrives at its decisions: increasing height and decreasing tail length both raise the predicted probability that üê± is actually a dog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Classification for toy dataset of cats and dogs. The contour indicates confidence in predicted labels. Left: MLP with weight regularization. Right: That same MLP, but with Laplace approximation for posterior predictive.](www/recourse_mlp.gif){#fig-recourse-mlp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... towards realistic counterfactuals.\n",
    "\n",
    "The generic search above yielded a counterfactual sample that is still quite distinct from all other individuals in the target class. While we successfully fooled the black-box model, a human might look at üê±'s counterfactual self and get a little suspicious. One of the requirements for algorithmic recourse is that counterfactuals are realistic and unambigous. A straight-forward way to meet this requirement is to generate counterfactuals by implicitly minimizing predictive uncertainty [@schut2021generating]. The simple neural network does not incorporate uncertainty, but its Bayesian counterpart does: note how in @fig-predictive above the contours for the Bayesian neural network (Laplace) fan out away from the sample. As before we will be using a pre-trained model. Laplace approximation was implemented using [BayesLaplace.jl](https://www.paltmeyer.com/BayesLaplace.jl/dev/) (see [here](https://towardsdatascience.com/go-deep-but-also-go-bayesian-ab25efa6f7b) for an introduction). The pre-trained Bayesian model can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CounterfactualExplanations.Data: cats_dogs_laplace\n",
    "la = cats_dogs_laplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we need to make the model compatible with CounterfactualExplanations.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1)\n",
    "struct LaplaceNeuralNetwork <: Models.FittedModel\n",
    "    la::BayesLaplace.LaplaceRedux\n",
    "end\n",
    "\n",
    "# Step 2)\n",
    "logits(ùë¥::LaplaceNeuralNetwork, X::AbstractArray) = ùë¥.la.model(X)\n",
    "probs(ùë¥::LaplaceNeuralNetwork, X::AbstractArray)= BayesLaplace.predict(ùë¥.la, X)\n",
    "ùë¥·¥∏ = LaplaceNeuralNetwork(la);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same target and desired confidence `Œ≥` as above we finally use the `GreedyGenerator` generator for our counterfactual search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GreedyGenerator(0.1,20,:logitbinarycrossentropy,nothing)\n",
    "recourse = generate_counterfactual(generator, xÃÖ, ùë¥·¥∏, target, Œ≥); # generate recourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GreedyGenerator` implements the approach proposed in @schut2021generating: by maximizing the predicted probability of the Bayesian model in @fig-recourse-laplace below, we implicitly minimize the predictive uncertainty around the counterfactual. This way we end up generating a counterfactual that looks more like the individuals üê∂ in the target class and is therefore more realistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Classification for toy dataset of cats and dogs. The contour indicates confidence in predicted labels. Left: MLP with weight regularization. Right: That same MLP, but with Laplace approximation for posterior predictive.](www/recourse_laplace.gif){#fig-recourse-laplace}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.4",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
