# Setup

## Set up

### Install dependencies
```{julia}
import Pkg
Pkg.activate("$(pwd())/dev/R_call_implementation")

using RCall
using CounterfactualExplanations
using Revise
```

### Load data

```{julia}
using Random

Random.seed!(42);
N = 100

counterfactual_data = CounterfactualExplanations.Data.load_moons(N)

x = Float32.(counterfactual_data.X)
y = Float32.(counterfactual_data.y)
```

## Implement and train RTorch model

### RTorch model declaration
```{julia}
R"""
# Data
library(torch)
X <- torch_tensor(t($x))
ys <- torch_tensor(t($y))

# Model:
mlp <- nn_module(
  initialize = function() {
    self$layer1 <- nn_linear(2, 32)
    self$layer2 <- nn_linear(32, 2)
  },
  forward = function(input) {
    input <- self$layer1(input)
    input <- nnf_sigmoid(input)
    input <- self$layer2(input)
    input
  }
)
"""
```

### Instantiate RTorch model, optimizer and loss functions
```{julia}
R"""
model <- mlp()
optimizer <- optim_adam(model$parameters, lr = 0.1)
loss_fun <- nnf_binary_cross_entropy_with_logits
"""
```

### Training

```{julia}
R"""
for (epoch in 1:50) {
  model$train()  
  # Compute prediction and loss:
  output <- model(X)
  loss <- loss_fun(output, ys)
  # Backpropagation:
  optimizer$zero_grad()
  loss$backward()
  optimizer$step()

  cat(sprintf("Loss at epoch %d: %7f\n", epoch, loss$item()))
}
"""
```

## Declare RTorchModel

### RTorchModel class
```{julia}
using Flux
using CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs

struct RTorchModel <: AbstractDifferentiableModel
    nn::Any
    likelihood::Symbol
end

function logits(model::RTorchModel, x::AbstractArray)
  if !isa(x, Matrix)
      x = reshape(x, length(x), 1)
  end

  model_nn = model.nn

  ŷ = rcopy(R"as_array($model_nn(torch_tensor(t($x))))")
  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]

  return transpose(ŷ)
end

function probs(model::RTorchModel, x::AbstractArray) 
  if model.likelihood == :classification_binary
    return σ.(logits(model, x))
  elseif model.likelihood == :classification_multi
    return softmax(logits(model, x)) 
  end
end

r_torch_model = RTorchModel(R"model", :classification_multi)
```

### Declare ∂ℓ
```{julia}
import CounterfactualExplanations.Generators: ∂ℓ

# Loss function for RCall model
function ∂ℓ(
    generator::AbstractGradientBasedGenerator, 
    model::RTorchModel, 
    ce::CounterfactualExplanation
  ) 
  x = ce.x
  target = Float32.(ce.target_encoded)

  model_nn = model.nn

  R"""
  x <- torch_tensor(t($x), requires_grad=TRUE)
  target <- torch_tensor(t($target))
  output <- $model_nn(x)
  obj_loss <- nnf_binary_cross_entropy_with_logits(output, target)
  obj_loss$backward()
  """

  grad = RCall.rcopy(R"t(as_array(x$grad))")
  grad = Float32.(grad)

  return grad
end
```

## Testing models

### Select random factual
```{julia}
# Randomly selected factual:
Random.seed!(42)

target = 0
factual = 1

y_chosen_factual_idx = rand(
  findall(
    predict_label(r_torch_model, counterfactual_data) .== factual
  )
)

x_random_factual = select_factual(counterfactual_data, y_chosen_factual_idx)
```

### Generate counterfactual
```{julia}
# Define generator:
generator = GenericGenerator()
# Generate recourse:
counterfactual = generate_counterfactual(
  x_random_factual, 
  target, 
  counterfactual_data, 
  r_torch_model, 
  generator
)
```

```{julia}
CounterfactualExplanations.plot(counterfactual; title = "RTorch path")
```

## Save RTorch model

```{julia}
m = r_torch_model.nn

R"""
library(torchvision)
model = $m
print(model)
save(model, file = "dev/R_call_implementation/model.pth")
"""
```

## Load RTorch model

```{julia}
R"""
loaded_model <- load("dev/R_call_implementation/model.pth")
print(loaded_model)
"""
```

## Interoperatbility

```{julia}
re = pyimport("re")
words = re.findall("[a-zA-Z]+", "PythonCall.jl is very useful!")

torch = pyimport("torch")
```

