# Setup

```{julia}
import Pkg
Pkg.activate("$(pwd())/dev/R_call_implementation")

using RCall
using CounterfactualExplanations
using Revise
```

# Declare RTorchModel
```{julia}
using CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs

struct RTorchModel <: AbstractDifferentiableModel
    nn::Any
end

function logits(M::RTorchModel, X::AbstractArray)
  nn = M.nn
  ŷ = rcopy(R"as_array($nn(torch_tensor(t($X))))")
  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]
  return ŷ'
end

probs(M::RTorchModel, X::AbstractArray)= σ.(logits(M, X))
```
```{julia}
import CounterfactualExplanations.Generators: ∂ℓ

# Loss function for RCall model
function ∂ℓ(generator::AbstractGradientBasedGenerator, M::RTorchModel, counterfactual_state::CounterfactualExplanation) 
    nn = M.nn
    x = counterfactual_state.x
    t = counterfactual_state.target_encoded

    R"""
    x <- torch_tensor($x, requires_grad=TRUE)
    output <- $nn(x)
    obj_loss <- nnf_binary_cross_entropy_with_logits(output,$t)
    obj_loss$backward()
    """

    grad = RCall.rcopy(R"as_array(x$grad)")

    return grad
end
```

# Legacy notebook

To get started we will first load some two-dimensional toy data:

```{julia}
Random.seed!(42);
N = 100

counterfactual_data = CounterfactualExplanations.Data.load_moons(N)

x = counterfactual_data.X
y = counterfactual_data.y
```

## `torch` model trained in R

The code below builds a simple MLP in R:

!!!PROBLEM: `ERROR: REvalError: Error in library(torch) : there is no package called 'torch'`
```{julia}
using RCall
R"""
# Data
library(torch)
X <- torch_tensor(t($x))
ys <- torch_tensor($y)

# Model:
mlp <- nn_module(
  initialize = function() {
    self$layer1 <- nn_linear(2, 32)
    self$layer2 <- nn_linear(32, 1)
  },
  forward = function(input) {
    input <- self$layer1(input)
    input <- nnf_sigmoid(input)
    input <- self$layer2(input)
    input
  }
)

model <- mlp()
optimizer <- optim_adam(model$parameters, lr = 0.1)
loss_fun <- nnf_binary_cross_entropy_with_logits
"""
```

The following code trains the MLP for the binary prediction task at hand:

```{julia}
R"""
for (epoch in 1:100) {
  model$train()  
  # Compute prediction and loss:
  output <- model(X)[,1]
  loss <- loss_fun(output, ys)
  # Backpropagation:
  optimizer$zero_grad()
  loss$backward()
  optimizer$step()
  
  cat(sprintf("Loss at epoch %d: %7f\n", epoch, loss$item()))
}
"""
```

```{julia}
M = RTorchModel(R"model")
```

### Making the model compatible

As always we need to extend the `logits` and `probs` functions to make the model compatible with `CounterfactualExplanations.jl`. As evident from the code below, this is actually quite straight-forward: the logits are returned by the `torch` model and copied form R into the Julia environment. Probabilities are then computed in Julia, by passing the logits through the sigmoid function.

```{julia}
using Flux
using CounterfactualExplanations, CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs # import functions in order to extend
# Step 1)
struct MyRTorchModel <: Models.AbstractDifferentiableModel
    nn::Any
end
# Step 2)
function logits(M::MyRTorchModel, X::AbstractArray)
  nn = M.nn
  ŷ = rcopy(R"as_array($nn(torch_tensor(t($X))))")
  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]
  return ŷ'
end
probs(M::MyRTorchModel, X::AbstractArray)= σ.(logits(M, X))
M = MyRTorchModel(R"model")
```

### Adapting the generator

Next we need to do a tiny bit of work on the `AbstractGenerator` side. By default methods underlying the `GenericGenerator` are desiged to work with models that have gradient access through `Zygote.jl`, one of Julia's main autodifferentiation packages. Of course, `Zygote.jl` cannot access the gradients of our `torch` model, so we need to adapt the code slightly. Fortunately, it turns out that all we need to do is extend the function that computes the gradient with respect to the loss function for the generic counterfactual search: `∂ℓ(generator::GenericGenerator, x′, M, t)`. In particular, we will extend the function by a method that is specific to the `MyRTorchModel` type we defined above. The code below implements this: our new method `∂ℓ` calls R in order to use `torch`'s autodifferentiation functionality for computing the gradient. 

```{julia}
import CounterfactualExplanations.Generators: ∂ℓ
using LinearAlgebra
# Counterfactual loss:
function ∂ℓ(generator::AbstractGradientBasedGenerator, M::MyRTorchModel, counterfactual_state::CounterfactualState) 
  nn = M.nn
  x′ = counterfactual_state.x′
  t = counterfactual_state.target_encoded
  R"""
  x <- torch_tensor($x′, requires_grad=TRUE)
  output <- $nn(x)
  obj_loss <- nnf_binary_cross_entropy_with_logits(output,$t)
  obj_loss$backward()
  """
  grad = rcopy(R"as_array(x$grad)")
  return grad
end
```

### Generating counterfactuals

From here on onwards we use the `CounterfactualExplanations.jl` functionality as always. Below we choose a random sample, define our generic generator and finally run the search:

```{julia}
# Randomly selected factual:
Random.seed!(123)
x = select_factual(counterfactual_data, rand(1:length(xs))) 
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target
```

```{julia}
# Define generator:
generator = GenericGenerator()
# Generate recourse:
counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)
```

```{julia}
#| echo: false
T = total_steps(counterfactual)
X_path = reduce(hcat,path(counterfactual))
ŷ = target_probs(counterfactual,X_path)
p1 = plot_contour(X',ys,M;colorbar=false, title="Posterior predictive")
anim = @animate for t in 1:T
    scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label="")
    p2 = plot(1:t, ŷ[1:t], xlim=(0,T), ylim=(0, 1), label="p(y′=" * string(target) * ")", title="Validity", lc=:black)
    Plots.abline!(p2,0,counterfactual.params[:γ],label="threshold γ", ls=:dash) # decision boundary
    plot(p1,p2,size=(800,400))
end
gif(anim, joinpath(www_path, "interop_r.gif"), fps=5)
```

