# Notebook for testing the pytorch model

## Set up

### Install dependencies
```{julia}
using Pkg
Pkg.activate("$(pwd())/dev/pytorch_test")

# Dependencies
using Revise
using PythonCall
using CounterfactualExplanations
using Random
```

### Load data
```{julia}
Random.seed!(42);
N = 100

counterfactual_data = CounterfactualExplanations.Data.load_moons(N)

x_julia = counterfactual_data.X
y_julia = counterfactual_data.y
```

### Convert data to tensors
```{julia}
torch = PythonCall.pyimport("torch")
np = PythonCall.pyimport("numpy")

x_python = Float32.(counterfactual_data.X)
x_python = np.array(x_python)
x_python = torch.tensor(x_python).T

y_python = Float32.(counterfactual_data.y)
y_python = np.array(y_python)
y_python = torch.tensor(y_python)
```

## Implement and train PyTorch model

### PyTorch model declaration 
```{julia}
MLP = PythonCall.pytype("MLP", (torch.nn.Module,), [
    "__module__" => "__main__",

    pyfunc(
        name = "__init__",
        function(self)
            torch.nn.Module.__init__(self)
            self.model = torch.nn.Sequential(
                torch.nn.Flatten(),
                torch.nn.Linear(2, 32),
                torch.nn.Sigmoid(),
                torch.nn.Linear(32, 2)
            )
            return
        end
    ),

    pyfunc(
        name = "forward",
        function(self, x)
            return self.model(x)
        end
    )
])
```

### Instantiate PyTorch model, optimizer and loss function
```{julia}
model = MLP()
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)
loss_fun = torch.nn.BCEWithLogitsLoss()
```

### Training
```{julia}
for epoch in 1:100
    # Compute prediction and loss:
    output = model(x_python).squeeze()
    loss = loss_fun(output, y_python.t())
    # Backpropagation:
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch % 5 == 0)
        @info "Loss at epoch $epoch: $(loss.item())"
    end
end
```

## Declare MyPyTorchModel

### MyPyTorch class
```{julia}
using Flux
using CounterfactualExplanations, CounterfactualExplanations.Models
import CounterfactualExplanations.Models: logits, probs

struct MyPyTorchModel <: AbstractDifferentiableModel
    nn::Any
    likelihood::Symbol
end

function logits(model::MyPyTorchModel, x::AbstractArray)    
    if !isa(x, Matrix)
      x = reshape(x, length(x), 1)
    end

    ŷ_python = model.nn(torch.tensor(np.array(x)).T).detach().numpy()
    ŷ = pyconvert(Matrix, ŷ_python)

    return transpose(ŷ)
end

function probs(model::MyPyTorchModel, x::AbstractArray)
    if model.likelihood == :classification_binary
        return σ.(logits(model, x))
    elseif model.likelihood == :classification_multi
        return softmax(logits(model, x))
    end
end

model_pytorch = MyPyTorchModel(model, :classification_multi)
```

### Declare ∂ℓ
```{julia}
import CounterfactualExplanations.Generators: ∂ℓ
using LinearAlgebra

function ∂ℓ(
        generator::AbstractGradientBasedGenerator, 
        M::MyPyTorchModel, 
        counterfactual_state::CounterfactualExplanation
    )

    x = counterfactual_state.x
    target = Float32.(counterfactual_state.target_encoded)

    x = to_tensor(x)
    x.requires_grad = true

    target = to_tensor(target)
    target = target.squeeze()

    output = M.nn(x).squeeze()

    obj_loss = loss_fun(output, target)
    obj_loss.backward()

    grad = pyconvert(Matrix, x.grad.t().detach().numpy())

    return grad
end

function to_tensor(array::AbstractArray)
    reshaped_array = reshape(array, 1, length(array))
    np_array = np.array(reshaped_array)
    tensor_array = torch.tensor(np_array)
    return tensor_array
end
```

## Create a flux mlp model

### Set up the NN_flux
```{julia}
# Data: x_julia, y_julia
data = Flux.DataLoader((x_julia, y_julia), batchsize = 1)
input_dim = size(x_julia, 1)
n_hidden = 32
activation = relu
output_dim = 2

nn_flux = Chain(
    Dense(input_dim, n_hidden, activation),
    Dropout(0.1),
    Dense(n_hidden, output_dim)
)

loss(yhat, y_julia) = Flux.Losses.logitbinarycrossentropy(nn_flux(yhat), y_julia)
```

### Train the NN_flux
```{julia}
using Flux.Optimise: update!, Adam
using Statistics

opt = Adam()
avg_loss(data) = Statistics.mean(map(d -> loss(d[1],d[2]), data))

# Training:
for epoch = 1:100
    for d in data
        gs = gradient(Flux.params(nn_flux)) do
            l = loss(d...)
    end
        update!(opt, Flux.params(nn_flux), gs)
    end

    if epoch % 5 == 0
        @info "Epoch " * string(epoch) 
        @show avg_loss(data)
    end
end

M_flux = FluxModel(nn_flux, :classification_multi)
```

## Testing models

### Select random factual
```{julia}
Random.seed!(42)

target = 0
factual = 1

@info "Choose factual's index"
y_chosen_factual_idx = rand(
    findall(
        predict_label(model_pytorch, counterfactual_data) .== factual
    )
)

@info "Select factual"
x_random_factual = select_factual(counterfactual_data, y_chosen_factual_idx)
```

### Generate counterfactual
```{julia}
# Define a generator:
generator = GenericGenerator()

# Generate a recourse:
@info "Generate counterfactual_flux"
counterfactual_flux = generate_counterfactual(
    x_random_factual, 
    target, 
    counterfactual_data, 
    M_flux, 
    generator;
    max_iter=1000
)

@info "Generate counterfactual_pytorch"
counterfactual_pytorch = generate_counterfactual(
    x_random_factual, 
    target, 
    counterfactual_data, 
    model_pytorch, 
    generator;
    max_iter=1000
)

```

### Plot model and datapoints only
```{julia}
CounterfactualExplanations.plot(M_flux, counterfactual_data; title = "Flux model")
```
```{julia}
CounterfactualExplanations.plot(model_pytorch, counterfactual_data; title = "PyTorch model")
```

### Plot
```{julia}
CounterfactualExplanations.plot(counterfactual_flux; title = "Flux path")
```

```{julia}
CounterfactualExplanations.plot(counterfactual_pytorch; title = "PyTorch path")
```