{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a521610f-12dc-4e6e-80e7-b51328794d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\drobi\\Desktop\\uni\\master_thesis\\CounterfactualExplanations.jl\\dev\\notebooks\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\drobi\\Desktop\\uni\\master_thesis\\CounterfactualExplanations.jl\\dev\\notebooks\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"CUDNN_jll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de19327-1494-4f9c-a799-d5551fac5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using Transformers\n",
    "using Transformers.TextEncoders\n",
    "using Transformers.HuggingFace\n",
    "using TrillionDollarWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e29646-4837-4121-a9fe-6426a352811e",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c6c853-d63f-4f48-b188-0d12d9a11be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>10×7 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">sentence</th><th style = \"text-align: left;\">year</th><th style = \"text-align: left;\">label</th><th style = \"text-align: left;\">seed</th><th style = \"text-align: left;\">sentence_splitting</th><th style = \"text-align: left;\">event_type</th><th style = \"text-align: left;\">split</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"InlineStrings.String7\" style = \"text-align: left;\">String7</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th><th title = \"InlineStrings.String31\" style = \"text-align: left;\">String31</th><th title = \"InlineStrings.String7\" style = \"text-align: left;\">String7</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">remained well below their levels at the beginning of the year, and that weaker demand and earlier declines in oil prices had been holding down consumer price inflation.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">hawkish</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">A few participants also noted that uncertainty about the extent of resource slack in the economy was considerable and that it was quite possible that the economy could soon be operating close to potential, particularly if labor force participation rates did not turn up much while employment continued to register gains.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">neutral</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">inflation was projected to pick up gradually in association with a partial reversal of the decline in energy prices this year.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">neutral</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">They noted that the realization of such a development could make it harder for the Committee to achieve 2 percent inflation over the longer run.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">neutral</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">In the view of one member, however, aggregate final demand was so strong that, with economic activity and the associated demand for labor having expanded at an unsustainable pace for some time, one could be reasonably confident that inflation would most likely pick up in the absence of policy action.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">hawkish</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">In the circumstances, most members endorsed a proposal to delete as no longer necessary the previous summary statement relating to the risks to growth and inflation taken together.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">neutral</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">In the staff forecast prepared for this meeting, the economy was seen as likely to expand at a moderate pace, supported by accommodative monetary policy and financial conditions.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">dovish</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">Housing starts and the demand for new homes had declined further, house prices in many parts of the country were falling faster than they had towards the end of 2007, and inventories of unsold homes remained quite elevated.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">dovish</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">Pressures on resources would rise as the anticipated upturn and possible above-trend growth brought the economy closer to full capacity utilization.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">hawkish</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">Price inflation had picked up a little but, abstracting from energy, had remained relatively subdued.</td><td style = \"text-align: right;\">1996</td><td style = \"text-align: left;\">neutral</td><td style = \"text-align: right;\">5768</td><td style = \"text-align: right;\">true</td><td style = \"text-align: left;\">meeting minutes</td><td style = \"text-align: left;\">test</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& sentence & \\\\\n",
       "\t\\hline\n",
       "\t& String & \\\\\n",
       "\t\\hline\n",
       "\t1 & remained well below their levels at the beginning of the year, and that weaker demand and earlier declines in oil prices had been holding down consumer price inflation. & $\\dots$ \\\\\n",
       "\t2 & A few participants also noted that uncertainty about the extent of resource slack in the economy was considerable and that it was quite possible that the economy could soon be operating close to potential, particularly if labor force participation rates did not turn up much while employment continued to register gains. & $\\dots$ \\\\\n",
       "\t3 & inflation was projected to pick up gradually in association with a partial reversal of the decline in energy prices this year. & $\\dots$ \\\\\n",
       "\t4 & They noted that the realization of such a development could make it harder for the Committee to achieve 2 percent inflation over the longer run. & $\\dots$ \\\\\n",
       "\t5 & In the view of one member, however, aggregate final demand was so strong that, with economic activity and the associated demand for labor having expanded at an unsustainable pace for some time, one could be reasonably confident that inflation would most likely pick up in the absence of policy action. & $\\dots$ \\\\\n",
       "\t6 & In the circumstances, most members endorsed a proposal to delete as no longer necessary the previous summary statement relating to the risks to growth and inflation taken together. & $\\dots$ \\\\\n",
       "\t7 & In the staff forecast prepared for this meeting, the economy was seen as likely to expand at a moderate pace, supported by accommodative monetary policy and financial conditions. & $\\dots$ \\\\\n",
       "\t8 & Housing starts and the demand for new homes had declined further, house prices in many parts of the country were falling faster than they had towards the end of 2007, and inventories of unsold homes remained quite elevated. & $\\dots$ \\\\\n",
       "\t9 & Pressures on resources would rise as the anticipated upturn and possible above-trend growth brought the economy closer to full capacity utilization. & $\\dots$ \\\\\n",
       "\t10 & Price inflation had picked up a little but, abstracting from energy, had remained relatively subdued. & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m10×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m sentence                          \u001b[0m\u001b[1m year  \u001b[0m\u001b[1m label   \u001b[0m\u001b[1m seed  \u001b[0m\u001b[1m sentence_spli\u001b[0m ⋯\n",
       "     │\u001b[90m String                            \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Bool         \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ remained well below their levels…   1996  hawkish   5768                ⋯\n",
       "   2 │ A few participants also noted th…   1996  neutral   5768\n",
       "   3 │ inflation was projected to pick …   1996  neutral   5768\n",
       "   4 │ They noted that the realization …   1996  neutral   5768\n",
       "   5 │ In the view of one member, howev…   1996  hawkish   5768                ⋯\n",
       "   6 │ In the circumstances, most membe…   1996  neutral   5768\n",
       "   7 │ In the staff forecast prepared f…   1996  dovish    5768\n",
       "   8 │ Housing starts and the demand fo…   1996  dovish    5768\n",
       "   9 │ Pressures on resources would ris…   1996  hawkish   5768                ⋯\n",
       "  10 │ Price inflation had picked up a …   1996  neutral   5768\n",
       "\u001b[36m                                                               3 columns omitted\u001b[0m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "data = load_training_sentences()\n",
    "texts = filter(:split => n -> n == \"test\", data)[1:n, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716f911-920d-468b-92e8-8ca639367303",
   "metadata": {},
   "source": [
    "### Get attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a7c85-2a74-41bf-ad75-899f3317dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using CondaPkg\n",
    "# CondaPkg.add(\"pytorch\")\n",
    "# CondaPkg.add(\"transformers\"; version=\"4.15.0\")\n",
    "# CondaPkg.add(\"transformers-interpret\")\n",
    "# CondaPkg.add(\"cuDNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a605169-6109-41fb-973b-7d5a044e71f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    CondaPkg \u001b[22m\u001b[39m\u001b[0mFound dependencies: C:\\Users\\drobi\\Desktop\\uni\\master_thesis\\CounterfactualExplanations.jl\\dev\\notebooks\\CondaPkg.toml\n",
      "\u001b[32m\u001b[1m    CondaPkg \u001b[22m\u001b[39m\u001b[0mFound dependencies: C:\\Users\\drobi\\.julia\\packages\\PythonCall\\wXfah\\CondaPkg.toml\n",
      "\u001b[32m\u001b[1m    CondaPkg \u001b[22m\u001b[39m\u001b[0mDependencies already up to date\n"
     ]
    }
   ],
   "source": [
    "using PythonCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c9f296-f8bb-4142-a7db-ece5061b635e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Python: <module 'transformers' from 'C:\\\\Users\\\\drobi\\\\Desktop\\\\uni\\\\master_thesis\\\\CounterfactualExplanations.jl\\\\dev\\\\notebooks\\\\.CondaPkg\\\\env\\\\lib\\\\site-packages\\\\transformers\\\\__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers_interpret = PythonCall.pyimport(\"transformers_interpret\")\n",
    "transformers = PythonCall.pyimport(\"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ced2a6-e9dd-4179-b917-007e377cccd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Python: PreTrainedTokenizerFast(name_or_path='gtfintechlab/FOMC-RoBERTa', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained classifier and corresponding tokenizer\n",
    "model = transformers.RobertaForSequenceClassification.from_pretrained(\"model\", local_files_only=true)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gtfintechlab/FOMC-RoBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21ab1d00-867f-4bd4-a2f6-83e4940554be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Python: <transformers_interpret.explainers.sequence_classification.SequenceClassificationExplainer object at 0x0000026835C80DF0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer = transformers_interpret.SequenceClassificationExplainer(model, tokenizer, attribution_type=\"lig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec54e441-653c-44ef-9cf5-684267a663bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Python: [('<s>', 0.0), ('rem', -0.09617849663936606), ('ained', -0.29702715615956665), ('well', 0.25551209840018674), ('below', -0.4015987185377847), ('their', -0.013975036770146217), ('levels', -0.13794157805498133), ('at', -0.002103001077563767), ('the', 0.14926277592717438), ('beginning', 0.08228264530334284), ('of', 0.08006335674570937), ('the', 0.24284418735078794), ('year', 0.20003154332566433), (',', 0.13104142887853437), ('and', 0.17795123590947837), ('that', 0.177958452206897), ('weaker', 0.07827504560711215), ('demand', 0.06225654464144761), ('and', 0.20338402474663345), ('earlier', -0.2643532357406369), ('declines', 0.14990056209456956), ('in', -0.3258371366392156), ('oil', -0.06976564966318043), ('prices', 0.011989646035101107), ('had', -0.15355389676950998), ('been', -0.10492307025848874), ('holding', -0.07771222018418246), ('down', -0.024919108109520634), ('consumer', 0.09788758630638592), ('price', 0.011979387105458176), ('inflation', -0.09257150340664654), ('.', 0.15492799445950947), ('', -0.3214473479860127), ('</s>', 0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribs = scorer(texts[1, :].sentence, index=0, internal_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6680f1d-690d-45a0-b98c-17b4986dee46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34-element Vector{Tuple{String, Float64}}:\n",
       " (\"<s>\", 0.0)\n",
       " (\"rem\", -0.09617849663936606)\n",
       " (\"ained\", -0.29702715615956665)\n",
       " (\"well\", 0.25551209840018674)\n",
       " (\"below\", -0.4015987185377847)\n",
       " (\"their\", -0.013975036770146217)\n",
       " (\"levels\", -0.13794157805498133)\n",
       " (\"at\", -0.002103001077563767)\n",
       " (\"the\", 0.14926277592717438)\n",
       " (\"beginning\", 0.08228264530334284)\n",
       " (\"of\", 0.08006335674570937)\n",
       " (\"the\", 0.24284418735078794)\n",
       " (\"year\", 0.20003154332566433)\n",
       " ⋮\n",
       " (\"oil\", -0.06976564966318043)\n",
       " (\"prices\", 0.011989646035101107)\n",
       " (\"had\", -0.15355389676950998)\n",
       " (\"been\", -0.10492307025848874)\n",
       " (\"holding\", -0.07771222018418246)\n",
       " (\"down\", -0.024919108109520634)\n",
       " (\"consumer\", 0.09788758630638592)\n",
       " (\"price\", 0.011979387105458176)\n",
       " (\"inflation\", -0.09257150340664654)\n",
       " (\".\", 0.15492799445950947)\n",
       " (\"\", -0.3214473479860127)\n",
       " (\"</s>\", 0.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributions = pyconvert(Array{Tuple{String, Float64}}, attribs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25039fe-329f-46e4-b308-33bc42c734ea",
   "metadata": {},
   "source": [
    "### Mask the word attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48278c99-7368-4eaa-b26f-1b91674fc514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mfuse_unk is unsupported, the tokenization result might be slightly different in some cases.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Transformers.HuggingFace C:\\Users\\drobi\\.julia\\packages\\Transformers\\lD5nW\\src\\huggingface\\tokenizer\\utils.jl:42\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mmatch token `<mask>` require to match with space on either side but that is not implemented here, the tokenization result might be slightly different in some cases.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Transformers.HuggingFace C:\\Users\\drobi\\.julia\\packages\\Transformers\\lD5nW\\src\\huggingface\\tokenizer\\utils.jl:42\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaselineModel(GPT2TextEncoder(\n",
       "├─ TextTokenizer(MatchTokenization(CodeNormalizer(BPETokenization(GPT2Tokenization, bpe = CachedBPE(BPE(50000 merges))), codemap = CodeMap{UInt8 => UInt16}(3 code-ranges)), 5 patterns)),\n",
       "├─ vocab = Vocab{String, SizedArray}(size = 50265, unk = <unk>, unki = 4),\n",
       "├─ codemap = CodeMap{UInt8 => UInt16}(3 code-ranges),\n",
       "├─ startsym = <s>,\n",
       "├─ endsym = </s>,\n",
       "├─ padsym = <pad>,\n",
       "├─ trunc = 256,\n",
       "└─ process = Pipelines:\n",
       "  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n",
       "  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n",
       "  ╰─ target[(token, segment)] := SequenceTemplate{String}(<s>:<type=1> Input:<type=1> </s>:<type=1> (</s>:<type=1> Input:<type=1> </s>:<type=1>)...)(target.token)\n",
       "  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(256))(target.token)\n",
       "  ╰─ target[token] := TextEncodeBase.trunc_or_pad(256, <pad>, tail, tail)(target.token)\n",
       "  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n",
       "  ╰─ target := (target.token, target.attention_mask)\n",
       "), HGFRobertaForSequenceClassification(HGFRobertaModel(Chain(CompositeEmbedding(token = Embed(1024, 50265), position = ApplyEmbed(.+, FixedLenPositionEmbed(1024, 514), Transformers.HuggingFace.roberta_pe_indices(1,)), segment = ApplyEmbed(.+, Embed(1024, 1), Transformers.HuggingFace.bert_ones_like)), DropoutLayer<nothing>(LayerNorm(1024, ϵ = 1.0e-5))), Transformer<24>(PostNormTransformerBlock(DropoutLayer<nothing>(SelfAttention(MultiheadQKVAttenOp(head = 16, p = nothing), Fork<3>(Dense(W = (1024, 1024), b = true)), Dense(W = (1024, 1024), b = true))), LayerNorm(1024, ϵ = 1.0e-5), DropoutLayer<nothing>(Chain(Dense(σ = NNlib.gelu, W = (1024, 4096), b = true), Dense(W = (4096, 1024), b = true))), LayerNorm(1024, ϵ = 1.0e-5))), nothing), Branch{(:logit,) = (:hidden_state,)}(Chain(DropoutLayer<nothing>(Transformers.HuggingFace.FirstTokenPooler()), DropoutLayer<nothing>(Dense(σ = NNlib.tanh_fast, W = (1024, 1024), b = true)), Dense(W = (1024, 3), b = true)))), Transformers.HuggingFace.HGFConfig{:roberta, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Dict{Symbol, Any}}(:use_cache => true, :torch_dtype => \"float32\", :vocab_size => 50265, :output_hidden_states => true, :hidden_act => \"gelu\", :num_hidden_layers => 24, :num_attention_heads => 16, :classifier_dropout => nothing, :type_vocab_size => 1, :intermediate_size => 4096…))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = TrillionDollarWords.load_model(; output_hidden_states=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9badf647-2250-450d-95c5-e83f79117514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256-element Vector{String}:\n",
       " \"<s>\"\n",
       " \"rem\"\n",
       " \"ained\"\n",
       " \" well\"\n",
       " \" below\"\n",
       " \" their\"\n",
       " \" levels\"\n",
       " \" at\"\n",
       " \" the\"\n",
       " \" beginning\"\n",
       " \" of\"\n",
       " \" the\"\n",
       " \" year\"\n",
       " ⋮\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\"\n",
       " \"<pad>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = decode(cls.tkr, encode(cls.tkr, texts[1, :].sentence).token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acf928f4-af1f-4cce-9c85-bb51d0609f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attributions = []\n",
    "for (i, (dec_tok, attrib)) in enumerate(zip(toks, attributions))\n",
    "    if startswith(dec_tok, \"<\")\n",
    "        continue\n",
    "    elseif length(word_attributions) == 0 || startswith(dec_tok, \" \")\n",
    "        push!(word_attributions, ([i], [attrib[1]], [attrib[2]]))\n",
    "    else \n",
    "        last_processed = last(word_attributions)\n",
    "        push!(last_processed[1], i)\n",
    "        push!(last_processed[2], attrib[1])\n",
    "        push!(last_processed[3], attrib[2])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49be7015-c7c2-43de-83b3-a17468fbc778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28-element Vector{Any}:\n",
       " ([4], [\"well\"], [0.25551209840018674])\n",
       " ([12], [\"the\"], [0.24284418735078794])\n",
       " ([19], [\"and\"], [0.20338402474663345])\n",
       " ([13, 14], [\"year\", \",\"], [0.20003154332566433, 0.13104142887853437])\n",
       " ([16], [\"that\"], [0.177958452206897])\n",
       " ([15], [\"and\"], [0.17795123590947837])\n",
       " ([31, 32], [\"inflation\", \".\"], [-0.09257150340664654, 0.15492799445950947])\n",
       " ([21], [\"declines\"], [0.14990056209456956])\n",
       " ([9], [\"the\"], [0.14926277592717438])\n",
       " ([29], [\"consumer\"], [0.09788758630638592])\n",
       " ([10], [\"beginning\"], [0.08228264530334284])\n",
       " ([11], [\"of\"], [0.08006335674570937])\n",
       " ([17], [\"weaker\"], [0.07827504560711215])\n",
       " ⋮\n",
       " ([8], [\"at\"], [-0.002103001077563767])\n",
       " ([6], [\"their\"], [-0.013975036770146217])\n",
       " ([28], [\"down\"], [-0.024919108109520634])\n",
       " ([23], [\"oil\"], [-0.06976564966318043])\n",
       " ([27], [\"holding\"], [-0.07771222018418246])\n",
       " ([2, 3], [\"rem\", \"ained\"], [-0.09617849663936606, -0.29702715615956665])\n",
       " ([26], [\"been\"], [-0.10492307025848874])\n",
       " ([7], [\"levels\"], [-0.13794157805498133])\n",
       " ([25], [\"had\"], [-0.15355389676950998])\n",
       " ([20], [\"earlier\"], [-0.2643532357406369])\n",
       " ([22], [\"in\"], [-0.3258371366392156])\n",
       " ([5], [\"below\"], [-0.4015987185377847])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted = sort(word_attributions, by = x -> -maximum(x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e513d132-c2c8-4947-8ea8-2bda508c99b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Any}:\n",
       " ([4], [\"well\"], [0.25551209840018674])\n",
       " ([12], [\"the\"], [0.24284418735078794])\n",
       " ([19], [\"and\"], [0.20338402474663345])\n",
       " ([13, 14], [\"year\", \",\"], [0.20003154332566433, 0.13104142887853437])\n",
       " ([16], [\"that\"], [0.177958452206897])\n",
       " ([15], [\"and\"], [0.17795123590947837])\n",
       " ([31, 32], [\"inflation\", \".\"], [-0.09257150340664654, 0.15492799445950947])\n",
       " ([21], [\"declines\"], [0.14990056209456956])\n",
       " ([9], [\"the\"], [0.14926277592717438])\n",
       " ([29], [\"consumer\"], [0.09788758630638592])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(sorted, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a596f684-1b82-4823-987b-adc633545977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BertTextEncoder(\n",
       "├─ TextTokenizer(MatchTokenization(WordPieceTokenization(bert_uncased_tokenizer, WordPiece(vocab_size = 30522, unk = [UNK], max_char = 100)), 5 patterns)),\n",
       "├─ vocab = Vocab{String, SizedArray}(size = 30522, unk = [UNK], unki = 101),\n",
       "├─ startsym = [CLS],\n",
       "├─ endsym = [SEP],\n",
       "├─ padsym = [PAD],\n",
       "├─ trunc = 512,\n",
       "└─ process = Pipelines:\n",
       "  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n",
       "  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n",
       "  ╰─ target[(token, segment)] := SequenceTemplate{String}([CLS]:<type=1> Input[1]:<type=1> [SEP]:<type=1> (Input[2]:<type=2> [SEP]:<type=2>)...)(target.token)\n",
       "  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(512))(target.token)\n",
       "  ╰─ target[token] := TextEncodeBase.trunc_and_pad(512, [PAD], head, tail)(target.token)\n",
       "  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n",
       "  ╰─ target[segment] := TextEncodeBase.trunc_and_pad(512, 1, head, tail)(target.segment)\n",
       "  ╰─ target[segment] := TextEncodeBase.nested2batch(target.segment)\n",
       "  ╰─ target := (target.token, target.segment, target.attention_mask)\n",
       "), HGFBertForMaskedLM(HGFBertModel(Chain(CompositeEmbedding(token = Embed(768, 30522), position = ApplyEmbed(.+, FixedLenPositionEmbed(768, 512)), segment = ApplyEmbed(.+, Embed(768, 2), Transformers.HuggingFace.bert_ones_like)), DropoutLayer<nothing>(LayerNorm(768, ϵ = 1.0e-12))), Transformer<12>(PostNormTransformerBlock(DropoutLayer<nothing>(SelfAttention(MultiheadQKVAttenOp(head = 12, p = nothing), Fork<3>(Dense(W = (768, 768), b = true)), Dense(W = (768, 768), b = true))), LayerNorm(768, ϵ = 1.0e-12), DropoutLayer<nothing>(Chain(Dense(σ = NNlib.gelu, W = (768, 3072), b = true), Dense(W = (3072, 768), b = true))), LayerNorm(768, ϵ = 1.0e-12))), nothing), Branch{(:logit,) = (:hidden_state,)}(Chain(Dense(σ = NNlib.gelu, W = (768, 768), b = true), LayerNorm(768, ϵ = 1.0e-12), EmbedDecoder(Embed(768, 30522), bias = true)))), Transformers.HuggingFace.HGFConfig{:bert, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Nothing}(:_name_or_path => \"bert-base-uncased\", :architectures => [\"BertForMaskedLM\"], :attention_probs_dropout_prob => 0.1, :classifier_dropout => nothing, :gradient_checkpointing => false, :hidden_act => \"gelu\", :hidden_dropout_prob => 0.1, :hidden_size => 768, :initializer_range => 0.02, :intermediate_size => 3072…))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_model(; kwrgs...)\n",
    "    model_name = \"karoldobiczek/relitc-FOMC-CMLM\"\n",
    "    tkr = Transformers.load_tokenizer(model_name)\n",
    "    cfg = Transformers.HuggingFace.HGFConfig(Transformers.load_config(model_name); kwrgs...)\n",
    "    mod = Transformers.load_model(model_name, \"ForMaskedLM\"; config = cfg)\n",
    "\n",
    "    return tkr, mod, cfg\n",
    "end\n",
    "cmlm_tkr, cmlm_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8c6d2-6b2c-4a03-8b2f-345ffe7a3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sentence = encode(cls.tkr, texts[1, :].sentence)\n",
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faeb671-dead-4ba1-867f-39eaf990a507",
   "metadata": {},
   "source": [
    "### Fill in masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f760e4-68e9-44ef-aee6-654dedabe619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff831709-34d2-4823-9b6d-021a61eaf132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(hidden_state = Float32[-0.00065652834 0.148323 … -0.2656779 1.094993; 0.23470013 0.49248493 … -0.01542875 0.3666349; … ; -0.04797123 0.12968777 … 0.18314485 -0.5371204; -0.060299776 -0.20840645 … -0.004100313 -0.24968606;;; -0.39127347 -0.25087723 … -0.3486044 -0.3757742; -0.0037795287 -0.28035903 … 0.23318379 0.24946694; … ; 0.17403156 0.048295036 … 0.5432 0.53646606; 0.07622922 0.5221045 … 0.3127785 0.3254434], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[11, 9]), logit = Float32[-6.6425447 -11.8635645 … -11.268725 -13.800734; -6.5911 -11.931757 … -11.690113 -13.577938; … ; -5.8243623 -10.447554 … -10.825976 -10.625192; -4.114048 -9.63646 … -5.55217 -11.8213415;;; -6.785516 -10.138466 … -6.143492 -6.1645293; -6.736014 -10.05456 … -6.352455 -6.376718; … ; -5.999074 -6.742794 … -5.291667 -5.314741; -3.9786663 -5.861206 … -3.6715782 -3.697316])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = cmlm_model(encode(cmlm_tkr, [\"hello [MASK] world [MASK] [MASK] and my [MASK]!\", \"bonjour mes [MASK].\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "245bbd2a-c8e4-48c9-9ece-2c196d86bed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartesianIndex(102, 1)CartesianIndex(101, 1)CartesianIndex(104, 1)CartesianIndex(103, 1)"
     ]
    }
   ],
   "source": [
    "enc = encode(cmlm_tkr, [\"[UNK] [MASK]\"])\n",
    "for i in 1:size(enc.token)[2]\n",
    "    print(argmax(enc.token[:, i, :]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1a1381f-a69f-47dd-ad61-8609f1103bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11×2 Matrix{String}:\n",
       " \".\"        \".\"\n",
       " \"hello\"    \"bon\"\n",
       " \"to\"       \"##jou\"\n",
       " \"world\"    \"##s\"\n",
       " \"of\"       \"me\"\n",
       " \"me\"       \"##s\"\n",
       " \"and\"      \"##q\"\n",
       " \"my\"       \".\"\n",
       " \"friends\"  \".\"\n",
       " \"!\"        \"de\"\n",
       " \".\"        \"de\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = decode(cmlm_tkr, out.logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b014982f-8df6-4f10-b675-b60822dcfea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merge_truncated_words (generic function with 4 methods)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function merge_truncated_words(tokens, in_word=\"##\", between_word=\"\", separators=[\".\", \"de\"])\n",
    "    decoded = Vector{Char}()\n",
    "    for token in tokens\n",
    "        token = string(token)\n",
    "        if token in separators\n",
    "            continue\n",
    "        else\n",
    "            if startswith(token, in_word)\n",
    "                if length(decoded) == 0\n",
    "                    append!(decoded, token)\n",
    "                else\n",
    "                    last = pop!(decoded)\n",
    "                    new_token = last * chop(token, head=2, tail=0)\n",
    "                    append!(decoded, new_token)\n",
    "                end\n",
    "            else\n",
    "                append!(decoded, \" \" * token)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return decoded\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8adb05cb-84f0-4b14-a74b-1cfec686280a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" hello , world , me and my friends ! bonjous mesnant\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "String(merge_truncated_words(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8ee90-b306-4237-a76f-dab338639c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
