{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b3440e-acd9-4c06-ad66-acd36c663ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c619a7-9c69-4952-9b7f-a02509f7b896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_model (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_model(; load_head = true, kwrgs...)\n",
    "    model_name = \"karoldobiczek/relitc-FOMC-CMLM\"\n",
    "    tkr = Transformers.load_tokenizer(model_name)\n",
    "    cfg = Transformers.HuggingFace.HGFConfig(Transformers.load_config(model_name); kwrgs...)\n",
    "    mod = Transformers.load_model(model_name, \"ForMaskedLM\"; config = cfg)\n",
    "\n",
    "    return tkr, mod, cfg\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1fa980d-7508-469d-8f0c-db26bd82cc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BertTextEncoder(\n",
       "├─ TextTokenizer(MatchTokenization(WordPieceTokenization(bert_uncased_tokenizer, WordPiece(vocab_size = 30522, unk = [UNK], max_char = 100)), 5 patterns)),\n",
       "├─ vocab = Vocab{String, SizedArray}(size = 30522, unk = [UNK], unki = 101),\n",
       "├─ startsym = [CLS],\n",
       "├─ endsym = [SEP],\n",
       "├─ padsym = [PAD],\n",
       "├─ trunc = 512,\n",
       "└─ process = Pipelines:\n",
       "  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n",
       "  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n",
       "  ╰─ target[(token, segment)] := SequenceTemplate{String}([CLS]:<type=1> Input[1]:<type=1> [SEP]:<type=1> (Input[2]:<type=2> [SEP]:<type=2>)...)(target.token)\n",
       "  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(512))(target.token)\n",
       "  ╰─ target[token] := TextEncodeBase.trunc_and_pad(512, [PAD], head, tail)(target.token)\n",
       "  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n",
       "  ╰─ target[segment] := TextEncodeBase.trunc_and_pad(512, 1, head, tail)(target.segment)\n",
       "  ╰─ target[segment] := TextEncodeBase.nested2batch(target.segment)\n",
       "  ╰─ target := (target.token, target.segment, target.attention_mask)\n",
       "), HGFBertForMaskedLM(HGFBertModel(Chain(CompositeEmbedding(token = Embed(768, 30522), position = ApplyEmbed(.+, FixedLenPositionEmbed(768, 512)), segment = ApplyEmbed(.+, Embed(768, 2), Transformers.HuggingFace.bert_ones_like)), DropoutLayer<nothing>(LayerNorm(768, ϵ = 1.0e-12))), Transformer<12>(PostNormTransformerBlock(DropoutLayer<nothing>(SelfAttention(MultiheadQKVAttenOp(head = 12, p = nothing), Fork<3>(Dense(W = (768, 768), b = true)), Dense(W = (768, 768), b = true))), LayerNorm(768, ϵ = 1.0e-12), DropoutLayer<nothing>(Chain(Dense(σ = NNlib.gelu, W = (768, 3072), b = true), Dense(W = (3072, 768), b = true))), LayerNorm(768, ϵ = 1.0e-12))), nothing), Branch{(:logit,) = (:hidden_state,)}(Chain(Dense(σ = NNlib.gelu, W = (768, 768), b = true), LayerNorm(768, ϵ = 1.0e-12), EmbedDecoder(Embed(768, 30522), bias = true)))), Transformers.HuggingFace.HGFConfig{:bert, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Nothing}(:_name_or_path => \"bert-base-uncased\", :architectures => [\"BertForMaskedLM\"], :attention_probs_dropout_prob => 0.1, :classifier_dropout => nothing, :gradient_checkpointing => false, :hidden_act => \"gelu\", :hidden_dropout_prob => 0.1, :hidden_size => 768, :initializer_range => 0.02, :intermediate_size => 3072…))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkr, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f83d8a1-bb74-445c-ba2f-908fe19f5149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(token = Bool[0 0 … 0 0; 0 0 … 0 0; … ; 0 0 … 0 0; 0 0 … 0 0], segment = [1, 1, 1, 1, 1, 1], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[6]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = TextEncoders.encode(tkr, \"[SEP] hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3caedfa7-6d74-468a-8d58-42cc0ace447c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(hidden_state = Float32[-0.0068912334 1.0873084 … -0.60200036 1.0715828; 0.12056827 0.10844539 … -0.6849348 0.116397835; … ; -0.026128935 -0.5781372 … -0.010042516 -0.5654973; 0.13880502 -0.25988412 … -0.09458274 -0.27784675;;;], attention_mask = NeuralAttentionlib.LengthMask{1, Vector{Int32}}(Int32[6]), logit = Float32[-6.8121076 -13.4576 … -11.932923 -13.186207; -6.7392855 -13.34565 … -12.13822 -13.120222; … ; -6.0473905 -10.81515 … -10.769718 -10.597319; -4.2169976 -12.677718 … -3.9266496 -12.219977;;;])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af5e931b-d4cd-4d65-a3e8-79b43bc60f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×1 Matrix{String}:\n",
       " \".\"\n",
       " \".\"\n",
       " \"hello\"\n",
       " \"world\"\n",
       " \"!\"\n",
       " \".\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextEncoders.decode(tkr, out.logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d9c78-b952-43f0-b1c0-764520bb2acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
