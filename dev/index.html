<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>🏠 Home · CounterfactualExplanations.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://pat-alt.github.io/CounterfactualExplanations.jl/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img src="assets/logo.svg" alt="CounterfactualExplanations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href>CounterfactualExplanations.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>🏠 Home</a><ul class="internal"><li><a class="tocitem" href="#Installation"><span>Installation 🚩</span></a></li><li><a class="tocitem" href="#Background-and-Motivation"><span>Background and Motivation</span></a></li><li><a class="tocitem" href="#Usage-example"><span>Usage example 🔍</span></a></li><li><a class="tocitem" href="#Implemented-Counterfactual-Generators:"><span>Implemented Counterfactual Generators:</span></a></li><li><a class="tocitem" href="#Goals-and-limitations"><span>Goals and limitations 🎯</span></a></li><li><a class="tocitem" href="#Contribute"><span>Contribute 🛠</span></a></li><li><a class="tocitem" href="#Citation"><span>Citation 🎓</span></a></li><li><a class="tocitem" href="#References"><span>References 📚</span></a></li></ul></li><li><span class="tocitem">🫣 Tutorials</span><ul><li><a class="tocitem" href="tutorials/_index/">Overview</a></li><li><a class="tocitem" href="tutorials/whistle_stop/">Whiste-Stop Tour</a></li><li><a class="tocitem" href="tutorials/data_preprocessing/">Handling Data</a></li><li><a class="tocitem" href="tutorials/data_catalogue/">Data Catalogue</a></li><li><a class="tocitem" href="tutorials/models/">Handling Models</a></li><li><a class="tocitem" href="tutorials/model_catalogue/">Model Catalogue</a></li></ul></li><li><span class="tocitem">🤓 Explanation</span><ul><li><a class="tocitem" href="explanation/_index/">Overview</a></li><li><a class="tocitem" href="explanation/architecture/">Package Architecture</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Generators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="explanation/generators/overview/">Overview</a></li><li><a class="tocitem" href="explanation/generators/generic/">Generic</a></li><li><a class="tocitem" href="explanation/generators/gravitational/">Gravitational</a></li><li><a class="tocitem" href="explanation/generators/revise/">REVISE</a></li><li><a class="tocitem" href="explanation/generators/dice/">DiCE</a></li><li><a class="tocitem" href="explanation/generators/clap_roar/">ClaPROAR</a></li><li><a class="tocitem" href="explanation/generators/greedy/">Greedy</a></li></ul></li><li><a class="tocitem" href="explanation/categorical/">Categorical Features</a></li></ul></li><li><span class="tocitem">🫡 How-To ...</span><ul><li><a class="tocitem" href="how_to_guides/_index/">Overview</a></li><li><a class="tocitem" href="how_to_guides/custom_generators/">... add custom generators</a></li><li><a class="tocitem" href="how_to_guides/custom_models/">... add custom models</a></li></ul></li><li><a class="tocitem" href="_reference/">🧐 Reference</a></li><li><a class="tocitem" href="_contribute/">🛠 Contribute</a></li><li><a class="tocitem" href="assets/_resources/">📚 Additional Resources</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>🏠 Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>🏠 Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/pat-alt/CounterfactualExplanations.jl/blob/main/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="CounterfactualExplanations"><a class="docs-heading-anchor" href="#CounterfactualExplanations">CounterfactualExplanations</a><a id="CounterfactualExplanations-1"></a><a class="docs-heading-anchor-permalink" href="#CounterfactualExplanations" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/pat-alt/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a>.</p><p><code>CounterfactualExplanations.jl</code> is a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box algorithms. Both CE and AR are related tools for explainable artificial intelligence (XAI). While the package is written purely in Julia, it can be used to explain machine learning algorithms developed and trained in other popular programming languages like Python and R. See below for a short introduction and other resources or dive straight into the <a href="https://pat-alt.github.io/CounterfactualExplanations.jl/dev">docs</a>.</p><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation 🚩</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>You can install the stable release from <a href="https://github.com/JuliaRegistries/General">Julia’s General Registry</a> as follows:</p><pre><code class="language-julia hljs">using Pkg
Pkg.add(&quot;CounterfactualExplanations&quot;)</code></pre><p><code>CounterfactualExplanations.jl</code> is under active development. To install the development version of the package you can run the following command:</p><pre><code class="language-julia hljs">using Pkg
Pkg.add(url=&quot;https://github.com/pat-alt/CounterfactualExplanations.jl&quot;)</code></pre><h2 id="Background-and-Motivation"><a class="docs-heading-anchor" href="#Background-and-Motivation">Background and Motivation</a><a id="Background-and-Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Background-and-Motivation" title="Permalink"></a></h2><h4 id="The-Need-for-Explainability"><a class="docs-heading-anchor" href="#The-Need-for-Explainability">The Need for Explainability ⬛</a><a id="The-Need-for-Explainability-1"></a><a class="docs-heading-anchor-permalink" href="#The-Need-for-Explainability" title="Permalink"></a></h4><p>Machine learning models like deep neural networks have become so complex, opaque and underspecified in the data that they are generally considered Black Boxes. Nonetheless, such models often play a key role in data-driven decision-making systems. This often creates the following problem: human operators in charge of such systems have to rely on them blindly, while those individuals subject to them generally have no way of challenging an undesirable outcome:</p><blockquote><p>“You cannot appeal to (algorithms). They do not listen. Nor do they bend.”</p><p>— Cathy O’Neil in <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction"><em>Weapons of Math Destruction</em></a>, 2016</p></blockquote><h4 id="Enter:-Counterfactual-Explanations"><a class="docs-heading-anchor" href="#Enter:-Counterfactual-Explanations">Enter: Counterfactual Explanations 🔮</a><a id="Enter:-Counterfactual-Explanations-1"></a><a class="docs-heading-anchor-permalink" href="#Enter:-Counterfactual-Explanations" title="Permalink"></a></h4><p>Counterfactual Explanations can help human stakeholders make sense of the systems they develop, use or endure: they explain how inputs into a system need to change for it to produce different decisions. Explainability benefits internal as well as external quality assurance. The figure below, for example, shows various counterfactuals generated through different approaches that all turn the predicted label of some classifier from a 9 into a 4.</p><p><img src="https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/examples/image/www/MNIST_9to4.png" alt="Realistic counterfactual explanations for MNIST data: turning a 4 into a 9."/></p><p>Explanations that involve realistic and actionable changes can be used for Algorithmic Recourse (AR): they offer human stakeholders a way to not only understand the system’s behaviour but also react to it or adjust it. The figure below illustrates the point of AR through a toy example: it shows the counterfactual path of one sad cat 🐱 that would like to be grouped with her cool dog friends. Unfortunately, based on her tail length and height she was classified as a cat by a black-box classifier. The recourse algorithm perturbs her features in such a way that she ends up crossing the decision boundary into a dense region inside the target class.</p><p><img src="https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/www/recourse_laplace.gif" alt="A sad 🐱 on its counterfactual path to its cool dog friends."/></p><p>Counterfactual Explanations have certain advantages over related tools for explainable artificial intelligence (XAI) like surrogate explainers (LIME and SHAP). These include:</p><ul><li>Full fidelity to the black-box model, since no proxy is involved.</li><li>No need for (reasonably) interpretable features as opposed to LIME and SHAP.</li><li>Clear link to Causal Inference and Bayesian Machine Learning.</li><li>Less susceptible to adversarial attacks than LIME and SHAP.</li></ul><h2 id="Usage-example"><a class="docs-heading-anchor" href="#Usage-example">Usage example 🔍</a><a id="Usage-example-1"></a><a class="docs-heading-anchor-permalink" href="#Usage-example" title="Permalink"></a></h2><p>Generating counterfactuals will typically look like follows:</p><pre><code class="language-julia hljs">using CounterfactualExplanations
using CounterfactualExplanations.Data
using CounterfactualExplanations.Models
using Random
Random.seed!(1234)

# Data and Classifier:
counterfactual_data = load_linearly_separable()
M = fit_model(counterfactual_data, :Linear)

# Randomly selected factual:
x = select_factual(counterfactual_data,rand(1:size(counterfactual_data.X,2)))
y = predict_label(M, counterfactual_data, x)[1]
target = counterfactual_data.y_levels[counterfactual_data.y_levels .!= y][1]

# Counterfactual search:
generator = GenericGenerator()</code></pre><p>Running the counterfactual search yields:</p><pre><code class="language-julia hljs">counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)</code></pre><pre><code class="nohighlight hljs">Convergence: ✅

 after 3 steps.</code></pre><h2 id="Implemented-Counterfactual-Generators:"><a class="docs-heading-anchor" href="#Implemented-Counterfactual-Generators:">Implemented Counterfactual Generators:</a><a id="Implemented-Counterfactual-Generators:-1"></a><a class="docs-heading-anchor-permalink" href="#Implemented-Counterfactual-Generators:" title="Permalink"></a></h2><p>Currently, the following counterfactual generators are implemented:</p><ul><li>Generic (Wachter, Mittelstadt, and Russell 2017)</li><li>Greedy (Schut et al. 2021)</li><li>DiCE (Mothilal, Sharma, and Tan 2020)</li><li>Latent Space Search as in REVISE (Joshi et al. 2019) and CLUE (Antorán et al. 2020)</li><li>ClaPROAR (upcoming IEEE SaTML ’23 paper)</li><li>GravitationalGenerator (upcoming IEEE SaTML ’23 paper)</li></ul><h2 id="Goals-and-limitations"><a class="docs-heading-anchor" href="#Goals-and-limitations">Goals and limitations 🎯</a><a id="Goals-and-limitations-1"></a><a class="docs-heading-anchor-permalink" href="#Goals-and-limitations" title="Permalink"></a></h2><p>The goal of this library is to contribute to efforts towards trustworthy machine learning in Julia. The Julia language has an edge when it comes to trustworthiness: it is very transparent. Packages like this one are generally written in pure Julia, which makes it easy for users and developers to understand and contribute to open-source code. Eventually, this project aims to offer a one-stop-shop of counterfactual explanations. We want to deliver a package that is at least at par with the <a href="https://github.com/carla-recourse/CARLA">CARLA</a> Python library in terms of its functionality. Currently, the package falls short of this goal in some ways:</p><ol><li>The number of counterfactual generators is still limited.</li><li>Mutability constraints are still not supported for Latent Space generators.</li></ol><p>Additionally, our ambition is to enhance the package through the following features:</p><ol><li>Language interoperability with Python and R: currently still only experimental.</li><li>Support for machine learning models trained in <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/"><code>MLJ.jl</code></a>.</li><li>Additional datasets for testing, evaluation and benchmarking.</li><li>Support for regression models.</li></ol><h2 id="Contribute"><a class="docs-heading-anchor" href="#Contribute">Contribute 🛠</a><a id="Contribute-1"></a><a class="docs-heading-anchor-permalink" href="#Contribute" title="Permalink"></a></h2><p>Contributions of any kind are very much welcome! If any of the below applies to you, this might be the right open-source project for you:</p><ul><li>You’re an expert in Counterfactual Explanations or Explainable AI more broadly and you are curious about Julia.</li><li>You’re experienced with Julia and are happy to help someone less experienced to up their game. Ideally, you are also curious about Trustworthy AI.</li><li>You’re new to Julia and open-source development and would like to start your learning journey by contributing to a recent and active development. Ideally, you are familiar with machine learning.</li></ul><p><a href="https://github.com/pat-alt">@pat-alt</a> here: I am still very much at the beginning of my Julia journey, so if you spot any issues or have any suggestions for design improvement, please just open <a href="https://github.com/pat-alt/CounterfactualExplanations.jl/issues">issue</a> or start a <a href="https://github.com/pat-alt/CounterfactualExplanations.jl/discussions">discussion</a>. Our goal is to provide a go-to place for counterfactual explanations in Julia.</p><p>For more details on how to contribute see <a href="https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/contributing/">here</a>. Please follow the <a href="https://github.com/SciML/ColPrac">SciML ColPrac guide</a>.</p><h2 id="Citation"><a class="docs-heading-anchor" href="#Citation">Citation 🎓</a><a id="Citation-1"></a><a class="docs-heading-anchor-permalink" href="#Citation" title="Permalink"></a></h2><p>If you want to use this codebase, please consider citing:</p><pre><code class="nohighlight hljs">@software{altmeyer2022CounterfactualExplanations,
  author = {Patrick Altmeyer},
  title = {{CounterfactualExplanations.jl - a Julia package for Counterfactual Explanations and Algorithmic Recourse}},
  url = {https://github.com/pat-alt/CounterfactualExplanations.jl},
  year = {2022}
}</code></pre><h2 id="References"><a class="docs-heading-anchor" href="#References">References 📚</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><p>Antorán, Javier, Umang Bhatt, Tameem Adel, Adrian Weller, and José Miguel Hernández-Lobato. 2020. “Getting a Clue: A Method for Explaining Uncertainty Estimates.” <a href="https://arxiv.org/abs/2006.06848">https://arxiv.org/abs/2006.06848</a>.</p><p>Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. “Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.” <a href="https://arxiv.org/abs/1907.09615">https://arxiv.org/abs/1907.09615</a>.</p><p>Mothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. “Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.” In <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 607–17.</p><p>Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. “Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.” In <em>International Conference on Artificial Intelligence and Statistics</em>, 1756–64. PMLR.</p><p>Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” <em>Harv. JL &amp; Tech.</em> 31: 841.</p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="tutorials/_index/">Overview »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Thursday 2 February 2023 06:52">Thursday 2 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
