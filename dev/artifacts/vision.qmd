# Vision Data and Models

```{julia}
#| echo: false
include("$(pwd())/dev/artifacts/setup.jl")
eval(setup)
www_path = www_dir("vision")
data_path = data_dir("vision")
model_path = model_dir("vision")
www_name = "www-vision"
data_name = "data-vision"
model_name = "model-vision"
```

## MNIST 

```{julia}
counterfactual_data = CounterfactualData(TaijaData.load_mnist()...)
X = counterfactual_data.X
y = counterfactual_data.output_encoder.y  
input_dim, n_obs = size(X)
```

### Classifiers

```{julia}
# MLP and Deep Ensemble
flux_training_params.batchsize = Int(round(n_obs/10))
flux_training_params.verbose = true
M = fit_model(counterfactual_data, :MLP, n_hidden=32)
M_ens = fit_model(counterfactual_data, :DeepEnsemble, n_hidden=32)
CounterfactualExplanations.reset!(flux_training_params)
```

```{julia}
test_data = CounterfactualData(TaijaData.load_mnist_test()...)
println("MLP test score (F1): $(model_evaluation(M, test_data))")
println("Deep Ensemble test score (F1): $(model_evaluation(M_ens, test_data))")
```

```{julia}
Serialization.serialize(joinpath(model_path,"mnist_mlp.jls"),M.fitresult())
Serialization.serialize(joinpath(model_path,"mnist_ensemble.jls"),M_ens.fitresult())
```

```{julia}
# Joint Energy Model
n_obs = 10000           # undersample to speed up training
data = counterfactual_data = CounterfactualData(TaijaData.load_mnist(n_obs)...)
data.domain = (-1.0, 1.0)
n_hidden = 32
α = [1.0, 1.0, 5e-2]
sampling_batchsize = 10
bs = 64
sampling_steps = 25
epochs = 10
M = Models.fit_model(
    data,:JEM;
    builder=MLJFlux.MLP(
        hidden=(n_hidden,), 
        σ=Flux.relu
    ),
    batch_size=bs,
    finaliser=Flux.softmax,
    loss=Flux.Losses.crossentropy,
    jem_training_params=(
        α=α,
        verbosity=epochs,
    ),
    epochs=epochs,
    sampling_steps=sampling_steps,
    sampling_batchsize=sampling_batchsize,
)
```

```{julia}
println("JEM test score (F1): $(model_evaluation(M, test_data))")
Serialization.serialize(joinpath(model_path,"mnist_jem.jls"),M.fitresult())
```

### Generative Model

```{julia}
using CounterfactualExplanations.GenerativeModels: VAE, train!
vae = VAE(input_dim; nll=Flux.Losses.mse, epochs=100, λ=0.0001, latent_dim=28, hidden_dim=50)
flux_training_params.verbose = true
train!(vae, X)
Serialization.serialize(joinpath(model_path,"mnist_vae_strong.jls"),vae)
```

```{julia}
vae = VAE(input_dim; nll=Flux.Losses.mse, epochs=100, λ=0.05, latent_dim=2, hidden_dim=5)
flux_training_params.verbose = true
train!(vae, X)
Serialization.serialize(joinpath(model_path,"mnist_vae_weak.jls"),vae)
```

## FashionMNIST 

```{julia}
counterfactual_data = CounterfactualData(TaijaData.load_fashion_mnist()...)
X = counterfactual_data.X
y = counterfactual_data.output_encoder.y  
input_dim, n_obs = size(X)
```

### Classifiers

```{julia}
flux_training_params.batchsize = Int(round(n_obs/10))
flux_training_params.verbose = true
M = fit_model(counterfactual_data, :MLP, n_hidden=64)
M_ens = fit_model(counterfactual_data, :DeepEnsemble, n_hidden=64)
CounterfactualExplanations.reset!(flux_training_params)
```

```{julia}
test_data = CounterfactualData(TaijaData.load_fashion_mnist_test()...)
println("MLP test score (F1): $(model_evaluation(M, test_data)[1])")
println("Deep Ensemble test score (F1): $(model_evaluation(M_ens, test_data)[1])")
```

```{julia}
Serialization.serialize(joinpath(model_path,"fashion_mnist_mlp.jls"),M.fitresult())
Serialization.serialize(joinpath(model_path,"fashion_mnist_ensemble.jls"),M_ens.fitresult())
```

### Generative Model

```{julia}
using CounterfactualExplanations.GenerativeModels: VAE, train!
vae = VAE(input_dim; nll=Flux.Losses.mse, epochs=100, λ=0.0001, latent_dim=32, hidden_dim=64)
flux_training_params.verbose = true
train!(vae, X)
Serialization.serialize(joinpath(model_path,"fashion_mnist_vae_strong.jls"),vae)
```

```{julia}
vae = VAE(input_dim; nll=Flux.Losses.mse, epochs=100, λ=0.05, latent_dim=2, hidden_dim=10)
flux_training_params.verbose = true
train!(vae, X)
Serialization.serialize(joinpath(model_path,"fashion_mnist_vae_weak.jls"),vae)
```

## CIFAR 10

```{julia}
counterfactual_data = CounterfactualData(TaijaData.load_cifar_10()...)
X = counterfactual_data.X
y = counterfactual_data.output_encoder.y  
input_dim, n_obs = size(X)
```

### Classifiers

```{julia}
flux_training_params.batchsize = Int(round(n_obs/10))
flux_training_params.verbose = true
M = fit_model(counterfactual_data, :MLP, n_hidden=64)
M_ens = fit_model(counterfactual_data, :DeepEnsemble, n_hidden=64)
CounterfactualExplanations.reset!(flux_training_params)
```

```{julia}
test_data = CounterfactualData(TaijaData.load_cifar_10_test()...)
println("MLP test score (F1): $(model_evaluation(M, test_data)[1])")
println("Deep Ensemble test score (F1): $(model_evaluation(M_ens, test_data)[1])")
```

```{julia}
Serialization.serialize(joinpath(model_path,"cifar_10_mlp.jls"),M.fitresult())
Serialization.serialize(joinpath(model_path,"cifar_10_ensemble.jls"),M_ens.fitresult())
```

### Generative Model

```{julia}
using CounterfactualExplanations.GenerativeModels: VAE, train!

vae = VAE(input_dim; nll=Flux.Losses.mse, epochs=100, λ=0.0001, latent_dim=32, hidden_dim=64)
flux_training_params.verbose = true
train!(vae, X)
Serialization.serialize(joinpath(model_path,"cifar_10_vae_strong.jls"),vae)
```

```{julia}
vae = VAE(input_dim; nll=Flux.Losses.mse, epochs=100, λ=0.05, latent_dim=2, hidden_dim=10)
flux_training_params.verbose = true
train!(vae, X)
Serialization.serialize(joinpath(model_path,"cifar_10_vae_weak.jls"),vae)
```

## Generate Artifacts

```{julia}
artifact_name = "$(model_name)"
generate_artifacts(model_path; artifact_name=artifact_name)
```