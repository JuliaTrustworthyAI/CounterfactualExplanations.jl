var documenterSearchIndex = {"docs":
[{"location":"resources/resources.commonmark/#Further-Resources","page":"-","title":"Further Resources","text":"","category":"section"},{"location":"resources/resources.commonmark/#JuliaCon-2022","page":"-","title":"JuliaCon 2022","text":"","category":"section"},{"location":"resources/resources.commonmark/","page":"-","title":"-","text":"Slides: link","category":"page"},{"location":"resources/resources.commonmark/","page":"-","title":"-","text":"@raw html <iframe style=\"width:560px;height:315px\" src=\"https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/resources/juliacon22/presentation.html\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"tutorials/generators/#Custom-generators","page":"Custom generators","title":"Custom generators","text":"","category":"section"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"As we will see in this short tutorial, building custom counterfactual generators is straight-forward. We hope that this will facilitate contributions through the community.","category":"page"},{"location":"tutorials/generators/#Generic-generator-with-dropout","page":"Custom generators","title":"Generic generator with dropout","text":"","category":"section"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"To illustrate how custom generators can be implemented we will consider a simple example of a generator that extends the functionality of our GenericGenerator. We have noted elsewhere that the effectiveness of ounterfactual explanations depends to some degree on the quality of the fitted model. Another, perhaps trivial, thing to note is that counterfactual explanations are not unique: there are potentially many valid counterfactual paths. One interesting (or silly) idea following these two observations might be to introduce some form of regularization in the counterfactual search. For example, we could use dropout to randomly switch features on and off in each iteration. Without dwelling further on the usefulness of this idea, let us see how it can be implemented.","category":"page"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"The first code chunk below implements two important steps: 1) create an abstract subtype of the AbstractGradientBasedGenerator and 2) create a constructor similar to the GenericConstructor, but with one additional field for the probability of dropout.","category":"page"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"# Abstract suptype:\nabstract type AbstractDropoutGenerator <: AbstractGradientBasedGenerator end\n\n# Constructor:\nstruct DropoutGenerator <: AbstractDropoutGenerator\n    loss::Symbol # loss function\n    complexity::Function # complexity function\n    mutability::Union{Nothing,Vector{Symbol}} # mutibility constraints \n    λ::AbstractFloat # strength of penalty\n    ϵ::AbstractFloat # step size\n    τ::AbstractFloat # tolerance for convergence\n    p_dropout::AbstractFloat # dropout rate\nend\n\n# Instantiate:\nusing LinearAlgebra\ngenerator = DropoutGenerator(\n    :logitbinarycrossentropy,\n    norm,\n    nothing,\n    0.1,\n    0.1,\n    1e-5,\n    0.5\n)","category":"page"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"Next, we define how feature perturbations are generated for our dropout generator: in particular, we extend the relevant function through a method that implemented the dropout logic.","category":"page"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"import CounterfactualExplanations.Generators: generate_perturbations, ∇\nusing StatsBase\nfunction generate_perturbations(generator::AbstractDropoutGenerator, counterfactual_state::CounterfactualState)\n    𝐠ₜ = ∇(generator, counterfactual_state) # gradient\n    # Dropout:\n    set_to_zero = sample(1:length(𝐠ₜ),Int(round(generator.p_dropout*length(𝐠ₜ))),replace=false)\n    𝐠ₜ[set_to_zero] .= 0\n    Δx′ = - (generator.ϵ .* 𝐠ₜ) # gradient step\n    return Δx′\nend","category":"page"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"Finally, we proceed to generate counterfactuals in the same way we always do. The code below simply generates some toy data, randomly selects a sample and runs the counterfactual search. The resulting counterfactual path is shown in Figure 1.","category":"page"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"# Data:\nusing CounterfactualExplanations.Data\nRandom.seed!(1234)\nN = 25\nw = [1.0 1.0]# true coefficients\nb = 0\nxs, ys = Data.toy_data_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')\n\n# Model:\nusing CounterfactualExplanations.Models: LogisticModel, probs \n# Logit model:\nM = LogisticModel(w, [b])\n# Randomly selected factual:\nRandom.seed!(123)\nx = select_factual(counterfactual_data,rand(1:size(X)[2]))\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)","category":"page"},{"location":"tutorials/generators/","page":"Custom generators","title":"Custom generators","text":"(Image: Figure 1: Counterfactual path for a generic generator with dropout.)","category":"page"},{"location":"contributing/","page":"Overview","title":"Overview","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"contributing/#Contributing","page":"Overview","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Overview","title":"Overview","text":"Our goal is to provide a go-to place for counterfactual explanations in Julia. To this end, the following is a non-exhaustive list of exciting feature developments we envision:","category":"page"},{"location":"contributing/","page":"Overview","title":"Overview","text":"Additional counterfactual generators and predictive models.\nAdditional datasets for testing, evaluation and benchmarking.\nImproved preprocessing including native support for categorical features.\nSupport for regression models.","category":"page"},{"location":"contributing/","page":"Overview","title":"Overview","text":"We are also interested in suggestions on how to adjust and improve the inner workings of our package. To facilitate this process the following page explain and justify the package architecture and the design choices we have made. ","category":"page"},{"location":"contributing/#How-to-contribute?","page":"Overview","title":"How to contribute?","text":"","category":"section"},{"location":"contributing/","page":"Overview","title":"Overview","text":"All of the following contributions are welcome:","category":"page"},{"location":"contributing/","page":"Overview","title":"Overview","text":"Should you spot any errors or something is not working, please just open an issue.\nIf you want to contribute your own code, please proceed as follows:\nFork this repo and clone your fork: git clone https://github.com/your_username/CounterfactualExplanations.jl.\nAdd a remote corresponding to this repository: git remote add upstream https://github.com/pat-alt/CounterfactualExplanations.jl.git\nImplement your modifications and submit a pull request.\nFor any other questions or comments you can also start a discussion.","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"generators/gradient_based/dice/#Diversity","page":"Diverse Counterfactuals","title":"Diversity","text":"","category":"section"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"Counterfactual Explanations are not unique and there are therefore many different ways through which valid counterfactuals can be generated. In the context of Algorithmic Recourse this can be leveraged to offer individuals not one, but possibly many different ways to change a negative outcome into a positive one. One might argue that it makes sense for those different options to be as diverse as possible. This idea is at the core of DiCE, a counterfactual generator introduce by Mothilal, Sharma, and Tan (2020) that generate a diverse set of counterfactual explanations.","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"(Image: )","category":"page"},{"location":"generators/gradient_based/dice/#Adding-a-diversity-constraint","page":"Diverse Counterfactuals","title":"Adding a diversity constraint","text":"","category":"section"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"To ensure that the generates counterfactuals are diverse Mothilal, Sharma, and Tan (2020) add a diversity constraint to the counterfactual search objective. In particular, they define diversity in terms of …","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"We can implement this metric in Julia as follows:[1]","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"using LinearAlgebra\nfunction ddp_diversity(X::AbstractArray{<:Real, 3})\n    xs = eachslice(X, dims = ndims(X))\n    K = [1/(1 + norm(x .- y)) for x in xs, y in xs]\n    return det(K)\nend","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"Below we generate some random points in ℝ² and apply gradient ascent on this function evaluated at the whole array of points. As we can see in Figure 1, the points are sent away from each other. In other words, diversity across the array of points increases as we ascend the ddp_diversity function.","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"theme(:dark)\nlims = 5\nN = 5\nX = rand(2,1,N)\nT = 50\nη = 0.1\nanim = @animate for t in 1:T\n    X .+= gradient(ddp_diversity, X)[1]\n    Z = reshape(X,2,N)\n    scatter(\n        Z[1,:],Z[2,:],ms=25, \n        xlims=(-lims,lims),ylims=(-lims,lims),\n        label=\"\",colour=1:N,\n        size=(500,500),\n        title=\"Diverse Counterfactuals\"\n    )\nend\ngif(anim, joinpath(www_path, \"dice_intro.gif\"))","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"(Image: Figure 1: Diversifying an array of points.)","category":"page"},{"location":"generators/gradient_based/dice/#DiCE","page":"Diverse Counterfactuals","title":"DiCE","text":"","category":"section"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"Now let’s see how this concept carries over to our DiCEGenerator. Below we first load a simple synthetic data set and instantiate a linear classifier that perfectly separate the two classes.","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"# Some random data:\nRandom.seed!(1234)\nN = 100\nw = [1.0 1.0]# true coefficients\nb = 0\nxs, ys = toy_data_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')\nM = LogisticModel(w, [b])\n# Randomly selected factual:\nRandom.seed!(123)\nx = select_factual(counterfactual_data,rand(1:size(X)[2]))\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"We then use DiCE to generate multiple counterfactuals for varying degrees of λ₂, that is the hyperparameter that governs the strength of the penalty for non-diverse outcomes.","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"Λ₂ = [0.1, 1, 5]\ncounterfactuals = []\nn_cf = 5\nfor λ₂ ∈ Λ₂  \n    λ = [0.1, λ₂]\n    generator = DiCEGenerator(λ=λ; ϵ=1)\n    counterfactuals = vcat(\n      counterfactuals...,\n      generate_counterfactual(x, target, counterfactual_data, M, generator; num_counterfactuals=n_cf)\n    )\nend","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"Figure 2 shows the resulting counterfactual paths. As expected, the resulting counterfactuals are more dispersed across the feature domain for higher choices of λ₂","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"using CounterfactualExplanations.Counterfactuals: animate_path\ntheme(:wong)\nT = 100\nlim_ = 9\nplts = []\nfor i ∈ 1:length(Λ₂)\n    λ₂ = Λ₂[i]\n    counterfactual = counterfactuals[i]  \n    plt = plot(counterfactual, xlims=(-lim_,lim_), ylims=(-lim_,lim_), plot_up_to=T, title=\"λ₂=$(λ₂)\")\n    plts = vcat(plts..., plt)\nend\nplt = plot(plts..., size=(1200,300), layout=(1,3))\nsavefig(plt, joinpath(www_path,\"dice.png\"))","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"(Image: Figure 2: Generating diverse counterfactuals through DiCE. The penalty for non-diverse outcomes ($\\_2) increase from left to right.)","category":"page"},{"location":"generators/gradient_based/dice/#References","page":"Diverse Counterfactuals","title":"References","text":"","category":"section"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"Mothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. “Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 607–17.","category":"page"},{"location":"generators/gradient_based/dice/","page":"Diverse Counterfactuals","title":"Diverse Counterfactuals","text":"[1] With thanks to the respondents on Discourse","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"tutorials/binary/#Binary-classification","page":"Binary target","title":"Binary classification","text":"","category":"section"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"To understand the core functionality of CounterfactualExplanations.jl we will look at two example use cases of the generate_counterfactual function. This function takes a structure of type AbstractGenerator as its main argument. You can utilize one of the default generators: GenericGenerator <: AbstractGenerator, GreedyGenerator <: AbstractGenerator. Alternatively, you can also create your own custom generators as we will see in own of the following sections.","category":"page"},{"location":"tutorials/binary/#Default-generators","page":"Binary target","title":"Default generators","text":"","category":"section"},{"location":"tutorials/binary/#GenericGenerator","page":"Binary target","title":"GenericGenerator","text":"","category":"section"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"Let t ∈ {0, 1} denote the target label, M the model (classifier) and x′ ∈ ℝᴰ the vector of counterfactual features. In order to generate recourse the GenericGenerator optimizes the following objective function through steepest descent","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"xprime = arg min_xprime  ell(M(xprime)t) + lambda h(xprime)","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"where ℓ denotes some loss function targeting the deviation between the target label and the predicted label and h(⋅) is a complexity penalty generally addressing the realism or cost of the proposed counterfactual.","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"Let’s generate some toy data:","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"# Some random data:\nusing CounterfactualExplanations.Data\nRandom.seed!(1234)\nN = 25\nw = [1.0 1.0]# true coefficients\nb = 0\nxs, ys = Data.toy_data_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')\nplt = plot()\nplt = plot_data!(plt,X',ys);\nsavefig(plt, joinpath(www_path, \"binary_samples.png\"))","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"(Image: )","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"For this toy data we will now generate counterfactual explanations as follows:","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"Use the coefficients w and b (assumed to be known or estimated) to define our model using CounterfactualExplanations.Models.LogisticModel(w, b). (The first figure below shows the posterior predictive surface for this plugin estimator.)\nDefine our GenericGenerator.\nGenerate counterfactual.","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"# Logit model:\nusing CounterfactualExplanations.Models: LogisticModel, probs \nM = LogisticModel(w, [b])\n# Randomly selected factual:\nRandom.seed!(123)\nx = select_factual(counterfactual_data,rand(1:size(X)[2]))\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n\n# Define generator:\ngenerator = GenericGenerator()\n\n# Generate explanations:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"(Image: )","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"The animation below shows the resulting counterfactual path in the feature space (left) and the predicted probability (right). We can observe that the sample crosses the decision boundary and reaches the threshold target class probability γ.","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"(Image: )","category":"page"},{"location":"tutorials/binary/#GreedyGenerator","page":"Binary target","title":"GreedyGenerator","text":"","category":"section"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"Next we will repeat the exercise above, but instead use the GreedyGenerator in the context of a Bayesian classifier. This generator is greedy in the sense that it simply chooses the most salient feature {x′}ᵈ where","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"d=argmax_d in 1D nabla_xprime ell(M(xprime)t)","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"and perturbs it by a fixed amount δ. In other words, optimization is penalty-free. This is possible in the Bayesian context, because maximizing the predictive probability corresponds to minimizing the predictive uncertainty: by construction the generated counterfactual will therefore be realistic (low epistemic uncertainty) and unambiguous (low aleotoric uncertainty).","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"using LinearAlgebra\nΣ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix\nμ = hcat(b, w)\nM = CounterfactualExplanations.Models.BayesianLogisticModel(μ, Σ)\ngenerator = GreedyGenerator(;δ=0.1,n=25))\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"Once again we plot the resulting counterfactual path (left) and changes in the predicted probability (right). For the Bayesian classifier predicted probabilities fan out: uncertainty increases in regions with few samples. Note how the greedy approach selects the same most salient feature over and over again until its exhausted.","category":"page"},{"location":"tutorials/binary/","page":"Binary target","title":"Binary target","text":"(Image: )","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"tutorials/mutability/#Mutability-and-domain-constraints","page":"Mutability constraints","title":"Mutability and domain constraints","text":"","category":"section"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"In practice, features usually cannot be perturbed arbitrarily. Suppose, for example, that one of the features used by a bank to predict the credit worthiness of its clients is gender. If a counterfactual explanation for the prediction model indicates that female clients should change their gender to improve their credit worthiness, then this is an interesting insight (it reveals gender bias), but it is not usually an actionable transformation in practice. In such cases we may want to constrain the mutability of features to ensure actionable and realistic recourse. To illustrate how this can be implemented in CounterfactualExplanations.jl we will look at the linearly separable toy dataset again.","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"# Data:\nusing CounterfactualExplanations.Data\nxs, ys = Data.toy_data_linear()\nX = hcat(xs...)\n\n# Model\nw = [1.0 1.0]# true coefficients\nb = 0\n# Logit model:\nM = LogisticModel(w, [b])","category":"page"},{"location":"tutorials/mutability/#Mutability","page":"Mutability constraints","title":"Mutability","text":"","category":"section"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"Mutability of features can be defined in terms of four different options: 1) the feature is mutable in both directions, 2) the feature can only increase (e.g. age), 3) the feature can only decrease (e.g. time left until your next deadline) and 4) the feature is not mutable (e.g. skin colour, ethnicity, …). To specify which category a feature belongs to, you can pass a vector of symbols containing the mutability constraints at the pre-processing stage. For each feature you can choose from these four options: :both (mutable in both directions), :increase (only up), :decrease (only down) and :none (immutable). By default, nothing is passed to that keyword argument and it is assumed that all features are mutable in both directions.","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"Below we impose that the second feature is immutable. The resulting counterfactual path is shown in Figure 1 below. Since only the first feature can be perturbed, the sample can only move along the horizontal axis.","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"counterfactual_data = CounterfactualData(X,ys';mutability=[:both, :none])","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"(Image: Figure 1: Counterfactual path with immutable feature.)","category":"page"},{"location":"tutorials/mutability/#Domain-constraints","page":"Mutability constraints","title":"Domain constraints","text":"","category":"section"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"In some cases we may also want to constrain the domain of some feature. For example, age as a feature is constrained to a range from 0 to some upper bound corresponding perhaps to the average life expectancy of humans. Below, for example, we impose an upper bound of 0.5 for our two features. This results in the counterfactual path shown in Figure 2: since features are not allowed to be perturbed beyond the upper bound, the resulting counterfactual falls just short of the threshold probability γ.","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"counterfactual_data = CounterfactualData(X,ys';domain=(-Inf,0.5))","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"(Image: Figure 2: Counterfactual path with domain constraints.)","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"If we only want to constrain one particular feature to a certain domain we can pass a vector of tuples as below. In this case the resulting counterfactual path shown in Figure 3 does converge to the desired threshold γ.","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"counterfactual_data = CounterfactualData(X,ys';domain=[(-Inf,Inf),(-Inf,-0.5)])","category":"page"},{"location":"tutorials/mutability/","page":"Mutability constraints","title":"Mutability constraints","text":"(Image: Figure 3: Counterfactual path with only one feature constrained to a certain domain.)","category":"page"},{"location":"tutorials/","page":"Overview","title":"Overview","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"tutorials/#Using-CounterfactualExplanations.jl","page":"Overview","title":"Using CounterfactualExplanations.jl","text":"","category":"section"},{"location":"tutorials/","page":"Overview","title":"Overview","text":"The following pages introduce the main package functionality through various illustrative examples involving synthetic data. Among other things, the tutorials cover binary classification, multi-class problem and language interoperability with Python and R. For anyone primarily interested in using and understanding the existing package functionality, these tutorials should provide sufficient detail. If you are interested in contributing your own custom generators and models or have ideas for how to improve this package, we encourage you to also take a look at the Contributor's Guide. ","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"contributing/interop/#Interoperability","page":"Interoperability","title":"Interoperability","text":"","category":"section"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"!!! info \"Contributor's Guide\" \n    Our work on language interoperability is still experimental. In this tutorial we demonstrate our early attempts to extend the package to accommodate `torch` models trained in R and Python. The goal here is to provide a template and/or starting point for contributors that would like to add support for other models trained in foreign programming languages. If that contributor could be you, please see the related [issue](https://github.com/pat-alt/CounterfactualExplanations.jl/issues/67) and work in the linked [branch](https://github.com/pat-alt/CounterfactualExplanations.jl/tree/67-interoperability-broken-after-moving-to-julia-18).\n\n!!! info \"Native support currently broken and removed\" \n    After bumping Julia version compatibility to `1.8`, interoperability broke for some reason (it was admittedly always experimental and hacky). The code that was previously used for interoperability was removed from the main branch, but still lives [here](https://github.com/pat-alt/CounterfactualExplanations.jl/tree/67-interoperability-broken-after-moving-to-julia-18).","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"The Julia language offers unique support for programming language interoperability. For example, calling Python and R is made remarkably easy through PyCall.jl (potentially PythonCall.jl is a better option) and RCall.jl. In this tutorial we will see how CounterfactualExplanations.jl can leverage this functionality.","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"To get started we will first load some two-dimensional toy data:","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"using Random\n# Some random data:\nRandom.seed!(1234);\nN = 100\nusing CounterfactualExplanations\nusing CounterfactualExplanations.Data\nxs, ys = Data.toy_data_non_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')","category":"page"},{"location":"contributing/interop/#torch-model-trained-in-R","page":"Interoperability","title":"torch model trained in R","text":"","category":"section"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"The code below builds a simple MLP in R:","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"using RCall\nR\"\"\"\n# Data\nlibrary(torch)\nX <- torch_tensor(t($X))\nys <- torch_tensor($ys)\n\n# Model:\nmlp <- nn_module(\n  initialize = function() {\n    self$layer1 <- nn_linear(2, 32)\n    self$layer2 <- nn_linear(32, 1)\n  },\n  forward = function(input) {\n    input <- self$layer1(input)\n    input <- nnf_sigmoid(input)\n    input <- self$layer2(input)\n    input\n  }\n)\nmodel <- mlp()\noptimizer <- optim_adam(model$parameters, lr = 0.1)\nloss_fun <- nnf_binary_cross_entropy_with_logits\n\"\"\"","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"The following code trains the MLP for the binary prediction task at hand:","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"R\"\"\"\nfor (epoch in 1:100) {\n\n  model$train()  \n\n  # Compute prediction and loss:\n  output <- model(X)[,1]\n  loss <- loss_fun(output, ys)\n\n  # Backpropagation:\n  optimizer$zero_grad()\n  loss$backward()\n  optimizer$step()\n  \n  cat(sprintf(\"Loss at epoch %d: %7f\\n\", epoch, loss$item()))\n}\n\"\"\"","category":"page"},{"location":"contributing/interop/#Making-the-model-compatible","page":"Interoperability","title":"Making the model compatible","text":"","category":"section"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"As always we need to extend the logits and probs functions to make the model compatible with CounterfactualExplanations.jl. As evident from the code below, this is actually quite straight-forward: the logits are returned by the torch model and copied form R into the Julia environment. Probabilities are then computed in Julia, by passing the logits through the sigmoid function.","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"using Flux\nusing CounterfactualExplanations, CounterfactualExplanations.Models\nimport CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n\n# Step 1)\nstruct MyRTorchModel <: Models.AbstractDifferentiableModel\n    model::Any\nend\n\n# Step 2)\nfunction logits(M::MyRTorchModel, X::AbstractArray)\n  nn = M.model\n  ŷ = rcopy(R\"as_array($nn(torch_tensor(t($X))))\")\n  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]\n  return ŷ'\nend\nprobs(M::MyRTorchModel, X::AbstractArray)= σ.(logits(M, X))\nM = MyRTorchModel(R\"model\")","category":"page"},{"location":"contributing/interop/#Adapting-the-generator","page":"Interoperability","title":"Adapting the generator","text":"","category":"section"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"Next we need to do a tiny bit of work on the AbstractGenerator side. By default methods underlying the GenericGenerator are desiged to work with models that have gradient access through Zygote.jl, one of Julia’s main autodifferentiation packages. Of course, Zygote.jl cannot access the gradients of our torch model, so we need to adapt the code slightly. Fortunately, it turns out that all we need to do is extend the function that computes the gradient with respect to the loss function for the generic counterfactual search: ∂ℓ(generator::GenericGenerator, x′, M, t). In particular, we will extend the function by a method that is specific to the MyRTorchModel type we defined above. The code below implements this: our new method ∂ℓ calls R in order to use torch’s autodifferentiation functionality for computing the gradient.","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"import CounterfactualExplanations.Generators: ∂ℓ\nusing LinearAlgebra\n\n# Counterfactual loss:\nfunction ∂ℓ(generator::AbstractGradientBasedGenerator, M::MyRTorchModel, counterfactual_state::CounterfactualState) \n  nn = M.model\n  x_cf = counterfactual_state.x′\n  t = counterfactual_state.target_encoded\n  R\"\"\"\n  x <- torch_tensor($x_cf, requires_grad=TRUE)\n  output <- $nn(x)\n  obj_loss <- nnf_binary_cross_entropy_with_logits(output,$t)\n  obj_loss$backward()\n  \"\"\"\n  grad = rcopy(R\"as_array(x$grad)\")\n  return grad\nend","category":"page"},{"location":"contributing/interop/#Generating-counterfactuals","page":"Interoperability","title":"Generating counterfactuals","text":"","category":"section"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"From here on onwards we use the CounterfactualExplanations.jl functionality as always. Below we choose a random sample, define our generic generator and finally run the search:","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"# Randomly selected factual:\nRandom.seed!(123)\nx = select_factual(counterfactual_data, rand(1:length(xs))) \ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"(Image: )","category":"page"},{"location":"contributing/interop/#torch-model-in-Python","page":"Interoperability","title":"torch model in Python","text":"","category":"section"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"!!! warning \"'PyTorch' and 'torch for R' interplay\" \n    We have noted that using both ['PyTorch'](https://pytorch.org/) through `PyCall.jl` and ['torch for R'](https://torch.mlverse.org/packages) through `RCall.jl` in the same Julia session causes issues. In particular, loading 'PyTorch' after loading 'torch for R' cause the Julia session to crash and vice versa. For the time being, we therefore advise not to use both `RTorchModel()` and `PyTorchModel` in the same session.","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"The steps involved are largely analogous to the above, so we leave the following code uncommented.","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"using PyCall\npy\"\"\"\n# Data\nimport torch\nfrom torch import nn\nX = torch.Tensor($X).T\nys = torch.Tensor($ys)\n\nclass MLP(nn.Module):\n  def __init__(self):\n    super(MLP, self).__init__()\n    self.model = nn.Sequential(\n      nn.Flatten(),\n      nn.Linear(2, 32),\n      nn.Sigmoid(),\n      nn.Linear(32, 1)\n    )\n\n  def forward(self, x):\n    logits = self.model(x)\n    return logits\n\nmodel = MLP()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_fun = nn.BCEWithLogitsLoss()\n\"\"\"","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"py\"\"\"\nfor epoch in range(100):\n  # Compute prediction and loss:\n  output = model(X).squeeze()\n  loss = loss_fun(output, ys)\n  \n  # Backpropagation:\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n  print(f\"Loss at epoch {epoch+1}: {loss.item():>7f}\")\n\"\"\"","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"using Flux\nusing CounterfactualExplanations, CounterfactualExplanations.Models\nimport CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n\n# Step 1)\nstruct MyPyTorchModel <: Models.AbstractDifferentiableModel\n    model::Any\nend\n\n# Step 2)\nfunction logits(M::MyPyTorchModel, X::AbstractArray)\n  nn = M.model\n  if !isa(X, Matrix)\n    X = reshape(X, length(X), 1)\n  end\n  ŷ = py\"$nn(torch.Tensor($X).T).detach().numpy()\"\n  ŷ = isa(ŷ, AbstractArray) ? ŷ : [ŷ]\n  return ŷ'\nend\nprobs(M::MyPyTorchModel, X::AbstractArray)= σ.(logits(M, X))\nM = MyPyTorchModel(py\"model\")","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"import CounterfactualExplanations.Generators: ∂ℓ\nusing LinearAlgebra\n\n# Countefactual loss:\nfunction ∂ℓ(generator::AbstractGradientBasedGenerator, M::MyPyTorchModel, counterfactual_state::CounterfactualState) \n  nn = M.model\n  x′ = counterfactual_state.x′\n  t = counterfactual_state.target_encoded\n  x = reshape(x′, 1, length(x′))\n  py\"\"\"\n  x = torch.Tensor($x)\n  x.requires_grad = True\n  t = torch.Tensor($[t]).squeeze()\n  output = $nn(x).squeeze()\n  obj_loss = nn.BCEWithLogitsLoss()(output,t)\n  obj_loss.backward()\n  \"\"\"\n  grad = vec(py\"x.grad.detach().numpy()\")\n  return grad\nend","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"# Randomly selected factual:\nRandom.seed!(123)\nx = select_factual(counterfactual_data, rand(1:length(xs))) \ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)","category":"page"},{"location":"contributing/interop/","page":"Interoperability","title":"Interoperability","text":"(Image: )","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"contributing/loss/#Loss-functions","page":"Loss functions","title":"Loss functions","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"For the computation of loss functions and their gradients we leverage the functionality already implmented in Flux. All of the loss functions from Flux have been imported:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"julia> names(CounterfactualExplanations.Losses)\n19-element Vector{Symbol}:\n :Losses\n :binary_focal_loss\n :binarycrossentropy\n :crossentropy\n :ctc_loss\n :dice_coeff_loss\n :focal_loss\n :hinge_loss\n ⋮\n :logitbinarycrossentropy\n :logitcrossentropy\n :mae\n :mse\n :msle\n :poisson_loss\n :squared_hinge_loss\n :tversky_loss","category":"page"},{"location":"contributing/loss/#Classification","page":"Loss functions","title":"Classification","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"For most classification tasks the default :logitbinarycrossentropy (binary) and :logitcrossentropy should be sufficient. For both choices the package has been tested and works natively. When using other loss functions, some caution is recommended though:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"!!! warning \"External loss functions\"\n    Some margin-based loss functions like hinge loss do not expect inputs in the domain $\\mathcal{Y}=\\{0,1\\}$, but rather $\\mathcal{Y}=\\{-1,1\\}$. In those case one needs to ensure that the training labels $y$ are encoded accordingly. In order to use distance-based loss functions like mean squared error (MSE) loss needs to be computed with respect to probibilities rather than logits. This is currently not supported and we genenerally recommend not to use distance-based loss functions in the classification setting (more on this below).","category":"page"},{"location":"contributing/loss/#Regression","page":"Loss functions","title":"Regression","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"At this point CounterfactualExplanations.jl is designed to be used with classification models, since the overwhelming majority of the existing literature on counterfactual explanations is set in this context. By default margin-based loss functions are used and computed with respect to logits (more on this below). To produce counterfactual explanations for regression problems users currently need to binarize the problem: let t denote some target value for the continuous dependent variable y in the regression context, then we could respecify the dependent variable as t̃ = 0 for all y \\< t and t̃ = 1 otherwise. In future work we want to add full support for regression problems.","category":"page"},{"location":"contributing/loss/#Methodological-background","page":"Loss functions","title":"Methodological background","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"This is a short tutorial on loss functions and gradients typically involved in counterfactual search. It involves more maths than perhaps some of the other tutorials.","category":"page"},{"location":"contributing/loss/#General-setup","page":"Loss functions","title":"General setup","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"We begin by restating the general setup for generic counterfactual search. Let t ∈ {0, 1} denote the target label, M the model (classifier) and x′ ∈ ℝ^(D) the vector of counterfactual features (we will assume all features are continuous). Then the differentiable optimization problem in algorithmic recourse is generally of the following form","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"xprime = arg min_xprime  ell(M(xprime)t) + lambda h(xprime)","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"where ℓ denotes some loss function targeting the deviation between the target label and the predicted label and h(⋅) acts as a complexity penality generally addressing the realism or cost of the proposed counterfactual.","category":"page"},{"location":"contributing/loss/#Loss-function-ℓ","page":"Loss functions","title":"Loss function ℓ","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"Different choices for ℓ come to mind, each potentially leading to very different counterfactual outcomes. In practice, ℓ is often implemented with respect to the logits a = w^(T)x rather than the probabilities p(y′=1|x′) = σ(a) predicted by the classifier. We follow this convention here, but as we shall see depeding on the label domain this convention does not work well for every type of loss function. Common choices for ℓ in the literature include margin-based loss function like hinge loss and logit binary crossentropy (or log) loss. Some use distance-based loss such as mean squared error loss (MSE).","category":"page"},{"location":"contributing/loss/#Hinge-loss","page":"Loss functions","title":"Hinge loss","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"With respect to the logits a = w′x hinge loss can be defined as follows","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"ell(at^*)=(1-acdot t^*)_+=max01-acdot t^*","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"where t^(*) is the target label in { − 1, 1}. Since above we defined t ∈ {0, 1} we need a mapping h : {0, 1} ↦ { − 1, 1}. Specifically, we want to plug in h(t) = t^(*) where h(⋅) is just the following conditional:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"beginaligned\nh(t)=begincases\n-1  textif  t=0  1  textif  t=1\nendcases\nendaligned","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"Then our loss function as function of t can restated as follows:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"ell(at^*)=ell(at)=(1-acdot h(t))_+=max01-acdot h(t)","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"The first-order derivative of hinge loss with respect to the logits a is simply","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"beginaligned\nell(at)=begincases\n-h(t)  textif  a cdot h(t)=1  0  textotherwise \nendcases\nendaligned","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"In the context of counterfactual search the gradient with respect to the feature vector is then:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"beginaligned\n nabla_xprime ell(at)= begincases\n-h(t)mathbfw  textif  h(t)mathbfw^Txprime=1  0  textotherwise \nendcases\nendaligned","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"In practice gradients are commonly computed through autodifferentiation. In this tutorial we use the Zygote.jl package which is at the core of Flux.jl, the main deep learning library for Julia.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"The side-by-side plot below visualises the loss function and its derivative. The plot further below serves as a simple sanity check to verify that autodifferentiation indeed yields the same result as the closed-form solution for the gradient.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"h(t) = ifelse(t==1,1,-1)\nhinge(a,t) = max(0,1-a*h(t))","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"default(size=(500,500))\na = -2:0.05:2\np1 = plot(a, [hinge(a,1) for a=a], title=\"Loss, t=1\", xlab=\"logits\")\np2 = plot(a, [gradient(hinge,a,1)[1] for a=a], title=\"Gradient, t=1\", xlab=\"logits\")\np3 = plot(a, [hinge(a,0) for a=a], title=\"Loss, t=0\", xlab=\"logits\")\np4 = plot(a, [gradient(hinge,a,0)[1] for a=a], title=\"Gradient, t=0\", xlab=\"logits\")\nplot(p1, p2, p3, p4, layout = (2, 2), legend = false)\nsavefig(joinpath(www_path, \"loss_grad_hinge.png\"))","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"(Image: )","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"# Just verifying that the formula for the gradient above indeed yields the same result.\nfunction gradient_man(x,w,t)\n    𝐠 = ifelse(h(t)*w'x<=1, -h(t)*w, 0)\n    return 𝐠\nend;\nplot(a, [gradient_man(a,1,1) for a=a], legend=:bottomright, label=\"Manual\", title=\"Gradient\", xlab=\"logits\")\nscatter!(a, [gradient(hinge,a,1)[1] for a=a], label=\"Autodiff\")\nsavefig(joinpath(www_path, \"loss_grad_hinge_test.png\"))","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"(Image: )","category":"page"},{"location":"contributing/loss/#Logit-binary-crossentropy-loss","page":"Loss functions","title":"Logit binary crossentropy loss","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"Logit binary crossentropy loss loss (sometimes referred to as log loss) is defined as follows:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"beginaligned\n ell(at)=- left( t cdot log(sigma(a)) + (1-t) cdot log (1-sigma(a)) right) \nendaligned","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"where σ(a) is the logit/sigmoid link function.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"Once again for the purpose of counter factual search we are interested in the first-order derivative with respect to our feature vector x′. You can verify that the partial derivative with respect to feature x′_(d) is as follows:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"beginaligned\n fracpartial ell(at)partial xprime_d= (sigma(a) - t) w_d \nendaligned","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"The gradient just corresponds to the stacked vector of partial derivatives:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"beginaligned\n nabla_xprime ell(at)= (sigma(a) - t) mathbfw \nendaligned","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"As before implementation below is done through autodifferentiation. As before the side-by-side plot shows the resulting loss function and its gradient and the plot further below is a simple sanity check.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"# sigmoid function:\nfunction 𝛔(a)\n    trunc = 8.0 # truncation to avoid numerical over/underflow\n    a = clamp.(a,-trunc,trunc)\n    p = exp.(a)\n    p = p ./ (1 .+ p)\n    return p\nend\n\n# Logit binary crossentropy:\nlogitbinarycrossentropy(a, t) = - (t * log(𝛔(a)) + (1-t) * log(1-𝛔(a)))","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"(Image: )","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"p1 = plot(a, [logitbinarycrossentropy(a,1) for a=a], title=\"Loss, t=1\", xlab=\"logits\")\np2 = plot(a, [gradient(logitbinarycrossentropy,a,1)[1] for a=a], title=\"Gradient, t=1\", xlab=\"logits\")\np3 = plot(a, [logitbinarycrossentropy(a,0) for a=a], title=\"Loss, t=0\", xlab=\"logits\")\np4 = plot(a, [gradient(logitbinarycrossentropy,a,0)[1] for a=a], title=\"Gradient, t=0\", xlab=\"logits\")\nplot(p1, p2, p3, p4, layout = (2, 2), legend = false)\nsavefig(joinpath(www_path, \"loss_grad_log.png\"))","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"(Image: )","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"# Just verifying that the formula for the gradient above indeed yields the same result.\nfunction gradient_man(x,w,y)\n    𝐠 = (𝛔(w'x) - y) .* w\n    return 𝐠\nend;\nplot(a, [gradient_man(a,1,1) for a=a], legend=:bottomright, label=\"Manual\", title=\"Gradient\", xlab=\"logits\")\nscatter!(a, [gradient(logitbinarycrossentropy,a,1)[1] for a=a], label=\"Autodiff\")\nsavefig(joinpath(www_path, \"loss_grad_log_test.png\"))","category":"page"},{"location":"contributing/loss/#Mean-squared-error","page":"Loss functions","title":"Mean squared error","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"Some authors work with distance-based loss functions instead. Since in general we are interested in providing valid recourse, that is counterfactual explanations that indeed lead to the desired label switch, using one of the margin-based loss functions introduced above seems like a more natural choice. Nonetheless, we shall briefly introduce one of the common distance-based loss functions as well.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"The mean squared error for counterfactual search implemented with respect to the logits is simply the squared ℓ² norm between the target label and a = w^(T)x:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"beginaligned\n ell(at)= t-a^2\nendaligned","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"The gradient with respect to the vector of features is then:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"beginaligned\n nabla_xprime ell(at)= 2(a - t) mathbfw \nendaligned","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"As before implementation and visualizations follow below.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"mse(a,t) = norm(t - a)^2","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"NOTE: I hinted above that the convention of taking derivatives with respect to logits can go wrong depending on the loss function we choose. The plot below demonstrates this point: for t = 0 the global minimum of the MSE is of course also at 0. The implication for counterfactual search is that for t = 0 the search stops when w^(T)x′ = 0. But at this point σ(w^(T)x′) = 0.5, in other words we stop right at the decision boundary, but never cross it. We will see an example of this below. Key takeaway: carefully think about the choice of your loss function and DON’T use distance-based loss functions when optimizing with respect to logits.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"p1 = plot(a, [mse(a,1) for a=a], title=\"Loss, t=1\", xlab=\"logits\")\np2 = plot(a, [gradient(mse,a,1)[1] for a=a], title=\"Gradient, t=1\", xlab=\"logits\")\np3 = plot(a, [mse(a,0) for a=a], title=\"Loss, t=0\", xlab=\"logits\")\np4 = plot(a, [gradient(mse,a,0)[1] for a=a], title=\"Gradient, t=0\", xlab=\"logits\")\nplot(p1, p2, p3, p4, layout = (2, 2), legend = false)\nsavefig(joinpath(www_path, \"loss_grad_mse.png\"))","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"(Image: )","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"# Just verifying that the formula for the gradient above indeed yields the same result.\nfunction gradient_man(x,w,y)\n    𝐠 = 2*(w'x - y) .* w\n    return 𝐠\nend;\nplot(a, [gradient_man(a,1,1) for a=a], legend=:bottomright, label=\"Manual\", title=\"Gradient\", xlab=\"logits\")\nscatter!(a, [gradient(mse,a,1)[1] for a=a], label=\"Autodiff\")\nsavefig(joinpath(www_path, \"loss_grad_mse_test.png\"))","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"(Image: )","category":"page"},{"location":"contributing/loss/#Example-in-2D","page":"Loss functions","title":"Example in 2D","text":"","category":"section"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"To understand the properties of the different loss functions we will now look at a toy example in 2D. The code below generates some random features and assigns labels based on a fixed vector of coefficients using the sigmoid function.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"# Some random data:\nusing Flux, Random, CounterfactualExplanations.Data\nRandom.seed!(1234)\nN = 25\nw = [1.0 1.0]# true coefficients\nb = 0\nxs, ys = Data.toy_data_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"The plot below shows the samples coloured by label along with the decision boundary. You can think of this as representing the outcome of some automated decision making system. The highlighted sample was chosen to receive algorithmic recourse in the following: we will search for a counterfactual that leads to a label switch.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"using CounterfactualExplanations\nusing CounterfactualExplanations.Models: LogisticModel\nM = LogisticModel(w, [b])\n\nRandom.seed!(1234)\nx = select_factual(counterfactual_data,rand(1:size(X)[2]))\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0)\nγ = 0.75\n\n# Plot with random sample chose for recourse\nplt = plot(M, counterfactual_data)\nscatter!(plt,[x[1]],[x[2]],ms=10,label=\"\", color=Int(y))\nPlots.abline!(plt,-w[2]/w[1],b,color=\"black\",label=\"\",lw=2)\nsavefig(joinpath(www_path, \"loss_example.png\"))","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"(Image: )","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"Now we instantiate different generators for our different loss functions and different choices of λ. Finally we generate recourse for each of them:","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"# Generating recourse\nΛ = [0.0, 1.0, 5.0] # varying complexity penalties\nlosses = [:logitbinarycrossentropy, :mse]\ncounterfactuals = []\nfor loss in losses\n    for λ in Λ\n        generator = GenericGenerator(;loss=loss,λ=λ,decision_threshold=γ) \n        t = loss == :hinge_loss ? h(target) : target # mapping for hinge loss\n        counterfactual = generate_counterfactual(x, t, counterfactual_data, M, generator; T=50)\n        counterfactuals = vcat(counterfactuals, counterfactual)\n    end\nend","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"The code below plots the resulting counterfactual paths.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"Complexity penalty (λ): has the expected effect of penalizing long counterfactual paths: as the distance between x and x′ the penalty exerts more and more pressure on the gradient in the opposite direction ∇ℓ. For large choices of λ valid recourse is not attainable.\nConfidence threshold (γ): note how for both log loss and hinge loss we overshoot a bit, that is we end up well beyond the decision boundary. This is because above we chose a confidence threshold of γ = 0.75. In the context of recourse this choice matters a lot: we have a longer distance to travel (=higher costs for the individual), but we can be more confident that recourse will remain valid. There is of course an interplay between λ and γ.\nThe choice of the loss function matters: the distance-based MSE does NOT work without further ajustments when optimizing with respect to logits, as discussed above.","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"# Plotting\nk = length(counterfactuals)\nfunction plot_recourse(counterfactual, t)\n    l = string(counterfactual.generator.loss)\n    l = l[1:minimum([5,length(l)])]\n    λ = string(counterfactual.generator.λ)\n    plt = plot_contour(X',ys,M;colorbar=false,title=\"Loss: $(l), λ: $(λ)\")\n    plt = plot(plt, size=(floor(√(k)) * 350, ceil(√(k)) * 350))\n    Plots.abline!(plt,-w[2]/w[1],b,color=\"black\",label=\"\",lw=2)\n    t = minimum([t, total_steps(counterfactual)])\n    scatter!(plt, hcat(path(counterfactual)[1:t]...)[1,:], hcat(path(counterfactual)[1:t]...)[2,:], ms=10, color=Int(y), label=\"\")\n    return plt\nend\nmax_path_length = maximum(map(counterfactual -> total_steps(counterfactual), counterfactuals))\nanim = @animate for i in 1:max_path_length\n    plots = map(counterfactual -> plot_recourse(counterfactual, i), counterfactuals)\n    plot(plots..., layout = (Int(floor(√(k))), Int(ceil(√(k)))), legend = false, plot_title=\"Iteration: \" * string(i))\nend\ngif(anim, joinpath(www_path, \"loss_paths.gif\"), fps=5)","category":"page"},{"location":"contributing/loss/","page":"Loss functions","title":"Loss functions","text":"(Image: )","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"generators/gradient_based/latent_space_generator/#Latent-Space-Search","page":"Latent Space Search","title":"Latent Space Search","text":"","category":"section"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"The current consensus in the literature is that Counterfactual Explanations should realistic: the generated counterfactuals should look like they were generated by the data generating process (DGP) that governs the problem at hand. With respect to Algorithmic Recourse it is certainly true that counterfactuals should be realistic in order to be actionable for individuals.[1] To address this need, researchers have come up with various different approaches in recent years. Among the most popular approaches is Latent Space Search, which was first proposed in Joshi et al. (2019): instead of traversing the feature space directly, this approach relies on a separate generative model that learns a latent space representation of the DGP. Assuming the generative model is well-specified, access to the learned latent embeddings of the data comes with two advantages:","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"Since the learned DGP is encoded in the latent space, the generated counterfactuals will respect the learned representation of the data. In practice this means that counterfactuals will be realistic.\nThe latent space is typically a compressed (i.e. lower dimensional) version of the feature space. This makes the counterfactual search less costly.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"There are also certain disadvantages though:","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"Learning generative models is (typically) an expensive task, which may well outweigh the benefits associated with utlimately traversing a lower dimensional space.\nIf the generative model is poorly specified, this will affect the quality of the counterfactuals.[2]","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"Anyway, traversing latent embeddings is a powerful idea that may be very useful depending on the specific context. This tutorial introduces the concept and how it is implemented in this package.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/#Synthetic-data","page":"Latent Space Search","title":"Synthetic data","text":"","category":"section"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"We start by looking at synthetic data.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/#D-Example","page":"Latent Space Search","title":"2D Example","text":"","category":"section"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"The first few code snippets below generate the synthetic data, set up a simple neural network in Flux and train it for the binary classification task.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"using CounterfactualExplanations\nusing Random\ninput_dim = 2\nn = 100\nη = 3\nμ₀ = rand(-input_dim:input_dim,input_dim) .* η\nμ₁ = rand(-input_dim:input_dim,input_dim) .* η\nX₀ = μ₀ .+ randn(input_dim, n) \nX₁ = μ₁ .+ randn(input_dim, n) \nX = hcat(X₀, X₁)\nusing MLUtils\nxs = MLUtils.unstack(X,dims=2)\nys = hcat(zeros(1,n), ones(1,n))","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"using Flux\nn_hidden = 50\nnn = Chain(\n    Dense(input_dim, n_hidden, relu),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y)\ndata = zip(xs,ys)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 100\nusing Statistics: mean\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend\n\nM = FluxModel(nn)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"We then draw a random factual and generate two counterfactuals - one using generic search in the feature space and one using generic search in the latent space. The default generative model - a Variational Autoencoder (VAE) - is set up and trained under the hood. The resulting counterfactual paths are shown in the animation below: note how latent space search results in faster convergence to an optimum that sits right within the cluster of samples in the target class. For generic search in the feature space we instead end up just crossing the decision boundary before converging.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"counterfactual_data = CounterfactualData(X,ys)\nusing Random\n# Random.seed!(123)\nx = select_factual(counterfactual_data, rand(1:size(X)[2])) \ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator, latent_space=false)\ncounterfactual_latent = generate_counterfactual(x, target, counterfactual_data, M, generator, latent_space=true)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"(Image: Counterfactuals generated through generic search in feature space (left) and latent space (right).)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/#D-Example-2","page":"Latent Space Search","title":"3D Example","text":"","category":"section"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"This second example is largely analogous to what we just saw above. The figure below demonstrates the idea of searching counterfactuals in a lower dimensional latent space.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"using CounterfactualExplanations\nusing Random\ninput_dim = 3\nn = 100\nη = 3\nμ₀ = rand(-input_dim:input_dim,input_dim) .* η\nμ₁ = rand(-input_dim:input_dim,input_dim) .* η\nX₀ = μ₀ .+ randn(input_dim, n) \nX₁ = μ₁ .+ randn(input_dim, n) \nX = hcat(X₀, X₁)\nusing MLUtils\nxs = MLUtils.unstack(X,dims=2)\nys = hcat(zeros(1,n), ones(1,n))","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"using Flux\nn_hidden = 50\nnn = Chain(\n    Dense(input_dim, n_hidden, relu),\n    Dense(n_hidden, 1)\n)  \nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y)\ndata = zip(xs,ys)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 100\nusing Statistics: mean\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend\n\nM = FluxModel(nn)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"counterfactual_data = CounterfactualData(X,ys)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"using Random\nRandom.seed!(123)\nx = select_factual(counterfactual_data, rand(1:size(X)[2])) \ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator, latent_space=true)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"(Image: )","category":"page"},{"location":"generators/gradient_based/latent_space_generator/#MNIST-data","page":"Latent Space Search","title":"MNIST data","text":"","category":"section"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"Finally, let’s take the ideas introduced above to a more complex example. We first load the MNIST data and a simple pre-trained neural network. The test set accuracy is shown below.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"using Flux\nusing CounterfactualExplanations.Data: mnist_data, mnist_model\nX, ys = mnist_data()\nmodel = mnist_model()\nensemble = mnist_ensemble()\nM = FluxModel(model, likelihood=:classification_multi)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"using MLDatasets\ndata_test = MNIST(:test)\ntest_x, test_y = data_test[:]\nys_test = Flux.onehotbatch(test_y, 0:9)\nX_test = Flux.flatten(test_x)\nbs = Int(round(size(X)[2]/10))\ndata_test = DataLoader((X_test,ys_test),batchsize=bs)\naccuracy(data) = mean(map(d -> mean(Flux.onecold(Flux.softmax(model(d[1])), 0:9) .== Flux.onecold(d[2], 0:9)), data))\naccuracy(data_test)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/#Training-the-VAE","page":"Latent Space Search","title":"Training the VAE","text":"","category":"section"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"Here we show how a custom generative model can be trained …","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"input_dim = size(X)[1]\nusing CounterfactualExplanations.GenerativeModels: VAE, train!\nvae = VAE(input_dim; nll=Flux.logitbinarycrossentropy, epochs=100, λ=0.0001, latent_dim=28)\ntrain!(vae, X, ys)\nusing BSON: @save\n@save joinpath(www_path, \"vae.bson\") vae","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"draw_mnist(i) = reshape(X[:,rand(findall(Flux.onecold(ys,0:9).==i))],input_dim,1)\nreconstruct_mnist(x) = σ.(CounterfactualExplanations.GenerativeModels.reconstruct(vae,x)[1]) reshape_mnist(x) = reshape(x,28,28)\nplt_list = []\nfor i in 0:9\n  x = draw_mnist(i)\n  x_rec = reconstruct_mnist(x)\n  img = plot(convert2image(MNIST,reshape_mnist(x)), title=\"Original\", axis=nothing)\n  img_rec = plot(convert2image(MNIST,reshape_mnist(x_rec)), title=\"Rec\", axis=nothing)\n  plt = plot(img, img_rec)\n  plt_list = vcat(plt_list, plt)\nend\nplt = plot(plt_list..., layout=(10,1), size=(200,1000))\nsavefig(plt, joinpath(www_path, \"vae_reconstruct.png\"))","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"… and supplied to an instance of type CounterfactualData. The figure below shows randomly selected MNIST images (left) and their reconstructions (right).","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"counterfactual_data = CounterfactualData(X,ys)\ncounterfactual_data.generative_model = vae # assign generative model","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"(Image: Randomly selected MNIST images (left) and their reconstructions (right).)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/#Counterfactual-search","page":"Latent Space Search","title":"Counterfactual search","text":"","category":"section"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"Since the image reconstructions are decent, we can expect the counterfactual search through the latent embedding to yield realistic counterfactuals. Below we put this to the test: we select a random nine (9) and use generic search in the latent space to generate a four (4). Note, that we have set the threshold probability to 90% and we have chosen not to penalize the distance of the counterfactual from its factual. The result shown in the figure below is convincing.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"# Randomly selected factual:\nfactual = 10\nusing Random\nRandom.seed!(1234)\nx = reshape(X[:,rand(findall(Flux.onecold(ys,1:10).==factual))],input_dim,1)\ntarget = 5\nγ = 0.90","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"# Define generator:\ngenerator = GenericGenerator(;ϵ=1.0,λ=0.0)\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator, latent_space=true, γ=γ)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"p1 = plot(convert2image(MNIST, reshape(x,28,28)),axis=nothing, title=\"Factual\")\ntarget_prob = round(target_probs(counterfactual)[1],digits=2)\nŷ = CounterfactualExplanations.Counterfactuals.counterfactual_label(counterfactual)[1]-1\np2 = plot(\n  convert2image(MNIST, reshape(counterfactual.f(counterfactual.s′),28,28)),\n  axis=nothing,title=\"ŷ=$(ŷ); p̂(y=$(target-1))=$(target_prob)\"\n)\nplt = plot(p1,p2,size=(500,200))\nsavefig(plt, joinpath(www_path, \"mnist_$(factual-1)to$(target-1)_latent.png\"))","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator, latent_space=false, γ=0.95)","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"p1 = plot(convert2image(MNIST, reshape(x,28,28)),axis=nothing, title=\"Factual\")\ntarget_prob = round(target_probs(counterfactual)[1],digits=2)\nŷ = CounterfactualExplanations.Counterfactuals.counterfactual_label(counterfactual)[1]-1\np2 = plot(\n  convert2image(MNIST, reshape(counterfactual.f(counterfactual.s′),28,28)),\n  axis=nothing,title=\"ŷ=$(ŷ); p̂(y=$(target-1))=$(target_prob)\"\n)\nplt = plot(p1,p2,size=(500,200))\nsavefig(plt, joinpath(www_path, \"mnist_$(factual-1)to$(target-1)_wachter.png\"))","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"(Image: )","category":"page"},{"location":"generators/gradient_based/latent_space_generator/#References","page":"Latent Space Search","title":"References","text":"","category":"section"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. “Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.” arXiv Preprint arXiv:1907.09615.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"[1] In general, we believe that there may be a trade-off between creating counterfactuals that respect the DGP vs. counterfactuals reflect the behaviour of the black-model in question - both accurately and complete.","category":"page"},{"location":"generators/gradient_based/latent_space_generator/","page":"Latent Space Search","title":"Latent Space Search","text":"[2] We believe that there is another potentially crucial disadvantage of relying on a separate generative model: it reallocates the task of learning realisitic explanations for the data from the black-box model to the generative model.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"reference/#All-functions-and-types","page":"Reference","title":"All functions and types","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/#Exported-functions","page":"Reference","title":"Exported functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [\n    CounterfactualExplanations, \n    CounterfactualExplanations.Data,\n    CounterfactualExplanations.DataPreprocessing,\n    CounterfactualExplanations.Counterfactuals, \n    CounterfactualExplanations.Models, \n    CounterfactualExplanations.Generators, \n    CounterfactualExplanations.Losses\n]\nPrivate = false","category":"page"},{"location":"reference/#Internal-functions","page":"Reference","title":"Internal functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [\n    CounterfactualExplanations, \n    CounterfactualExplanations.Data,\n    CounterfactualExplanations.DataPreprocessing,\n    CounterfactualExplanations.Counterfactuals, \n    CounterfactualExplanations.Models, \n    CounterfactualExplanations.Generators, \n    CounterfactualExplanations.Losses\n]\nPublic = false","category":"page"},{"location":"resources/resources/#Further-Resources","page":"Additional Resources","title":"Further Resources","text":"","category":"section"},{"location":"resources/resources/#JuliaCon-2022","page":"Additional Resources","title":"JuliaCon 2022","text":"","category":"section"},{"location":"resources/resources/","page":"Additional Resources","title":"Additional Resources","text":"Slides: link","category":"page"},{"location":"resources/resources/","page":"Additional Resources","title":"Additional Resources","text":"<iframe style=\"width:560px;height:315px\" src=\"https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/resources/juliacon22/presentation.html\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"contributing/index.commonmark/","page":"Contributing","title":"Contributing","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"contributing/index.commonmark/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/index.commonmark/","page":"Contributing","title":"Contributing","text":"Our goal is to provide a go-to place for counterfactual explanations in Julia. To this end, the following is a non-exhaustive list of exciting feature developments we envision:","category":"page"},{"location":"contributing/index.commonmark/","page":"Contributing","title":"Contributing","text":"Additional counterfactual generators and predictive models.\nAdditional datasets for testing, evaluation and benchmarking.\nImproved preprocessing including native support for categorical features.\nSupport for regression models.","category":"page"},{"location":"contributing/index.commonmark/","page":"Contributing","title":"Contributing","text":"We are also interested in suggestions on how to adjust and improve the inner workings of our package. To facilitate this process the following page explain and justify the package architecture and the design choices we have made.","category":"page"},{"location":"contributing/index.commonmark/#How-to-contribute?","page":"Contributing","title":"How to contribute?","text":"","category":"section"},{"location":"contributing/index.commonmark/","page":"Contributing","title":"Contributing","text":"All of the following contributions are welcome:","category":"page"},{"location":"contributing/index.commonmark/","page":"Contributing","title":"Contributing","text":"Should you spot any errors or something is not working, please just open an issue.\nIf you want to contribute your own code, please proceed as follows:\nFork this repo and clone your fork: git clone https://github.com/your_username/CounterfactualExplanations.jl.\nAdd a remote corresponding to this repository: git remote add upstream https://github.com/pat-alt/CounterfactualExplanations.jl.git\nImplement your modifications and submit a pull request.\nFor any other questions or comments you can also start a discussion.","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"examples/image/MNIST/#MNIST","page":"MNIST","title":"MNIST","text":"","category":"section"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"In this example we will see how different counterfactual generators can be used to explain deep learning models for image classification. In particular, we will look at MNIST data and visually inspect how the different generators perturb images of handwritten digits in order to change the predicted label to a target label. Figure 1 shows a random sample of handwritten digits.","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"(Image: Figure 1: A few random handwritten digits.)","category":"page"},{"location":"examples/image/MNIST/#Pre-trained-classifiers","page":"MNIST","title":"Pre-trained classifiers","text":"","category":"section"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"Next we will load two pre-trained deep-learning classifiers:","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"Simple MLP - model\nDeep ensemble - ensemble","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"using Flux\nusing CounterfactualExplanations.Data: mnist_data, mnist_model, mnist_ensemble\nX, ys = mnist_data()\nmodel = mnist_model()\nensemble = mnist_ensemble()","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"The following code just prepares the models to be used with CounterfactualExplanations.jl:","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"using CounterfactualExplanations, CounterfactualExplanations.Models\nimport CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n\n# MLP:\n# Step 1)\nstruct NeuralNetwork <: Models.AbstractFittedModel\n    model::Any\nend\n# Step 2)\nlogits(M::NeuralNetwork, X::AbstractArray) = M.model(X)\nprobs(M::NeuralNetwork, X::AbstractArray)= softmax(logits(M, X))\nM = NeuralNetwork(model)\n\n# Deep ensemble:\n# Step 1)\nstruct FittedEnsemble <: Models.AbstractFittedModel\n    ensemble::AbstractArray\nend\n# Step 2)\nusing Statistics\nlogits(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([nn(X) for nn in M.ensemble],3), dims=3)\nprobs(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([softmax(nn(X)) for nn in M.ensemble],3),dims=3)\nM_ensemble=FittedEnsemble(ensemble)","category":"page"},{"location":"examples/image/MNIST/#Generating-counterfactuals","page":"MNIST","title":"Generating counterfactuals","text":"","category":"section"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"We will look at four different approaches here:","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"Generic approach for the MLP (Wachter, Mittelstadt, and Russell 2017).\nGreedy approach for the MLP.\nGeneric approach for the deep ensemble.\nGreedy approach for the deep ensemble (Schut et al. 2021).","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"They can be implemented using the GenericGenerator and the GreedyGenerator.","category":"page"},{"location":"examples/image/MNIST/#Turning-a-9-into-a-4","page":"MNIST","title":"Turning a 9 into a 4","text":"","category":"section"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"We will start with an example that should yield intuitive results: the process of turning a handwritten 9 in Figure 2 into a 4 is straight-forward for a human - just erase the top part. Let’s see how the different algorithmic approaches perform. First, we preprocess the data below, where we impose that the features (pixel values) are constrained to the follwoing domain: 𝒳 = [0,1] ⊂ ℝ.","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"counterfactual_data = CounterfactualData(X,ys';domain=(0,1))","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"Next we choose a random sample for which we will generate counterfactuals in the following:","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"# Randomly selected factual:\nusing Random\nRandom.seed!(1234)\nx = Flux.unsqueeze(select_factual(counterfactual_data, rand(1:size(X)[2])),2)\ntarget = 5\nγ = 0.95","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"(Image: Figure 2: A random handwritten 9.)","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"The code below implements the four different approaches one by one. Figure 3 shows the resulting counterfactuals. In every case the desired label switch is achieved, that is the corresponding classifier classifies the counterfactual as a four. But arguably from a human perspective only the counterfactuals for the deep ensemble look like a 4. For the MLP, both the generic and the greedy approach generate counterfactuals that look much like adversarial examples.","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"# Generic - MLP\ngenerator = GenericGenerator(;loss=:logitcrossentropy)\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator; γ=γ)\nimg = convert2image(reshape(counterfactual.x′,Int(√(input_dim)),Int(√(input_dim))))\nplt_wachter = plot(img, title=\"MLP - Wachter\")\n\n# Generic - Deep Ensemble\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M_ensemble, generator; γ=γ)\nimg = convert2image(reshape(counterfactual.x′,Int(√(input_dim)),Int(√(input_dim))))\nplt_wachter_de = plot(img, title=\"Ensemble - Wachter\")\n\n# Greedy - MLP\ngenerator = GreedyGenerator(;loss=:logitcrossentropy)\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator; γ=γ)\nimg = convert2image(reshape(counterfactual.x′,Int(√(input_dim)),Int(√(input_dim))))\nplt_greedy = plot(img, title=\"MLP - Greedy\")\n\n# Greedy - Deep Ensemble\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M_ensemble, generator; γ=γ)\nimg = convert2image(reshape(counterfactual.x′,Int(√(input_dim)),Int(√(input_dim))))\nplt_greedy_de = plot(img, title=\"Ensemble - Greedy\")\n\nplt_list = [plt_orig, plt_wachter, plt_greedy, plt_wachter_de, plt_greedy_de]\nplt = plot(plt_list...,layout=(1,length(plt_list)),axis=nothing, size=(1200,240))\nsavefig(plt, joinpath(www_path, \"MNIST_9to4.png\"))","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"(Image: Figure 3: Counterfactual explanations for MNIST data: turning a 9 into a 4)","category":"page"},{"location":"examples/image/MNIST/#References","page":"MNIST","title":"References","text":"","category":"section"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. “Generating Interpretable Counterfactual Explanations by Implicit Minimisation of Epistemic and Aleatoric Uncertainties.” In International Conference on Artificial Intelligence and Statistics, 1756–64. PMLR.","category":"page"},{"location":"examples/image/MNIST/","page":"MNIST","title":"MNIST","text":"Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” Harv. JL & Tech. 31: 841.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"tutorials/multi/#Counterfactuals-for-multi-class-targets","page":"Multi-class target","title":"Counterfactuals for multi-class targets","text":"","category":"section"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"In the existing literature counterfactual explanations have typically been applied in the binary classification setting (Verma, Dickerson, and Hines 2020). Research on algorithmic recourse in particular typically involves real-world datasets with an obvious target class - e.g. individual receives credit - and an adverse outcome - e.g. individual is denied loan (Karimi et al. 2020). Still, counterfactual explanations are very much also applicable in the multi-class setting. In this tutorial we will go through an illustrative example involving the toy dataset shown in Figure 1 below.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"using CounterfactualExplanations.Data\nusing MLJ\np = 5\nn = 1000\nX, ys = make_blobs(n, p, centers=3, as_table=false)\nX = X'\nusing MLUtils\nxs = unstack(X,dims=2)\ny_train = Flux.onehotbatch(ys, sort(unique(ys)))\ny_train = Flux.unstack(y_train',1)\ncounterfactual_data = CounterfactualData(X,ys')","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"(Image: Figure 1: Synthetic dataset containing four different classes.)","category":"page"},{"location":"tutorials/multi/#Classifier","page":"Multi-class target","title":"Classifier","text":"","category":"section"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"To classify the data we use a simple multi-layer perceptron (MLP). In this case the MLP outputs four logits, one for each class. Contrary to the binary setting we therefore choose logitcrossentropy as our loss functiona as opposed to logitbinarycrossentropy.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"n_hidden = 32\nout_dim = length(unique(ys))\nkw = (output_dim=out_dim,input_dim=size(X,1))\nnn = build_model(;kw...)\nloss(x, y) = Flux.Losses.logitcrossentropy(nn(x), y)\nps = Flux.params(nn)\ndata = zip(xs,y_train)","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"The following code just trains the neural network for the task:","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 100\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"To make the model compatible with our package we need to 1) declare it as a subtype of Models.AbstractFittedModel and 2) dispatch the relevant methods. Logits are returned by the model on call and passed through the softmax function to generate the vector of class probabilities.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"M = FluxModel(nn, likelihood=:classification_multi)","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"Figure 2 shows the resulting class probabilities in the two-dimensional feature domain.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"(Image: Figure 2: Class probabilities for MLP.)","category":"page"},{"location":"tutorials/multi/#Generating-counterfactuals","page":"Multi-class target","title":"Generating counterfactuals","text":"","category":"section"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"We randomly select an individual sample from any class and choose any of the remaining classes as our target at random.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"# Randomly selected factual:\n# Random.seed!(123)\nx = select_factual(counterfactual_data, rand(1:size(X)[2])) \ny = Flux.onecold(probs(M, x),unique(ys))\ntarget = rand(unique(ys)[1:end .!= y]) # opposite label as target","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"Generic counterfactual search can then be implemented as follows. The only difference to the binary setting is that we need to declare logitcrossentropy as the loss function for the counterfactual search. Figure 3 shows the resulting counterfactual path.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator, num_counterfactuals=1, γ=0.95)","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"(Image: Figure 3: Counterfactual path for generic generator.)","category":"page"},{"location":"tutorials/multi/#Deep-ensemble","page":"Multi-class target","title":"Deep ensemble","text":"","category":"section"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"Staying consistent with previous tutorial we will also briefly look at the Bayesian setting. To incorporate uncertainty we use a simple deep ensemble instead of a single MLP.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"ensemble = build_ensemble(5;kw=kw)\nensemble, = forward(ensemble, data, opt, n_epochs=epochs, plot_loss=false)","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"As before, we need to subtype and disptach:","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"import CounterfactualExplanations.Models: logits, probs\n# Step 1)\nstruct FittedEnsemble <: Models.AbstractDifferentiableModel\n    ensemble::AbstractArray\nend\n\n# Step 2)\nusing Statistics\nlogits(M::FittedEnsemble, X::AbstractArray) = selectdim(mean(Flux.stack([nn(X) for nn in M.ensemble],3), dims=3),3,1)\nprobs(M::FittedEnsemble, X::AbstractArray) = selectdim(mean(Flux.stack([softmax(nn(X)) for nn in M.ensemble],3),dims=3),3,1)\n\nM=FittedEnsemble(ensemble)","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"Figure 4 shows the resulting class probabilities.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"(Image: Figure 4: Class probabilities for deep ensemble.)","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"For the greedy recourse generator we also specify logitcrossentropy as our loss function and modify the hyperparameters slightly. Figure 5 shows the resulting counterfactual path.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"generator = GreedyGenerator(loss=:logitcrossentropy,δ=0.25,n=30)\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"(Image: Figure 5: Counterfactual path for greedy generator.)","category":"page"},{"location":"tutorials/multi/#References","page":"Multi-class target","title":"References","text":"","category":"section"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"Karimi, Amir-Hossein, Gilles Barthe, Bernhard Schölkopf, and Isabel Valera. 2020. “A Survey of Algorithmic Recourse: Definitions, Formulations, Solutions, and Prospects.” arXiv Preprint arXiv:2010.04050.","category":"page"},{"location":"tutorials/multi/","page":"Multi-class target","title":"Multi-class target","text":"Verma, Sahil, John Dickerson, and Keegan Hines. 2020. “Counterfactual Explanations for Machine Learning: A Review.” arXiv Preprint arXiv:2010.10596.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = CounterfactualExplanations","category":"page"},{"location":"#CounterfactualExplanations","page":"Home","title":"CounterfactualExplanations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for CounterfactualExplanations.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CounterfactualExplanations.jl is a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box algorithms. Both CE and AR are related tools for explainable artificial intelligence (XAI). While the package is written purely in Julia, it can be used to explain machine learning algorithms developed and trained in other popular programming languages like Python and R. See below for short introduction and other resources or dive straight into the docs.","category":"page"},{"location":"#Status","page":"Home","title":"🔁 Status","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is ready for research, but not for production. So far @pat-alt has developed this package entirely on his own and focussed on using it for his PhD research. The package is therefore still subject to some serious limitations (see below).","category":"page"},{"location":"#Installation","page":"Home","title":"Installation 🚩","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The first release of this package is now on Julia’s General Registry and can be installed as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"CounterfactualExplanations\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"CounterfactualExplanations.jl is currently under active development. To install the development version of the package you can run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/pat-alt/CounterfactualExplanations.jl\")","category":"page"},{"location":"#Background-and-motivation","page":"Home","title":"Background and motivation","text":"","category":"section"},{"location":"#The-Need-for-Explainability","page":"Home","title":"The Need for Explainability ⬛","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Machine learning models like deep neural networks have become so complex, opaque and underspecified in the data that they are generally considered as black boxes. Nonetheless, such models often play a key role in data-driven decision-making systems. This often creates the following problem: human operators in charge of such systems have to rely on them blindly, while those individuals subject to them generally have no way of challenging an undesirable outcome:","category":"page"},{"location":"","page":"Home","title":"Home","text":"“You cannot appeal to (algorithms). They do not listen. Nor do they bend.”— Cathy O’Neil in Weapons of Math Destruction, 2016","category":"page"},{"location":"#Enter:-Counterfactual-Explanations","page":"Home","title":"Enter: Counterfactual Explanations 🔮","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Counterfactual Explanations can help human stakeholders make sense of the systems they develop, use or endure: they explain how inputs into a system need to change for it to produce different decisions. Explainability benefits internal as well as external quality assurance. The figure below, for example, shows various counterfactuals generated through different approaches that all turn the predicted label of some classifier from a 9 into a 4. CEs that involve realistic and actionable changes such as the one on the far right can be used for the purpose of algorithmic recourse.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Realistic counterfactual explanations for MNIST data: turning a 4 into a 9.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Explanations that involve realistic and actionable changes can be used for the purpose of algorithmic recourse (AR): they offer human stakeholders a way to not only understand the system’s behaviour, but also react to it or adjust it. The figure below illustrates the point of AR through a toy example: it shows the counterfactual path of one sad cat 🐱 that would like to be grouped with her cool dog friends. Unfortunately, based on her tail length and height she was classified as a cat by a black-box classifier. The recourse algorithm perturbs her features in such a way that she ends up crossing the decision boundary into a dense region inside the target class.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: A sad 🐱 on its counterfactual path to its cool dog friends.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Counterfactual Explanations have certain advantages over related tools for explainable artificial intelligence (XAI) like surrogate eplainers (LIME and SHAP). These include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Full fidelity to the black-box model, since no proxy is involved.\nNo need for (reasonably) interpretable features as opposed to LIME and SHAP.\nClear link to Causal Inference and Bayesian Machine Learning.\nLess susceptible to adversarial attacks than LIME and SHAP.","category":"page"},{"location":"#Usage-example","page":"Home","title":"Usage example 🔍","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Generating counterfactuals will typically look like follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using CounterfactualExplanations\n\n# Data:\nusing CounterfactualExplanations.Data\nusing Random\nRandom.seed!(1234)\nxs, ys = Data.toy_data_linear()\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')\n\n# Model\nusing CounterfactualExplanations.Models: LogisticModel, probs \n# Logit model:\nw = [1.0 1.0] # true coefficients\nb = 0\nM = LogisticModel(w, [b])\n\n# Randomly selected factual:\nx = select_factual(counterfactual_data,rand(1:size(X)[2]))\ny = round(probs(M, x)[1])\ntarget = round(probs(M, x)[1])==0 ? 1 : 0 \n\n# Counterfactual search:\ngenerator = GenericGenerator()","category":"page"},{"location":"","page":"Home","title":"Home","text":"Running the counterfactual search yields:","category":"page"},{"location":"","page":"Home","title":"Home","text":"counterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Convergence: ✅\n\n after 28 steps.","category":"page"},{"location":"#Implemented-Counterfactual-Generators:","page":"Home","title":"Implemented Counterfactual Generators:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Currently the following counterfactual generators are implemented:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Generic (Wachter, Mittelstadt, and Russell 2017)\nGreedy (Schut et al. 2021)\nDiCE (Mothilal, Sharma, and Tan 2020)\nLatent Space Search as in REVISE (Joshi et al. 2019) and CLUE (Antorán et al. 2020)","category":"page"},{"location":"#Goals-and-limitations","page":"Home","title":"Goals and limitations 🎯","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The goal for this library is to contribute to efforts towards trustworthy machine learning in Julia. The Julia language has an edge when it comes to trustworthiness: it is very transparent. Packages like this one are generally written in pure Julia, which makes it easy for users and developers to understand and contribute to open source code. Eventually the aim for this project is to offer a one-stop-shop of counterfactual explanations. We want to deliver a package that is at least at par with the CARLA Python library in terms of its functionality. Currently the package falls short of this goal in a number of ways:","category":"page"},{"location":"","page":"Home","title":"Home","text":"The number of counterfactual generators is still limited.\nThe data preprocessing functionality needs to be extended; in particular, we need out-of-the-box support for categorical variables.\nThe functionality to add mutability constraints is still not implemented for Latent Space generators.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Additionally, our ambition is to enhance the package through the following features:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Language interoperability with Python and R: currently still only experimental.\nSupport for machine learning models trained in MLJ.jl.\nAdditional datasets for testing, evaluation and benchmarking.\nSupport for regression models.","category":"page"},{"location":"#Contribute","page":"Home","title":"Contribute 🛠","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributions of any kind are very much welcome! If any of the below applies to you, this might be the right open-source project for you:","category":"page"},{"location":"","page":"Home","title":"Home","text":"You’re an expert in Counterfactual Explanations or Explainable AI more broadly and you are curious about Julia.\nYou’re an experienced Julian and are happy to help a less experienced Julian (yours truly) up their game. Ideally, you are also curious about Trustworthy AI.\nYou’re new to Julia and open-source development and would like to start your learning journey by contributing to a recent but promising development. Ideally you are familiar with machine learning.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@pat-alt here: I am still very much at the beginning of my Julia journey, so if you spot any issues or have any suggestions for design improvement, please just open issue or start a discussion. Our goal is to provide a go-to place for counterfactual explanations in Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For more details on how to contribute see here. Please follow the SciML ColPrac guide.","category":"page"},{"location":"#Citation","page":"Home","title":"Citation 🎓","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you want to use this codebase, please consider citing:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@software{altmeyer2022CounterfactualExplanations,\n  author = {Patrick Altmeyer},\n  title = {{CounterfactualExplanations.jl - a Julia package for Counterfactual Explanations and Algorithmic Recourse}},\n  url = {https://github.com/pat-alt/CounterfactualExplanations.jl},\n  year = {2022}\n}","category":"page"},{"location":"#References","page":"Home","title":"References 📚","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Antorán, Javier, Umang Bhatt, Tameem Adel, Adrian Weller, and José Miguel Hernández-Lobato. 2020. “Getting a Clue: A Method for Explaining Uncertainty Estimates.” https://arxiv.org/abs/2006.06848.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. “Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.” https://arxiv.org/abs/1907.09615.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Mothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. “Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 607–17.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. “Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.” In International Conference on Artificial Intelligence and Statistics, 1756–64. PMLR.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” Harv. JL & Tech. 31: 841.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"CurrentModule = CounterfactualExplanations ","category":"page"},{"location":"tutorials/models/#Models","page":"Custom models","title":"Models","text":"","category":"section"},{"location":"tutorials/models/#Default-models","page":"Custom models","title":"Default models","text":"","category":"section"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"There are currently constructors for two default models, which mainly serve illustrative purposes (Figure 1 below). Both take sets of estimated parameters at the point of instantiation: the constructors will not fit a model for you, but assume that you have already estimated the respective model yourself and have access to its parameter estimates. Based on the supplied parameters methods to predict logits and probabilities are already implemented and used in the counterfactual search.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"(Image: Figure 1: Schematic overview of classes in `Models` module.)","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"For the simple logistic regression model logits are computed as a = X**w + b and probabilities are simply σ(a). For the Bayesian logistic regression model logits are computed as X**μ and the predictive posterior is computed through Laplace and Probit approximation.","category":"page"},{"location":"tutorials/models/#Custom-models","page":"Custom models","title":"Custom models","text":"","category":"section"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Apart from the default models you can use any arbitrary (differentiable) model and generate recourse in the same way as before. Only two steps are necessary to make your own Julia model compatible with this package:","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"The model needs to be declared as a subtype of CounterfactualExplanations.Models.AbstractFittedModel.\nYou need to extend the functions CounterfactualExplanations.Models.logits and CounterfactualExplanations.Models.probs to accept your custom model.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Below we will go through a simple example to see how this can be done in practice. In one of the following sections we will also see how to make model built and trained in other programming languages compatible with this library.","category":"page"},{"location":"tutorials/models/#Neural-network","page":"Custom models","title":"Neural network","text":"","category":"section"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"In this example we will build a simple artificial neural network using Flux for a binary classification task. First we generate some toy data below. The code that generates this data was borrowed from a great tutorial about Bayesian neural networks provided by Turing.jl, which you may find here.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"# Number of points to generate.\nN = 80\nM = round(Int, N / 4)\nRandom.seed!(1234)\n\nusing CounterfactualExplanations.Data\nxs, ys = Data.toy_data_non_linear(N)\nX = hcat(xs...)","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"The plot below shows the generated samples in the 2D feature space where colours indicate the associated labels. CounterfactualExplanationsly this data is not linearly separable and the default LogisticModel would be ill-suited for this classification task.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"(Image: )","category":"page"},{"location":"tutorials/models/#Training-the-model","page":"Custom models","title":"Training the model","text":"","category":"section"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Instead, we will build a simple artificial neural network nn with one hidden layer using a simple helper function build_model.[1] For additional resources on how to do deep learning with Flux just have a look at their documentation.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"nn = build_model(dropout=true,activation=Flux.σ)\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y)\nps = Flux.params(nn)\ndata = zip(xs,ys);","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"The code below trains the neural network for the task at hand while keeping track of the training loss. Note that normally we would be interested in loss with respect to a validation data set. But since we are primarily interested in generating counterfactual explanations for a trained classifier here, we will just keep things very simple on the training side.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"using Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 100\navg_loss(data) = mean(map(d -> loss(d[1],d[2]), data))\nshow_every = epochs/10\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\n  if epoch % show_every == 0\n    println(\"Epoch \" * string(epoch))\n    @show avg_loss(data)\n  end\nend","category":"page"},{"location":"tutorials/models/#Generating-counterfactuals","page":"Custom models","title":"Generating counterfactuals","text":"","category":"section"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Now it’s game time: we have a fitted model M : 𝒳 ↦ 𝒴 and are interested in generating recourse for some individual x ∈ 𝒳. As mentioned above we need to do a bit more work to prepare the model for use with our package.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"The code below takes care of all of that: in step 1) it declares our model as a subtype of Models.AbstractFittedModel and in step 2) it just extends the two functions.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"using CounterfactualExplanations, CounterfactualExplanations.Models\nimport CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n\n# Step 1)\nstruct NeuralNetwork <: Models.AbstractFittedModel\n    model::Any\nend\n\n# Step 2)\nlogits(M::NeuralNetwork, X::AbstractArray) = M.model(X)\nprobs(M::NeuralNetwork, X::AbstractArray)= σ.(logits(M, X))\nM = NeuralNetwork(nn)","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"The plot below shows the predicted probabilities in the feature domain. Evidently, our simple neural network is doing well on the training data.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"(Image: )","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"To preprocess the data for use with our package we simply run the following:","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"counterfactual_data = CounterfactualData(X,ys')","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Now we just select a random sample from our data and based on its current label we set as our target the opposite label.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"using Random\nRandom.seed!(123)\nx = select_factual(counterfactual_data, rand(1:size(X)[2])) \ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Then finally we use the GenericGenerator to generate counterfactual. The plot further below shows the resulting counterfactual path.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"(Image: )","category":"page"},{"location":"tutorials/models/#Ensemble-of-neural-networks","page":"Custom models","title":"Ensemble of neural networks","text":"","category":"section"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"In the context of Bayesian classifiers the GreedyGenerator can be used since minimizing the predictive uncertainty acts as a proxy for realism and unambiquity. In other words, if we have a model that incorporates uncertainty, we can generate realistic counterfactuals without the need for a complexity penalty.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"One efficient way to produce uncertainty estimates in the context of deep learning is to simply use an ensemble of artificial neural networks, also referred to as deep ensemble (Lakshminarayanan, Pritzel, and Blundell 2016). To this end, we can use the build_model function from above repeatedly to compose an ensemble of K neural networks:","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"ensemble = build_ensemble(5;kw=(dropout=true,activation=Flux.σ))","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Training this ensemble boils down to training each neural network separately:","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"ensemble, anim = forward(ensemble, data, opt, n_epochs=epochs, plot_every=show_every); # fit the ensemble\ngif(anim, joinpath(www_path, \"models_ensemble_loss.gif\"), fps=10)","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"(Image: )","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Once again it is straight-forward to make the model compatible with the package. Note that for an ensemble model the predicted logits and probabilities are just averages over predictions produced by all K models.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"# Step 1)\nstruct FittedEnsemble <: Models.AbstractFittedModel\n    ensemble::AbstractArray\nend\n\n# Step 2)\nlogits(M::FittedEnsemble, X::AbstractArray) = mean(Flux.flatten(Flux.stack([nn(X) for nn in M.ensemble],1)),dims=1)\nprobs(M::FittedEnsemble, X::AbstractArray) = mean(Flux.flatten(Flux.stack([σ.(nn(X)) for nn in M.ensemble],1)),dims=1)\n\nM=FittedEnsemble(ensemble)","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Again we plot the predicted probabilities in the feature domain. As expected the ensemble is more conservative because it incorporates uncertainty: the predicted probabilities splash out more than before, especially in regions that are not populated by samples.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"(Image: )","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Finally, we use the GreedyGenerator for the counterfactual search.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"generator = GreedyGenerator(Dict(:δ=>0.1,:n=>30))\ncounterfactual = generate_counterfactual(x, target, counterfactual_data, M, generator)","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"(Image: )","category":"page"},{"location":"tutorials/models/#References","page":"Custom models","title":"References","text":"","category":"section"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2016. “Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles.” arXiv Preprint arXiv:1612.01474.","category":"page"},{"location":"tutorials/models/","page":"Custom models","title":"Custom models","text":"[1] Helper functions like this one are not part of our package functionality. They can be found here.","category":"page"}]
}
