<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>- ¬∑ CounterfactualExplanations.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://pat-alt.github.io/CounterfactualExplanations.jl/intro/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">CounterfactualExplanations.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../install/">Installation</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/binary/">Binary target</a></li><li><a class="tocitem" href="../tutorials/models/">Models</a></li><li><a class="tocitem" href="../tutorials/multi/">Multi-class target</a></li><li><a class="tocitem" href="../tutorials/loss/">Loss functions</a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>-</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>-</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/pat-alt/CounterfactualExplanations.jl/blob/master/docs/src/intro.md#" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><ul><li><a href="#installation">Installation</a></li><li><a href="#background-and-motivation">Background and motivation</a></li><li><a href="#usage-example">Usage example</a></li><li><a href="#goals-and-limitations">Goals and limitations</a></li><li><a href="#citation">Citation</a></li></ul><p>CounterfactualExplanations.jl is a Julia package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box algorithms. Both CE and AR are related tools for interpretable machine learning. See below for short introduction and other resources or dive straight into the <a href="https://pat-alt.github.io/CounterfactualExplanations.jl/dev">docs</a>.</p><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>The package is in its early stages of development and currently awaiting registry on <a href="https://github.com/JuliaRegistries/General">Julia‚Äôs General Registry</a>. In the meantime it can be installed as follows:</p><pre><code class="language-julia hljs">using Pkg
Pkg.add(&quot;https://github.com/pat-alt/CounterfactualExplanations.jl&quot;)</code></pre><p>To instead install the development version of the package you can run the following command:</p><pre><code class="language-julia hljs">using Pkg
Pkg.add(url=&quot;https://github.com/pat-alt/CounterfactualExplanations.jl&quot;, rev=&quot;dev&quot;)</code></pre><h2 id="Background-and-motivation"><a class="docs-heading-anchor" href="#Background-and-motivation">Background and motivation</a><a id="Background-and-motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Background-and-motivation" title="Permalink"></a></h2><p>Algorithms used for automated decision-making such as deep neural networks have become so complex and opaque over recent years that they are generally considered as black boxes. This creates the following undesirable scenario: the human operators in charge of the black-box decision-making system do not understand how it works and essentially rely on it blindly. Conversely, those individuals who are subject to the decisions produced by such systems typically have no way of challenging them.</p><blockquote><p>‚ÄúYou cannot appeal to (algorithms). They do not listen. Nor do they bend.‚Äù</p><p>‚Äî Cathy O‚ÄôNeil in <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction"><em>Weapons of Math Destruction</em></a>, 2016</p></blockquote><p><strong>Counterfactual Explanations can help programmers make sense of the systems they build: they explain how inputs into a system need to change for it to produce a different output</strong>. <a href="#fig-mnist">Figure¬†1</a>, for example, shows various counterfactuals generated through different approaches that all turn the predicted label of some classifier from a 9 into a 4. CEs that involve realistic and actionable changes such as the one on the far right can be used for the purpose of individual recourse.</p><p>&lt;figure&gt; &lt;img src=&quot;examples/image/www/MNIST_9to4.png&quot; id=&quot;fig-mnist&quot; alt=&quot;Figure 1: Realistic counterfactual explanations for MNIST data: turning a 4 into a 9.&quot; /&gt; &lt;figcaption aria-hidden=&quot;true&quot;&gt;Figure 1: Realistic counterfactual explanations for MNIST data: turning a 4 into a 9.&lt;/figcaption&gt; &lt;/figure&gt;</p><p><strong>Algorithmic Recourse (AR) offers individuals subject to algorithms a way to turn a negative decision into positive one</strong>. <a href="#fig-cat">Figure¬†2</a> illustrates the point of AR through a toy example: it shows the counterfactual path of one sad cat üê± that would like to be grouped with her cool dog friends. Unfortunately, based on her tail length and height she was classified as a cat by a black-box classifier. The recourse algorithm perturbs her features in such a way that she ends up crossing the decision boundary into a dense region inside the target class.</p><p>&lt;figure&gt; &lt;img src=&quot;examples/www/recourse_laplace.gif&quot; id=&quot;fig-cat&quot; alt=&quot;Figure 2: A sad üê± on its counterfactual path to its cool dog friends.&quot; /&gt; &lt;figcaption aria-hidden=&quot;true&quot;&gt;Figure 2: A sad üê± on its counterfactual path to its cool dog friends.&lt;/figcaption&gt; &lt;/figure&gt;</p><h2 id="Usage-example"><a class="docs-heading-anchor" href="#Usage-example">Usage example</a><a id="Usage-example-1"></a><a class="docs-heading-anchor-permalink" href="#Usage-example" title="Permalink"></a></h2><h2 id="Goals-and-limitations"><a class="docs-heading-anchor" href="#Goals-and-limitations">Goals and limitations</a><a id="Goals-and-limitations-1"></a><a class="docs-heading-anchor-permalink" href="#Goals-and-limitations" title="Permalink"></a></h2><p>The goal for this library is to contribute to efforts towards trustworthy machine learning in Julia. The Julia language has an edge when it comes to trustworthiness: it is very transparent. Packages like this one are generally written in 100% Julia, which makes it easy for users and developers to understand and contribute to open source code.</p><p>Eventually the aim for this project is to be at least at par with the amazing <a href="https://github.com/carla-recourse/CARLA">CARLA</a> Python library which was presented at NeurIPS 2021. Currently CounterfactualExplanations.jl falls short of this goal in a number of ways: 1) the number of counterfactual generators is limited, 2) it lacks a framework for evaluating and benchmarking different generators, 3) it has so far been a one-person effort and not yet gone through a formal review.</p><h2 id="Citation"><a class="docs-heading-anchor" href="#Citation">Citation</a><a id="Citation-1"></a><a class="docs-heading-anchor-permalink" href="#Citation" title="Permalink"></a></h2><p>If you want to use this codebase, please cite:</p><pre><code class="nohighlight hljs">@software{altmeyer2022CounterfactualExplanations,
  author = {Patrick Altmeyer},
  title = {{CounterfactualExplanations.jl - a julia package for Counterfactual Explanations and Algorithmic Recourse}},
  url = {https://github.com/pat-alt/CounterfactualExplanations.jl},
  version = {0.1.0},
  year = {2022}
}</code></pre></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.14 on <span class="colophon-date" title="Friday 11 March 2022 16:22">Friday 11 March 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
