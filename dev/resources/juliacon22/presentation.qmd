---
title: Explaining Black-Box Models through Counterfactuals
subtitle: JuliaCon 2022
author: Patrick Altmeyer
format: 
  revealjs:
    logo: www/delft_logo.png
    footer: JuliaCon 2022 - Explaining Black-Box Models through Counterfactuals
    self-contained: true
    smaller: true
    scrollable: true
    preview-links: auto
    fig-align: center
    slide-number: true
    transition: slide
    background-transition: fade
bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib
execute:
  eval: false
  echo: true
---

```{julia}
#| echo: false
using Pkg; Pkg.activate("dev")
using Plots, PlotThemes
theme(:wong)
include("dev/utils.jl")
www_path = "dev/resources/juliacon22/www"
```


## Overview

:::{.incremental}
- The Problem with Black Boxes â¬›
    - What are black-box models? Why do we need explainability?
- Enter: Counterfactual Explanations ðŸ”®
    - What are counterfactuals? What are they not?
- Counterfactual Explanations in Julia (and beyond!) ðŸ“¦
    - Introducing: [`CounterfactualExplanations.jl`](https://www.paltmeyer.com/CounterfactualExplanations.jl/stable/).
    - Package architecture
    - Usage examples - what can it do?
- Goals and Ambitions ðŸŽ¯
    - Future developments - where can it go?
    - Contributor's guide
:::

# The Problem with Black Boxes â¬›

## Short Lists, Pandas and Gibbons

> From **human** to **data-driven** decision-making ...

. . . 

:::{.incremental}

- Black-box models like deep neural networks are being deployed virtually everywhere.
- More likely than not that your loan or employment application is handled by an algorithm.
- Includes critical domains: health care, autonomous driving, finance, ... 

::: 

. . .

> ... where black boxes are recipe for disaster.

. . .

:::{.incremental}
- We have no idea what exactly we're cooking up ...
    - Have you received an automated rejection email? Why didn't you "mEet tHe sHoRtLisTiNg cRiTeRia"? ðŸ™ƒ
- ... but we do know that some of it is junk. 
:::

. . .

![Adversarial attacks on deep neural networks. Source: @goodfellow2014explaining](www/panda.png){#fig-panda width=50%}

## "Weapons of Math Destruction"

::::{.columns}

:::{.column width="70%"}

> â€œYou cannot appeal to (algorithms). They do not listen. Nor do they bend.â€
>
> â€” Cathy O'Neil in [*Weapons of Math Destruction*](https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction), 2016

![Cathy Oâ€™Neil. Source: Cathy Oâ€™Neil](www/cathy.webp){#fig-cathy width=60%}

:::

:::{.column width="30%"}

:::{.incremental}
- If left unchallenged, these properties of black-box models can create undesirable dynamics.
- Human operators in charge of the model have to rely on it blindly.
- Individuals subject to the model's decisions generally have no way to challenge an outcome.
:::

:::

::::

## Towards Trustworthy AI {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack}

::: {data-id="box3" style="background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;"}
Ground Truthing
:::

::: {data-id="box2" style="background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;"}
Probabilistic Models
:::

::: {data-id="box1" style="background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;"}
Counterfactual Reasoning
:::
:::

## Towards Trustworthy AI {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack}

::: {data-id="box3" style="background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;"}
Ground Truthing
:::

::: {data-id="box2" style="background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Probabilistic Models
:::

::: {data-id="box1" style="background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Counterfactual Reasoning
:::
:::

#### Objective

Let $\mathcal{D}={(x,y)}$ denote our true population of input-output pairs. Then we want to find a subsample of the true population

$$\mathcal{D}_n \subset \mathcal{D}$$

such that 

$$\mathcal{D}_n \sim p(\mathcal{D})$$

> Lots of open questions and work to be done, but not here and today.

## Towards Trustworthy AI {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack style="text-align: center;"}
::: {data-id="box3" style="background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Ground Truthing
:::

::: {data-id="box2" style="background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;"}
Probabilistic Models
:::

::: {data-id="box1" style="background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Counterfactual Reasoning
:::
:::

#### Objective

Let $p(\mathcal{D}_n|\theta)$ denote the likelihood of observing our subsample $\mathcal{D}_n$ under some model parameterized by $\theta$. Then we typically want to maximize this likelihood with respect to the parameters [@murphy2022probabilistic]:

$$\arg \max_{\theta} p(\mathcal{D}_n|\theta)$$

:::{.incremental}
- In an ideal world we can rely on parsimonious and interpretable models [@rudin2019stop].
- In practice these models often have performance limitations.
- Black-box models like deep neural networks are popular, but also the very opposite of parsimonious.
:::

. . .

> [...] deep neural networks are typically very underspecified by the available data, and [...] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. 
> [@wilson2020case]

In this setting it is often crucial to treat models probabilistically!

. . .

> Probabilistic models covered briefly today. More in my other talk 

## Towards Trustworthy AI {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack}
::: {data-id="box3" style="background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Ground Truthing
:::

::: {data-id="box2" style="background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;"}
Probabilistic Models
:::

::: {data-id="box1" style="background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; align-items: center;"}
Counterfactual Reasoning
:::
:::

> We can now make predictions - great! But do we know how the predictions are actually made?

. . .

#### Objective

Let $\hat\theta$ denote our MLE estimate (or MAP in the probabilistic setting). Then we are interested in understanding how predictions of our model change with respect to input changes.

$$\nabla_x p(y|x,\mathcal{D}_n)$$

:::{.incremental}
- Counterfactual reasoning boils down to simple questions: what if $x \Rightarrow x\prime$?
- By (strategically) perturbing features and checking the model output, we can (begin to) understand how the model makes its decisions.
:::

. . .

# Enter: Counterfactual Explanations ðŸ”®

## A Framework for Counterfactual Explanations

> Even though [...] interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the â€œblack boxâ€.
> [@wachter2017counterfactual]



::::{.columns}

:::{.column width="50%"}

#### Framework

. . .
 
Objective originally proposed by @wachter2017counterfactual is as follows

$$
\min_{x\prime \in \mathcal{X}} h(x\prime) \ \ \ \mbox{s. t.} \ \ \ M(x\prime) = t
$$ {#eq-obj}

where $h$ relates to the complexity of the counterfactual and $M$ denotes the classifier.

. . .

Typically this is approximated through regularization:

$$
x\prime = \arg \min_{x\prime}  \ell(M(x\prime),t) + \lambda h(x\prime)
$$ {#eq-solution} 

:::

:::{.column width="50%"}

#### Intuition

. . .

![A cat performing gradient descent in the feature space Ã  la @wachter2017counterfactual](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/www/recourse_mlp.gif){#fig-cat-mlp}

:::
::::


## Counterfactuals ... as in Adversarial Examples?

. . .

> Yes and no! 

While both are methodologically very similar, adversarial examples are meant to go undetected while CEs ought to be meaningful.

. . .

> Effective counterfactuals should meet certain criteria âœ…

- **closeness**: the average distance between factual and counterfactual features should be small (@wachter2017counterfactual)
- **actionability**: the proposed feature perturbation should actually be actionable (@ustun2019actionable, @poyiadzi2020face)
- **plausibility**: the counterfactual explanation should be plausible to a human (@joshi2019towards)
- **unambiguity**: a human should have no trouble assigning a label to the counterfactual (@schut2021generating)
- **sparsity**: the counterfactual explanation should involve as few individual feature changes as possible (@schut2021generating)
- **robustness**: the counterfactual explanation should be robust to domain and model shifts (@upadhyay2021towards)
- **diversity**: ideally multiple diverse counterfactual explanations should be provided (@mothilal2020explaining)
- **causality**: counterfactual explanations reflect the structural causal model underlying the data generating process (@karimi2020algorithmic, @karimi2021algorithmic)

## Counterfactuals ... as in Causal Inference?

. . .

> NO!

**Causal inference**: counterfactuals are thought of as unobserved states of the world that we would like to observe in order to establish causality.

:::{.incremental}
- The only way to do this is by actually interfering with the state of the world: $p(y|do(x),\theta)$. 
- In practice we can only move some individuals to the counterfactual state of the world and compare their outcomes to a control group. 
- Provided we have controlled for confounders, properly randomized, ... we can estimate an average treatment effect: $\hat\theta$. 
:::

. . .

**Counterfactual Explanations**: involves perturbing features **after** some model has been trained.

:::{.incremental}
- We end up comparing **modeled outcomes** $p(y|x,\hat\phi)$ and $p(y|x\prime,\hat\phi)$ for individuals.
- We have **not** magically solved causality.
:::

. . .

> But still ... there is an intriguing link between the two domains.

:::{.incremental}
- If we do have causal knowledge, let's leverage it: from minimal **perturbations** to minimal **interventions** (@karimi2020algorithmic, @karimi2021algorithmic)
- If CEs that rely on minimal interventions fail, does that not provide some evidence that the assumed causal graph is inaccurate?
- Very much a open research field ...
:::


## Probabilistic Methods for Counterfactual Explanations

When people say that counterfactuals should look **realistic** or **plausible**, they really mean that counterfactuals should be generated by the same Data Generating Process (DGP) as the factuals:

$$
x\prime \sim p(x)
$$

But how do we estimate $p(x)$? Two probabilistic approaches ...

. . .

::: {.panel-tabset}

### APPROACH 1: use the model itself

::::{.columns}

:::{.column width="50%"}
@schut2021generating note that by maximizing predictive probabilities $\sigma(M(x\prime))$ for **probabilistic** models $M\in\mathcal{\widetilde{M}}$ one implicitly minimizes **epistemic** and **aleotoric** uncertainty.

$$
x\prime = \arg \min_{x\prime}  \ell(M(x\prime),t) \ \ \ , \ \ \ M\in\mathcal{\widetilde{M}}
$$ {#eq-bayes} 

:::

:::{.column width="50%"}
![A cat performing gradient descent in the feature space Ã  la @schut2021generating](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/www/recourse_laplace.gif){#fig-cat-laplace width="70%"}
:::

::::

### APPROACH 2: use some generative model

::::{.columns}

:::{.column width="50%"}
Instead of perturbing samples directly, some have proposed to instead traverse a lower-dimensional latent embedding learned through a generative model [@joshi2019towards].

$$
z\prime = \arg \min_{z\prime}  \ell(M(dec(z\prime)),t) + \lambda h(x\prime) 
$$ {#eq-latent} 

and 

$$x\prime = dec(z\prime)$$

where $dec(\cdot)$ is the decoder function.
:::

:::{.column width="50%"}
![Counterfactual (yellow) generated through latent space search (right panel) following @joshi2019towards. The corresponding counterfactual path in the feature space is shown in the left panel.](www/example_3d.png){#fig-latent-3d width="80%"}
:::

::::

:::

# Counterfactual Explanations in Julia (and beyond!)

## Limited Software Availability  

- Some of the existing approaches scattered across different GitHub repositories (ðŸ).
- Only one unifying Python ðŸ library: CARLA [@pawelczyk2021carla].
    - Comprehensive and (somewhat) extensible ...
    - ... but not language-agnostic and some desirable functionality not supported.
    - Also not composable: each generator is treated as different class/entity.
- Both R and Julia lacking any kind of implementation. Until now ...

## Enter: `CounterfactualExplanations.jl` ðŸ“¦

[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://pat-alt.github.io/CounterfactualExplanations.jl/dev)
[![Build Status](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl)

> A unifying framework for generating Counterfactual Explanations.

:::{.incremental}
- Built in Julia, but essentially language agnostic:
    - Currently supporting explanations for differentiable models built in Julia (e.g. Flux) and torch (R and Python).
- Designed to be easily extensible through dispatch.
- Designed to be composable allowing users and developers to combine different counterfactual generators.
:::

. . .

> Julia has an edge with respect to Trustworthy AI: it's open-source, uniquely transparent and interoperable ðŸ”´ðŸŸ¢ðŸŸ£

# Package Architecture

> Modular, composable, scalable! 

## Overview

![Overview of package architecture. Modules are shown in red, structs in green and functions in blue.](../pkg_architecture.png){#fig-architecture width="70%"}

## Generators 

```{julia}
using CounterfactualExplanations, Plots, GraphRecipes
plt = plot(AbstractGenerator, method=:tree, fontsize=10, nodeshape=:rect, size=(1000,700))
savefig(plt, joinpath(www_path,"generators.png"))
```

![Type tree for `AbstractGenerator`.](www/generators.png){#fig-generators width="60%"}

## Models

```{julia}
plt = plot(AbstractFittedModel, method=:tree, fontsize=10, nodeshape=:rect, size=(1000,700))
savefig(plt, joinpath(www_path,"models.png"))
```

![Type tree for `AbstractFittedModel`.](www/models.png){#fig-models width="60%"}

# Basic Usage

## A simple example

::::{.columns}

:::{.column width="50%"}
1. Load and prepare some toy data.
2. Select a random sample.
3. Generate counterfactuals using different approaches.

```{julia}
# Data:
using Random
Random.seed!(123)
N = 100
using CounterfactualExplanations
xs, ys = toy_data_linear(N)
X = hcat(xs...)
counterfactual_data = CounterfactualData(X,ys')

# Randomly selected factual:
x = select_factual(counterfactual_data,rand(1:size(X)[2]))
```
:::

:::{.column width="50%"}
```{julia}
#| echo: false
X = hcat(xs...)
pyplot()
plt = plot()
plt = plot_data!(plt,X',ys)
plt = plot(plt, legend=:bottomright)
scatter!(plt, [x[1]], [x[2]], ms=10, mc=:transparent, msc=:darkred, label="Chosen sample")
savefig(plt, joinpath(www_path,"example_data.png"))
gr()
```

![Synthetic data.](www/example_data.png){#fig-data}
:::

::::

## Generic Generator

:::: {.columns}

::: {.column width="40%"}

#### Code 

```{julia}
# Model
using CounterfactualExplanations.Models: LogisticModel
w = [1.0 1.0] # estimated coefficients
b = 0 # estimated bias
M = LogisticModel(w, [b])

# Select target class:
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target

# Counterfactual search:
generator = GenericGenerator()
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator
)
```

```{julia}
#| echo: false
function anim_path(counterfactual)
    T = total_steps(counterfactual)
    X_path = reduce(hcat,path(counterfactual))
    yÌ‚ = target_probs(counterfactual,X_path)
    p1 = plot_contour(X',ys,M;colorbar=false, title="Counterfactual Path")
    anim = @animate for t in 1:T
        scatter!(p1, [path(counterfactual)[t][1]], [path(counterfactual)[t][2]], ms=5, color=Int(y), label="")
        p2 = plot(1:t, yÌ‚[1:t], xlim=(0,T), ylim=(0, 1), label="p(yâ€²=" * string(target) * ")", title="Validity", lc=:black)
        Plots.abline!(p2,0,counterfactual.params[:Î³],label="threshold Î³", ls=:dash) # decision boundary
        plot(p1,p2,size=(800,400))
    end
end
anim = anim_path(counterfactual)
gif(anim, joinpath(www_path, "example_generic.gif"), fps=25)
```
:::

::: {.column width="60%"}
#### Output

![Counterfactual path (left) and predicted probability (right) for `GenericGenerator`.](www/example_generic.gif){#fig-generic}
:::

::::

## Greedy Generator

:::: {.columns}

::: {.column width="40%"}

#### Code 

```{julia}
using LinearAlgebra
Î£ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix
Î¼ = hcat(b, w)
M = CounterfactualExplanations.Models.BayesianLogisticModel(Î¼, Î£)

# Select target class:
y = round(probs(M, x)[1])
target = ifelse(y==1.0,0.0,1.0) # opposite label as target

# Counterfactual search:
params = GreedyGeneratorParams(
  Î´ = 0.5,
  n = 10
)
generator = GreedyGenerator(;params=params)
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator
)
```

```{julia}
#| echo: false
anim = anim_path(counterfactual)
gif(anim, joinpath(www_path, "example_greedy.gif"), fps=15)
```
:::

::: {.column width="60%"}
#### Output

![Counterfactual path (left) and predicted probability (right) for `GreedyGenerator`.](www/example_greedy.gif){#fig-greedy}
:::

::::

## REVISE Generator

:::: {.columns}

::: {.column width="40%"}

#### Code 

```{julia}
# Counterfactual search:
generator = REVISEGenerator()
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator
)
```

```{julia}
# Counterfactual search:
generator = GenericGenerator()
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator,
  latent_space=true
)
```

```{julia}
#| echo: false
anim = anim_path(counterfactual)
gif(anim, joinpath(www_path, "example_revise.gif"), fps=10)
```
:::

::: {.column width="60%"}
#### Output

![Counterfactual path (left) and predicted probability (right) for `REVISEGenerator`.](www/example_revise.gif){#fig-greedy}
:::

::::

## MNIST - Latent Space Search

:::: {.columns}

::: {.column width="50%"}

#### Good VAE

. . .

Loading pre-trained classifiers and VAE ...

```{julia}
X, ys = mnist_data() 
model = mnist_model() # simple MLP
```

. . .

... instantiating model and attaching VAE.

```{julia}
M = FluxModel(model, likelihood=:classification_multi)
counterfactual_data = CounterfactualData(X,ys)
vae = mnist_vae()
counterfactual_data.generative_model = vae
```

. . .

> The results in @fig-mnist-latent look great!

![Turning a nine (9) into a four (4) using REVISE. It appears that the VAE is well-specified in this case.](www/mnist_9to4_latent.png){#fig-mnist-latent width="80%"}

:::

::: {.column width="50%"}

#### Bad VAE

. . .

> But things can also go wrong ...

The VAE used to generate the counterfactual in @fig-latent-fail is not expressive enough.

![Turning a seven (7) into a nine (9) using REVISE with a weak VAE.](www/mnist_7to9_latent.png){#fig-latent-fail width="60%"}

. . .

> The counterfactual in @fig-wachter-fail is also valid ... what to do?

![Turning a seven (7) into a nine (9) using generic search.](www/mnist_7to9_wachter.png){#fig-wachter-fail width="60%"}
:::

::::

# Customization

## Custom Models - Deep Ensemble

. . . 

Loading the pre-trained deep ensemble ...

```{julia}
ensemble = mnist_ensemble() # deep ensemble
```

. . .

**Step 1**: add composite type as subtype of `AbstractFittedModel`.

```{julia}
struct FittedEnsemble <: Models.AbstractFittedModel
    ensemble::AbstractArray
end
```

. . .

**Step 2**: dispatch `logits` and `probs` methods for new model type.

```{julia}
using Statistics
import CounterfactualExplanations.Models: logits, probs
logits(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([nn(X) for nn in M.ensemble],3), dims=3)
probs(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([softmax(nn(X)) for nn in M.ensemble],3),dims=3)
M = FittedEnsemble(ensemble)
```

. . .

> Results for a simple deep ensemble also look convincing!

![Turning a nine (9) into a four (4) using generic (Wachter) and greedy search for MLP and deep ensemble.](www/MNIST_9to4.png){#fig-mnist-schut width="80%"}

## Custom Models - Interoperability

Adding support for `torch` models was easy! Here's how I implemented it for `torch` classifiers trained in R.

. . .

:::: {.columns}

::: {.column width="40%"}

#### Source code

. . .

**Step 1**: add composite type as subtype of `AbstractFittedModel`

> Done [here](https://github.com/pat-alt/CounterfactualExplanations.jl/blob/19795f547d87d561d5906d7966e6fd1022fa8ceb/src/models/differentiable/R.jl#L10).

**Step 2**: dispatch `logits` and `probs` methods for new model type.

> Done [here](https://github.com/pat-alt/CounterfactualExplanations.jl/blob/19795f547d87d561d5906d7966e6fd1022fa8ceb/src/models/differentiable/R.jl#L21).

. . .

**Step 3**: add gradient access.

> Done [here](https://github.com/pat-alt/CounterfactualExplanations.jl/blob/19795f547d87d561d5906d7966e6fd1022fa8ceb/src/generators/gradient_based/functions.jl#L22).

:::

::: {.column width="40%"}

#### Unchanged API

. . . 

```{julia}
using RCall
synthetic = load_synthetic([:r_torch])
model = synthetic[:classification_binary][:models][:r_torch][:raw_model]
M = RTorchModel(model)
# Define generator:
generator = GenericGenerator()
# Generate recourse:
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator
)
```

```{julia}
#| echo: false
anim = anim_path(counterfactual)
gif(anim, joinpath(www_path, "example_interop.gif"), fps=25)
```

:::

::::

## Custom Generators

Idea ðŸ’¡: let's implement a generic generator with dropout!

. . . 

:::: {.columns}

::: {.column width="50%"}
#### Dispatch

. . .

**Step 1**: create a subtype of `AbstractGradientBasedGenerator` (adhering to some basic rules).

```{julia}
# Constructor:
struct DropoutGenerator <: AbstractGradientBasedGenerator
    loss::Symbol # loss function
    complexity::Function # complexity function
    mutability::Union{Nothing,Vector{Symbol}} # mutibility constraints 
    Î»::AbstractFloat # strength of penalty
    Ïµ::AbstractFloat # step size
    Ï„::AbstractFloat # tolerance for convergence
    p_dropout::AbstractFloat # dropout rate
end
```

. . . 

**Step 2**: implement logic for generating perturbations.

```{julia}
import CounterfactualExplanations.Generators: generate_perturbations, âˆ‡
using StatsBase
function generate_perturbations(generator::AbstractDropoutGenerator, counterfactual_state::CounterfactualState)
    ð â‚œ = âˆ‡(generator, counterfactual_state) # gradient
    # Dropout:
    set_to_zero = sample(1:length(ð â‚œ),Int(round(generator.p_dropout*length(ð â‚œ))),replace=false)
    ð â‚œ[set_to_zero] .= 0
    Î”xâ€² = - (generator.Ïµ .* ð â‚œ) # gradient step
    return Î”xâ€²
end
```
:::

::: {.column width="50%"}
#### Unchanged API

. . .

```{julia}
# Instantiate:
using LinearAlgebra
generator = DropoutGenerator(
    :logitbinarycrossentropy,
    norm,
    nothing,
    0.1,
    0.1,
    1e-5,
    0.5
)
counterfactual = generate_counterfactual(
  x, target, counterfactual_data, M, generator
)
```

```{julia}
#| echo: false
anim = anim_path(counterfactual)
gif(anim, joinpath(www_path, "example_dropout.gif"), fps=25)
```

![](www/dropout_recourse.gif)

:::

::::

# Goals and Ambitions ðŸŽ¯

## JuliaCon 2022 and beyond

::::{.columns}

:::{.column width="50%"}

:::{.fragment .semi-fade-out fragment-index=4}
#### To JuliaCon ...

:::{.fragment .strike fragment-index=1}
Develop package, register and submit to [JuliaCon 2022](https://juliacon.org/2022/).
:::

:::{.fragment .strike fragment-index=2}
Native support for deep learning models (`Flux`, `torch`).
:::

:::{.fragment .strike fragment-index=3}
Add latent space search.
:::
:::

#### ... and beyond

. . .

- Add more generators:
  - DiCE [@mothilal2020explaining]
  - ROAR [@upadhyay2021towards]
  - MINT [@karimi2021algorithmic]

. . .

- Add support for more models: 
  - `MLJ`, `GLM`, ...
  - Non-differentiable

. . .

- Enhance preprocessing functionality.

. . .

- Extend functionality to regression problems.

. . .

- Use `Flux` optimizers.

. . .

- ...
:::

:::{.column width="50%"}
![Source: Ivan Diaz on [Unsplash](https://unsplash.com/photos/YOy-ek-aBR0)](www/launch.jpeg){width="70%"}
:::

::::

## Contributor's Guide

## More Resources

::::{.columns}

:::{.column width="50%"}
- Introductory [blog post: [[TDS](https://towardsdatascience.com/individual-recourse-for-black-box-models-5e9ed1e4b4cc), [homepage](https://www.paltmeyer.com/blog/posts/individual-recourse-for-black-box-models/)]
- Package [docs](https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/) with lots of examples
- Blog post on effortless Bayesian deep learning [[TDS]((https://towardsdatascience.com/go-deep-but-also-go-bayesian-ab25efa6f7b)), [homepage](https://www.paltmeyer.com/blog/posts/effortsless-bayesian-dl/)]

> ... or get in touch ðŸ¤—
:::

:::{.column width="50%"}
![](www/profile.jpg)
:::

::::

# Hidden {visibility="hidden"}

## Explainable AI (XAI) {visibility="hidden"}

- *interpretable* = inherently interpretable model, no extra tools needed (GLM, decision trees, rules, ...)  [@rudin2019stop]
- *explainable* = inherently not interpretable model, but explainable through XAI

#### Post-hoc Explainability:
- Local **surrogate explainers** like LIME and Shapley: useful and popular, but ... 
    - ... can be easily fooled [@slack2020fooling]
    - ... rely on reasonably interpretable features.
    - ... rely on the concept of fidelity.
- **Counterfactual explanations** explain how inputs into a system need to change for it to produce different decisions. 
    - Always full-fidelity, since no proxy involved. 
    - Intuitive interpretation and straight-forward implemenation.
    - Works well with Bayesian models. Clear link to Causal Inference. 
    - Does not rely on interpretable features.
- Realistic and actionable changes can be used for the purpose of **algorithmic recourse**.

## Feature Constraints {visibility="hidden"}

::: {.panel-tabset}

### Domain constraint

![](www/mutability_domain_2.gif)

### Code

Mutability constraints can be added at the preprocessing stage:

```{.julia}
counterfactual_data = CounterfactualData(X,ys';domain=[(-Inf,Inf),(-Inf,-0.5)])
```

:::

## Research Topics (1) - Student Project {visibility="hidden"}

> What happens once AR has actually been implemented? ðŸ‘€

:::{.incremental}
- Towards robust AR: protection against exogenous domain and model shifts [@upadhyay2021towards]
- What about endogenous model shifts?
:::

![](www/bayesian.gif){fig-align="center" width=800px} 

## Research Topics (2) {visibility="hidden"}

:::{.incremental}
- An effortless way to incorporate model uncertainty (w/o need for expensive generative model): *Laplace Redux*.
- Counterfactual explanations for time series data.
- Is CE really more intuitive? Could run a user-based study like in @kaur2020interpreting.
- More ideas form your side? ðŸ¤—
:::

## References 