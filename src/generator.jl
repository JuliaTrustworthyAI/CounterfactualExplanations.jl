# generators.jl
#
# Core package functionality that implements algorithmic recourse.

# --------------- Core constructor:
abstract type Generator end

# --- Outer methods:

# Generate recourse:
function generate_recourse(generator::Generator, xÌ…::Vector{x}, ğ“œ, target::Float64; T=1000, immutable_=[])
    
    # Setup and allocate memory:
    xÌ² = copy(xÌ…) # start from factual
    D = length(xÌ²)
    path = reshape(xÌ², 1, length(xÌ²)) # storing the path

    # Initialize:
    t = 1 # counter
    converged = convergence(generator, xÌ…, ğ“œ, target, xÌ²)

    # Search:
    while !converged && t < T 
        xÌ² = step(generator, xÌ…, ğ“œ, target, xÌ²)
        t += 1 # update number of times feature is changed
        converged = convergence(generator, xÌ…, ğ“œ, target, xÌ²) # check if converged
        path = vcat(path, reshape(xÌ², 1, D))
    end

    # Output:
    yÌ² = ğ“œ(xÌ²)
    recourse = Recourse(xÌ², yÌ², path, generator, immutable_, xÌ…, ğ“œ, target) 
    
    return recourse
    
end

# Generators:
struct GenericGenerator <: Generator
    Î»::Float64
    Ïµ::Float64
    Ï„::Float64
end

â„“(generator::GenericGenerator, ğ“œ, t) = - (t * log(ğ›”(a)) + (1-t) * log(1-ğ›”(a)))
cost(generator::GenericGenerator, xÌ…, xÌ²) = norm(xÌ…,xÌ²)^2
objective(generator::GenericGenerator, xÌ…, ğ“œ, target, xÌ²) = â„“(generator, a, t) + generator.Î» * cost(generator, xÌ…, xÌ²) 
âˆ‡(generator::GenericGenerator, xÌ…, ğ“œ, target, xÌ²) = gradient(() -> objective(generator, a, t, xÌ…, xÌ²), params(xÌ²))

function step(generator::GenericGenerator, xÌ…, ğ“œ, target, xÌ²) 
    ğ â‚œ = âˆ‡(generator, xÌ…, ğ“œ, target, xÌ²)
    return xÌ² - (generator.Ïµ .* ğ â‚œ)
end

function convergence(generator::GenericGenerator, xÌ…, ğ“œ, target, xÌ²)
    all(âˆ‡(generator, xÌ…, ğ“œ, target, xÌ²) .< generator.Ï„)
end

# --------------- Wachter et al (2018): 
function gradient_cost(x_f, xÌ²)
    (xÌ² .- x_f) ./ norm(xÌ² .- x_f)
end;

function generate_recourse_wachter(x, gradient, classifier, target; T=1000, immutable_=[], a=1, Ï„=1e-5, Î»=0.1, gradient_cost=gradient_cost)
    # Setup:
    w = coef(classifier)
    constant_needed = length(w) > length(x) # is adjustment for constant needed?
    if (constant_needed)
        x = vcat(1, x)
        immutable_ = vcat(1, immutable_ .+ 1) # adjust mask for immutable features
    end
    xÌ² = copy(x) # start from factual
    D = length(xÌ²)
    path = reshape(x, 1, length(x)) # storing the path
    function convergence_condition(xÌ², gradient, w, target, Ï„)
        all(gradient(xÌ²,w,target) .<= Ï„)
    end
    
    # Initialize:
    t = 1 # counter
    converged = convergence_condition(xÌ², gradient, w, target, Ï„)
    
    # Recursion:
    while !converged && t < T 
        ğ â‚œ = gradient(xÌ²,w,target) # compute gradient
        ğ â‚œ[immutable_] .= 0 # set gradient of immutable features to zero
        g_cost_t = gradient_cost(x,xÌ²) # compute gradient of cost function
        g_cost_t[immutable_] .= 0 # set gradient of immutable features to zero
        cost = norm(xÌ²-x) # update cost
        if cost != 0
            xÌ² -= (a .* (ğ â‚œ + Î» .* g_cost_t)) # counterfactual update
        else
            xÌ² -= (a .* ğ â‚œ)
        end
        t += 1 # update number of times feature is changed
        converged = convergence_condition(xÌ², gradient, w, target, Ï„) # check if converged
        path = vcat(path, reshape(xÌ², 1, D))
    end
    
    # Output:
    yÌ² = predict(classifier, xÌ²; proba=false)[1]
    valid = yÌ² == target * 1.0
    cost = norm(x.-xÌ²)
    if (constant_needed)
        path = path[:,2:end]
        xÌ² = xÌ²[2:end]
        x = x[2:end]
    end
    recourse = Recourse(xÌ², yÌ², path, target, valid, cost, x) 
    
    return recourse
end;

# --------------- Upadhyay et al (2021) 
function generate_recourse_roar(x, gradient, classifier, target; T=1000, immutable_=[], a=1, Ï„=1e-5, Î»=0.1, gradient_cost=gradient_cost)
    # Setup:
    w = coef(classifier)
    constant_needed = length(w) > length(x) # is adjustment for constant needed?
    if (constant_needed)
        x = vcat(1, x)
        immutable_ = vcat(1, immutable_ .+ 1) # adjust mask for immutable features
    end
    xÌ² = copy(x) # start from factual
    D = length(xÌ²)
    path = reshape(x, 1, length(x)) # storing the path
    function convergence_condition(xÌ², gradient, w, target, tol)
        all(gradient(xÌ²,w,target) .<= Ï„)
    end
    
    # Initialize:
    t = 1 # counter
    converged = convergence_condition(xÌ², gradient, w, target, Ï„)
    
    # Recursion:
    while !converged && t < T 
        ğ â‚œ = gradient(xÌ²,w,target) # compute gradient
        ğ â‚œ[immutable_] .= 0 # set gradient of immutable features to zero
        g_cost_t = gradient_cost(x,xÌ²) # compute gradient of cost function
        g_cost_t[immutable_] .= 0 # set gradient of immutable features to zero
        cost = norm(xÌ²-x) # update cost
        if cost != 0
            xÌ² -= (a .* (ğ â‚œ + Î» .* g_cost_t)) # counterfactual update
        else
            xÌ² -= (a .* ğ â‚œ)
        end
        t += 1 # update number of times feature is changed
        converged = convergence_condition(xÌ², gradient, w, target, Ï„) # check if converged
        path = vcat(path, reshape(xÌ², 1, D))
    end
    
    # Output:
    yÌ² = predict(classifier, xÌ²; proba=false)[1]
    valid = yÌ² == target * 1.0
    cost = norm(x.-xÌ²)
    if (constant_needed)
        path = path[:,2:end]
        xÌ² = xÌ²[2:end]
        x = x[2:end]
    end
    recourse = Recourse(xÌ², yÌ², path, target, valid, cost, x) 
    
    return recourse
end;

# --------------- Schut et al (2021) 
function generate_recourse_schut(x,gradient,classifier,target;T=1000,immutable_=[],Î“=0.95,Î´=1,n=nothing)
    # Setup:
    w = coef(classifier)
    constant_needed = length(w) > length(x) # is adjustment for constant needed?
    if (constant_needed)
        x = vcat(1, x)
        immutable_ = vcat(1, immutable_ .+ 1) # adjust mask for immutable features
    end
    xÌ² = copy(x) # start from factual
    D = length(xÌ²)
    D_mutable = length(setdiff(1:D, immutable_))
    path = reshape(x, 1, length(x)) # storing the path
    if isnothing(n)
        n = ceil(T/D_mutable)
    end
    
    # Intialize:
    t = 1 # counter
    P = zeros(D) # number of times feature is changed
    converged = posterior_predictive(classifier, xÌ²)[1] .> Î“ # check if converged
    max_number_changes_reached = all(P.==n)
    
    # Recursion:
    while !converged && t < T && !max_number_changes_reached
        ğ â‚œ = gradient(xÌ²,w,target) # compute gradient
        ğ â‚œ[P.==n] .= 0 # set gradient to zero, if already changed n times 
        ğ â‚œ[immutable_] .= 0 # set gradient of immutable features to zero
        i_t = argmax(abs.(ğ â‚œ)) # choose most salient feature
        xÌ²[i_t] -= Î´ * sign(ğ â‚œ[i_t]) # counterfactual update
        P[i_t] += 1 # update 
        t += 1 # update number of times feature is changed
        converged = posterior_predictive(classifier, xÌ²)[1] .> Î“ # check if converged
        max_number_changes_reached = all(P.==n)
        path = vcat(path, reshape(xÌ², 1, D))
    end
    
    # Output:
    yÌ² = predict(classifier, xÌ²; proba=false)[1]
    valid = yÌ² == target * 1.0
    cost = norm(x.-xÌ²)
    if (constant_needed)
        path = path[:,2:end]
        xÌ² = xÌ²[2:end]
        x = x[2:end]
    end
    recourse = Recourse(xÌ², yÌ², path, target, valid, cost, x) 
    
    return recourse
end;