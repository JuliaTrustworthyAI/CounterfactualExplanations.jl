
################################################################################
# --------------- Base type for gradient-based generator:
################################################################################
"""
    AbstractGradientBasedGenerator

An abstract type that serves as the base type for gradient-based counterfactual generators. 
"""
abstract type AbstractGradientBasedGenerator <: AbstractGenerator end

# ----- Julia models -----
"""
    âˆ‚â„“(generator::AbstractGradientBasedGenerator, M::Union{Models.LogisticModel, Models.BayesianLogisticModel}, counterfactual::Counterfactual)

The default method to compute the gradient of the loss function at the current counterfactual state for gradient-based generators. It assumes that `Zygote.jl` has gradient access.
"""
function âˆ‚â„“(generator::AbstractGradientBasedGenerator, M::Models.Models.AbstractDifferentiableJuliaModel, counterfactual::Counterfactual)
    gradient(() -> â„“(generator, counterfactual), Flux.params(counterfactual.xâ€²))[counterfactual.xâ€²]
end

# ----- RTorch model -----
using RCall
"""
    âˆ‚â„“(generator::AbstractGradientBasedGenerator, M::Models.RTorchModel, counterfactual::Counterfactual)

The default method to compute the gradient of the loss function at the current counterfactual state for gradient-based generators. It assumes that `Zygote.jl` has gradient access.
"""
function âˆ‚â„“(generator::AbstractGradientBasedGenerator, M::Models.RTorchModel, counterfactual::Counterfactual) 
    nn = M.nn
    x_cf = counterfactual.xâ€²
    t = counterfactual.target_encoded
    R"""
    x <- torch_tensor($x_cf, requires_grad=TRUE)
    output <- $nn(x)
    obj_loss <- nnf_binary_cross_entropy_with_logits(output,$t)
    obj_loss$backward()
    """
    grad = rcopy(R"as_array(x$grad)")
    return grad
end

# ----- PyTorch model -----
using PyCall
"""
    âˆ‚â„“(generator::AbstractGradientBasedGenerator, M::Models.RTorchModel, counterfactual::Counterfactual)

The default method to compute the gradient of the loss function at the current counterfactual state for gradient-based generators. It assumes that `Zygote.jl` has gradient access.
"""
function âˆ‚â„“(generator::AbstractGradientBasedGenerator, M::Models.PyTorchModel, counterfactual::Counterfactual) 
    py"""
    import torch
    from torch import nn
    """
    nn = M.nn
    xâ€² = counterfactual.xâ€²
    t = counterfactual.target_encoded
    x = reshape(xâ€², 1, length(xâ€²))
    py"""
    x = torch.Tensor($x)
    x.requires_grad = True
    t = torch.Tensor($[t]).squeeze()
    output = $nn(x).squeeze()
    obj_loss = nn.BCEWithLogitsLoss()(output,t)
    obj_loss.backward()
    """
    grad = vec(py"x.grad.detach().numpy()")
    return grad
end

"""
    âˆ‚h(generator::AbstractGradientBasedGenerator, counterfactual::Counterfactual)

The default method to compute the gradient of the complexity penalty at the current counterfactual state for gradient-based generators. It assumes that `Zygote.jl` has gradient access.
"""
âˆ‚h(generator::AbstractGradientBasedGenerator, counterfactual::Counterfactual) = gradient(() -> h(generator, counterfactual), Flux.params(counterfactual.xâ€²))[counterfactual.xâ€²]

# Gradient:
"""
    âˆ‡(generator::AbstractGradientBasedGenerator, M::Models.AbstractDifferentiableModel, counterfactual::Counterfactual)

The default method to compute the gradient of the counterfactual search objective for gradient-based generators. It simply computes the weighted sum over partial derivates. It assumes that `Zygote.jl` has gradient access.
"""
âˆ‡(generator::AbstractGradientBasedGenerator, M::Models.AbstractDifferentiableModel, counterfactual::Counterfactual) = âˆ‚â„“(generator, M, counterfactual) + generator.Î» * âˆ‚h(generator, counterfactual)

"""
    generate_perturbations(generator::AbstractGradientBasedGenerator, counterfactual::Counterfactual)

The default method to generate feature perturbations for gradient-based generators through simple gradient descent.
"""
function generate_perturbations(generator::AbstractGradientBasedGenerator, counterfactual::Counterfactual) 
    ğ â‚œ = âˆ‡(generator, counterfactual.M, counterfactual) # gradient
    Î”xâ€² = - (generator.Ïµ .* ğ â‚œ) # gradient step
    return Î”xâ€²
end

"""
    mutability_constraints(generator::AbstractGradientBasedGenerator, counterfactual::Counterfactual)

The default method to return mutability constraints that are dependent on the current counterfactual search state. For generic gradient-based generators, no state-dependent constraints are added.
"""
function mutability_constraints(generator::AbstractGradientBasedGenerator, counterfactual::Counterfactual)
    mutability = counterfactual.params[:mutability]
    return mutability # no additional constraints for GenericGenerator
end 

"""
    conditions_satisified(generator::AbstractGradientBasedGenerator, counterfactual::Counterfactual)

The default method to check if the all conditions for convergence of the counterfactual search have been satisified for gradient-based generators.
"""
function conditions_satisified(generator::AbstractGradientBasedGenerator, counterfactual::Counterfactual)
    ğ â‚œ = âˆ‡(generator, counterfactual.M, counterfactual)
    status = all(abs.(ğ â‚œ) .< generator.Ï„) 
    return status
end

##################################################
# Specific Generators
##################################################

include("GenericGenerator/GenericGenerator.jl") # Wachter et al. (2017)
include("GreedyGenerator/GreedyGenerator.jl") # Schut et al. (2021)