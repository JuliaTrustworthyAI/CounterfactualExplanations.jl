{
  "hash": "c764857f539f863c7b09119b599417d7",
  "result": {
    "markdown": "```@meta\nCurrentModule = CounterfactualExplanations \n```\n\n\n\n# Handling Data\n\nThe package works with custom data containers that contain the input and output data as well as information about the type and mutability of features. In this tutorial, we will see how data can be prepared for use with the package. \n\n## Basic Functionality\n\nTo demonstrate the basic way to prepare data, let's look at a standard benchmark dataset: Fisher's classic iris dataset. We can use `MLDatasets` to load this data.\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\ndataset = Iris()\n```\n:::\n\n\nOur data constructor `CounterfactualData` needs at least two inputs: features `X` and targets `y`. \n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nX = dataset.features\ny = dataset.targets\n```\n:::\n\n\nNext, we convert the input data to a `Tables.MatrixTable` (following `MLJ.jl`) convention. Concerning the target variable, we just assign grab the first column of the data frame. \n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nX = table(Tables.matrix(X))\ny = y[:,1]\n```\n:::\n\n\nNow we can feed these two ingredients to our constructor:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\ncounterfactual_data = CounterfactualData(X, y)\n```\n:::\n\n\nUnder the hood, the constructor performs basic preprocessing steps. For example, the output variable `y` is automatically one-hot encoded:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\ncounterfactual_data.y\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```\n3×150 Matrix{Bool}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     1  1  1  1  1  1  1  1  1  1  1  1\n```\n:::\n:::\n\n\nSimilarly, a transformer used to scale continuous input features is automatically fitted:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\ncounterfactual_data.dt\n```\n\n::: {.cell-output .cell-output-display execution_count=74}\n```\nZScoreTransform{Float64, Vector{Float64}}(4, 2, [5.843333333333335, 3.0540000000000007, 3.7586666666666693, 1.1986666666666672], [0.8280661279778629, 0.4335943113621737, 1.7644204199522617, 0.7631607417008414])\n```\n:::\n:::\n\n\n## Categorical Features\n\nFor the counterfactual search, it is important to distinguish between continuous and categorical features. This is because categorical features cannot be perturbed arbitrarily: they can take specific discrete values, but not just any value on the real line. \n\nConsider the following example:\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\ny = rand([1,0],4)\nX = (\n    name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n    grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n    sex=categorical([\"male\",\"female\",\"male\",\"male\"]),\n    height=[1.85, 1.67, 1.5, 1.67],\n)\nschema(X)\n```\n\n::: {.cell-output .cell-output-display execution_count=75}\n```\n┌────────┬──────────────────┬──────────────────────────────────┐\n│ names  │ scitypes         │ types                            │\n├────────┼──────────────────┼──────────────────────────────────┤\n│ name   │ Multiclass{4}    │ CategoricalValue{String, UInt32} │\n│ grade  │ OrderedFactor{3} │ CategoricalValue{String, UInt32} │\n│ sex    │ Multiclass{2}    │ CategoricalValue{String, UInt32} │\n│ height │ Continuous       │ Float64                          │\n└────────┴──────────────────┴──────────────────────────────────┘\n```\n:::\n:::\n\n\nTypically, in the context of Unserpervised Learning, categorical features are one-hot or dummy encoded. To this end, we could use `MLJ`, for example:\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nhot = OneHotEncoder()\nmach = MLJBase.fit!(machine(hot, X))\nW = MLJBase.transform(mach, X)\nX = permutedims(MLJBase.matrix(W))\n```\n:::\n\n\nIn all likelihood, this pre-processing step already happens at the stage, when the supervised model is trained. Since our counterfactual generators need to work in the same feature domain as the model they are intended to explain, we assume that categorical features are already encoded.\n\nThe `CounterfactualData` constructor takes two optional arguments that can be used to specify the indices of categorical and continuous features. By default, all features are assumed to be continuous. For categorical features, the constructor expects an array of arrays of integers (`Vector{Vector{Int}}`) where each subarray includes the indices of all one-hot encoded rows related to a single categorical feature. In the example above, the `name` feature is one-hot encoded across rows 1, 2, 3 and 4 of `X`, the `grade` feature is encoded across the following three rows, etc.\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nschema(W)\n```\n\n::: {.cell-output .cell-output-display execution_count=77}\n```\n┌──────────────┬────────────┬─────────┐\n│ names        │ scitypes   │ types   │\n├──────────────┼────────────┼─────────┤\n│ name__Danesh │ Continuous │ Float64 │\n│ name__John   │ Continuous │ Float64 │\n│ name__Lee    │ Continuous │ Float64 │\n│ name__Mary   │ Continuous │ Float64 │\n│ grade__A     │ Continuous │ Float64 │\n│ grade__B     │ Continuous │ Float64 │\n│ grade__C     │ Continuous │ Float64 │\n│ sex__female  │ Continuous │ Float64 │\n│ sex__male    │ Continuous │ Float64 │\n│ height       │ Continuous │ Float64 │\n└──────────────┴────────────┴─────────┘\n```\n:::\n:::\n\n\nThe code chunk below assigns the categorical and continuous feature indices:\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\nfeatures_categorical = [\n    [1,2,3,4],      # name\n    [5,6,7],        # grade\n    [8,9]           # sex\n]\nfeatures_continuous = [10]\n```\n:::\n\n\nWhen instantiating the data container, these indices just need to be supplied as keyword arguments:\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\ncounterfactual_data = CounterfactualData(\n    X,y;\n    features_categorical = features_categorical,\n    features_continuous = features_continuous\n)\n```\n:::\n\n\nThis will ensure that the discrete domain of categorical features is respected in the counterfactual search. We achieve this through a form of Projected Gradient Descent and it works for any of our counterfactual generators.\n\n### Example\n\nTo see this in action, let's load some synthetic data using `MLJ`:\n\n::: {.cell execution_count=13}\n``` {.julia .cell-code}\nN = 1000\nX, ys = MLJBase.make_blobs(N, 2; centers=2, as_table=false, center_box=(-5 => 5), cluster_std=0.5)\nys .= ys.==2\n```\n:::\n\n\nNext, we generate a synthetic categorical feature based on the output variable. First, we define the discrete levels:\n\n::: {.cell execution_count=14}\n``` {.julia .cell-code}\ncat_values = [\"X\",\"Y\",\"Z\"]\n```\n:::\n\n\n::: {.cell execution_count=15}\n\n::: {.cell-output .cell-output-display execution_count=82}\nNext, we impose that the categorical feature is most likely to take the first discrete level, namely X, whenever `y` is equal to $1$.\n\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.julia .cell-code}\nxcat = map(ys) do y\n    if y==1\n        x = sample(cat_values, Weights([0.8,0.1,0.1]))\n    else\n        x = sample(cat_values, Weights([0.1,0.1,0.8]))\n    end\nend\nxcat = categorical(xcat)\nX = (\n    x1 = X[:,1],\n    x2 = X[:,2],\n    x3 = xcat\n)\nschema(X)\n```\n:::\n\n\nAs above, we use a `OneHotEncoder` to transform the data:\n\n::: {.cell execution_count=17}\n``` {.julia .cell-code}\nhot = OneHotEncoder()\nmach = MLJBase.fit!(machine(hot, X))\nW = MLJBase.transform(mach, X)\nschema(W)\nX = permutedims(MLJBase.matrix(W))\n```\n:::\n\n\nFinally, we assign the categorical indices and instantiate our data container:\n\n::: {.cell execution_count=18}\n``` {.julia .cell-code}\nfeatures_categorical = [collect(3:size(X,1))]\ncounterfactual_data = CounterfactualData(\n    X,ys';\n    features_categorical = features_categorical,\n)\n```\n:::\n\n\nWith the data pre-processed we can use the [`fit_model`](@ref) function to train a simple classifier:\n\n::: {.cell execution_count=19}\n``` {.julia .cell-code}\nM = fit_model(counterfactual_data, :Linear)\n```\n:::\n\n\nNow it is finally time to generate counterfactuals. We first define $1$ as our target and then choose a random sample from the non-target class:\n\n::: {.cell execution_count=20}\n``` {.julia .cell-code}\ntarget = 1\nfactual = 0\nchosen = rand(findall(predict_label(M, counterfactual_data) .== factual))\nx = select_factual(counterfactual_data, chosen) \n```\n\n::: {.cell-output .cell-output-display execution_count=87}\n```\n5×1 Matrix{Float64}:\n -2.0187107665036255\n -2.1472582003885394\n  0.0\n  0.0\n  1.0\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n\n::: {.cell-output .cell-output-display execution_count=88}\nThe factual `x` belongs to group Z.\n\n:::\n:::\n\n\nWe generate a counterfactual for `x` using the standard API call:\n\n::: {.cell execution_count=22}\n``` {.julia .cell-code}\ngenerator = GenericGenerator()\nce = generate_counterfactual(x, target, counterfactual_data, M, generator)\n```\n\n::: {.cell-output .cell-output-display execution_count=89}\n```\nConvergence: ✅\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n after 3 steps.\n```\n:::\n:::\n\n\nThe search yields the following counterfactual:\n\n::: {.cell execution_count=23}\n``` {.julia .cell-code}\nx′ = counterfactual(ce)\n```\n\n::: {.cell-output .cell-output-display execution_count=90}\n```\n5×1×1 Align{Float64, 3} with eltype Float64:\n[:, :, 1] =\n -1.688050060018926\n  0.7516931376044904\n  1.0\n  0.0\n  0.0\n```\n:::\n:::\n\n\n::: {.cell execution_count=24}\n\n::: {.cell-output .cell-output-display execution_count=91}\nIt belongs to group X.\n\n:::\n:::\n\n\nThis is intuitive because by construction the categorical variable is most likely to take that value when `y` is equal to the target outcome.\n\n## Immutable Features\n\nIn practice, features usually cannot be perturbed arbitrarily. Suppose, for example, that one of the features used by a bank to predict the creditworthiness of its clients is *gender*. If a counterfactual explanation for the prediction model indicates that female clients should change their gender to improve their creditworthiness, then this is an interesting insight (it reveals gender bias), but it is not usually an actionable transformation in practice. In such cases, we may want to constrain the mutability of features to ensure actionable and realistic recourse. \n\nTo illustrate how this can be implemented in `CounterfactualExplanations.jl` we will continue to work with the synthetic data from the previous section. Mutability of features can be defined in terms of four different options: 1) the feature is mutable in both directions, 2) the feature can only increase (e.g. age), 3) the feature can only decrease (e.g. time left until your next deadline) and 4) the feature is not mutable (e.g. skin colour, ethnicity, ...). To specify which category a feature belongs to, you can pass a vector of symbols containing the mutability constraints at the pre-processing stage. For each feature you can choose from these four options: `:both` (mutable in both directions), `:increase` (only up), `:decrease` (only down) and `:none` (immutable). By default, `nothing` is passed to that keyword argument and it is assumed that all features are mutable in both directions.\n\nBelow we impose that the second feature is immutable. \n\n::: {.cell execution_count=25}\n``` {.julia .cell-code}\ncounterfactual_data = load_linearly_separable()\nM = fit_model(counterfactual_data, :Linear)\ncounterfactual_data.mutability = [:both, :none]\n```\n:::\n\n\n::: {.cell execution_count=26}\n``` {.julia .cell-code}\ntarget = 2\nfactual = 1\nchosen = rand(findall(predict_label(M, counterfactual_data) .== factual))\nx = select_factual(counterfactual_data, chosen) \nce = generate_counterfactual(x, target, counterfactual_data, M, generator)\n```\n:::\n\n\nThe resulting counterfactual path is shown in the chart below. Since only the first feature can be perturbed, the sample can only move along the horizontal axis.\n\n::: {.cell execution_count=27}\n``` {.julia .cell-code}\nplot(ce)\n```\n\n::: {.cell-output .cell-output-display execution_count=94}\n![Counterfactual path with an immutable feature.](data_preprocessing_files/figure-commonmark/fig-mutability-output-1.svg){#fig-mutability}\n:::\n:::\n\n\n<!-- ## Domain constraints\n\nIn some cases, we may also want to constrain the domain of some feature. For example, age as a feature is constrained to a range from 0 to some upper bound corresponding perhaps to the average life expectancy of humans. Below, for example, we impose an upper bound of $0.5$ for our two features.\n\n```{.julia}\ncounterfactual_data.mutability = [:both, :both]\ncounterfactual_data.domain = [(0,0) for var in counterfactual_data.features_continuous]\n```\n\nThis results in the counterfactual path shown below: since features are not allowed to be perturbed beyond the upper bound, the resulting counterfactual falls just short of the threshold probability $\\gamma$.\n\n```{.julia}\nce = generate_counterfactual(x, target, counterfactual_data, M, generator)\nplot(ce)\n``` -->\n\n",
    "supporting": [
      "data_preprocessing_files/figure-commonmark"
    ],
    "filters": []
  }
}