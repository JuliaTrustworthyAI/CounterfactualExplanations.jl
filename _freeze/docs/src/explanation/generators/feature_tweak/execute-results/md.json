{
  "hash": "9bd0362d515f3bf78500e51aef7bc7cb",
  "result": {
    "engine": "jupyter",
    "markdown": "``` @meta\nCurrentModule = CounterfactualExplanations \n```\n\n\n\n# `FeatureTweakGenerator`\n\n\n```{=commonmark}\n!!! warning \"Moved to extension\"\n    As of version `1.1.6`, the functionality of the `FeatureTweakGenerator` has been moved to the `DecisionTreeExt` extension. This means it is lazily loaded only if the `DecisionTree.jl` package is loaded by the user, since the `FeatureTweakGenerator` is only compatible with tree-based models. \n```\n\n\n**Feature Tweak** refers to the generator introduced by @tolomei2017interpretable. Our implementation takes inspiration from the [featureTweakPy library](https://github.com/upura/featureTweakPy).\n\n## Description\n\nFeature Tweak is a powerful recourse algorithm for ensembles of tree-based classifiers such as random forests. Though the problem of understanding how an input to an ensemble model could be transformed in such a way that the model changes its original prediction has been proven to be NP-hard [@tolomei2017interpretable], Feature Tweak provides an algorithm that manages to tractably solve this problem in multiple real-world applications. An example of a problem Feature Tweak is able to efficiently solve, explored in depth in @tolomei2017interpretable is the problem of transforming an advertisement that has been classified by the ensemble model as a low-quality advertisement to a high-quality one through small changes to its features. With the help of Feature Tweak, advertisers can both learn about the reasons a particular ad was marked to have a low quality, as well as receive actionable suggestions about how to convert a low-quality ad into a high-quality one.\n\nThough Feature Tweak is a powerful way of avoiding brute-force search in an exponential search space, it does not come without disadvantages. The primary limitations of the approach are that it's currently only applicable to tree-based classifiers and works only in the setting of binary classification. Another problem is that though the algorithm avoids exponential-time search, it is often still computationally expensive. The algorithm may be improved in the future to tackle all of these shortcomings.\n\nThe following equation displays how a true negative instance x can be transformed into a positively predicted instance **x'**. To be more precise, **x'** is the best possible transformation among all transformations **x\\***, computed with a cost function δ.\n\n``` math\n\\begin{aligned}\n\\mathbf{x}^\\prime = \\arg_{\\mathbf{x^*}} \\min \\{ {\\delta(\\mathbf{x}, \\mathbf{x^*}) | \\hat{f}(\\mathbf{x}) = -1 \\wedge \\hat{f}(\\mathbf{x^*}) = +1} \\}\n\\end{aligned}\n```\n\n## Example\n\nTo make use of the `FeatureTweakGenerator`, you need to have the `DecisionTree.jl` package installed. Loading the package will load the functionality of the `FeatureTweakGenerator` through the `DecisionTreeExt` extension:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing DecisionTree\n```\n:::\n\n\nIn this example we apply the Feature Tweak algorithm to a decision tree and a random forest trained on the moons dataset. We first load the data and fit the models:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nn = 500\ncounterfactual_data = CounterfactualData(TaijaData.load_moons(n)...)\n\n# Classifiers\ndecision_tree = CounterfactualExplanations.Models.fit_model(\n    counterfactual_data, :DecisionTree; max_depth=5, min_samples_leaf=3\n)\nforest = Models.fit_model(counterfactual_data, :RandomForest)\n```\n:::\n\n\nNext, we select a point to explain and a target class to transform the point to. We then search for counterfactuals using the `FeatureTweakGenerator`:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\n# Select a point to explain:\nx = float32.([1, -0.5])[:,:]\nfactual = Models.predict_label(forest, counterfactual_data, x)\ntarget = counterfactual_data.y_levels[findall(counterfactual_data.y_levels != factual)][1]\n\n# Search for counterfactuals:\ngenerator = FeatureTweakGenerator(ϵ=0.1)\ntree_counterfactual = generate_counterfactual(\n    x, target, counterfactual_data, decision_tree, generator\n)\nforest_counterfactual = generate_counterfactual(\n    x, target, counterfactual_data, forest, generator\n)\n```\n:::\n\n\nThe resulting counterfactuals are shown below:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\np1 = plot(\n    tree_counterfactual;\n    colorbar=false,\n    title=\"Decision Tree\",\n)\n\np2 = plot(\n    forest_counterfactual; title=\"Random Forest\",\n    colorbar=false,\n)\n\ndisplay(plot(p1, p2; size=(800, 400)))\n```\n\n::: {.cell-output .cell-output-display}\n![](feature_tweak_files/figure-commonmark/cell-6-output-1.svg){}\n:::\n:::\n\n\n## References\n\n",
    "supporting": [
      "feature_tweak_files"
    ],
    "filters": []
  }
}