{
  "hash": "00becf13887192ec34f2f0e3d12521b4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"When Causal meets Recourse\"\nsubtitle: \"Counterfactual Explanations through Structural Causal Models\"\ndate: '2024-09-03'\ndescription: |\n    This post introduces a new tool in CounterfactualExplanations.jl, enhancing the package with causal reasoning to generate counterfactual explanations. The functionality can be used to explain machine learning algorithms developed in Julia and other popular programming languages like Python and R. Unlike traditional counterfactual generators, this approach utilizes causal information to perturb features intelligently.\ncategories:\n  - counterfactuals\n  - explainable AI\n  - causality\n  - Julia\nimage: www/intro.gif\nexecute:\n  eval: false\n  echo: true\ndraft: false\n---\n\n\n# Introduction\n\nIn recent years, the need for interpretable and explainable AI has surged, particularly in high-stakes domains. Counterfactual explanations provide a means to understand how changes to input features could alter the outcomes of machine learning models. This blog post presents a new tool in the CounterfactualExplanations.jl package, developed during my GSoC project, which incorporates causal reasoning into counterfactual generation.\n\n# Project Overview\n\nThis project aimed to enhance the CounterfactualExplanations.jl package by infusing it with a robust mathematical foundation for algorithmic recourse, inspired by the principles of causal reasoning as delineated by Karimi et al. By bridging the gap between theoretical counterfactual understanding and practical change capabilities in machine learning decision processes, we enable actionable pathways stemming from counterfactual explanations.\n\n# Key Contributions\n\nDuring the project, I contributed to two key repositories:\n\n1. **CounterfactualExplanations.jl**: Developed a new tool for generating counterfactual explanations using causal information. This allows users to create smarter perturbations rather than random adjustments, ultimately providing more meaningful insights.\n\n2. **CausalInference.jl**: Implemented a Structural Causal Model (SCM) structure that extracts information from data, laying the groundwork for the causal reasoning capabilities in CounterfactualExplanations.jl.\n\nAdditionally, I integrated the optimization process proposed by Karimi et al. to generate counterfactuals, focusing on minimal interventions. \n\n## Optimization Problem\n\nThe goal is to find the minimal cost set of actions $A$ (in the form of structural interventions) that results in a counterfactual instance yielding a favorable output from the model. The optimization problem can be formulated as:\n\n$$\nA^* \\in \\arg\\min_A \\text{cost}(A; \\mathbf{x}_F)\n$$\n\nsubject to:\n\n\n\n$$\nh(\\mathbf{x}_{SCF}) \\neq h(\\mathbf{x}_F)\n$$\n\n## Lagrangian Formulation\n\nTo convert the constrained optimization problem into an unconstrained one, we utilize the Lagrangian:\n\n\\[\n\\mathcal{L}(A, \\lambda) = \\text{cost}(A; \\mathbf{x}_F) + \\lambda \\left(h(\\mathbf{x}_{SCF}) - h(\\mathbf{x}_F) \\right)\n\\]\n\nThis approach enables us to minimize the cost of interventions while ensuring the desired outcome change.\n\n# Conclusion\n\nIn conclusion, this project has successfully integrated causal reasoning into the CounterfactualExplanations.jl package, providing a valuable tool for generating counterfactual explanations that are not only interpretable but also actionable. This advancement enhances the ability of practitioners to make informed decisions based on machine learning models, ultimately fostering a more interpretable AI landscape.\n\n# References\n\n- Karimi, A., et al. \"Algorithmic Recourse via Minimal Interventions.\" \n- Upadhyay, S., et al. \"Towards Algorithmic Recourse: A Survey of Challenges and Opportunities.\" \n- Mahajan, D., et al. \"Preserving Causal Constraints in Counterfactual Explanations.\"\n- Altmeyer, P. \"ECCCo.jl: A tool for counterfactual explanations.\"\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": []
  }
}