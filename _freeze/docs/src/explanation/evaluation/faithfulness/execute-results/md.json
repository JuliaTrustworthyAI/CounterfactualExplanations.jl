{
  "hash": "b94354e906ca2cd44d9b92f00b7f420b",
  "result": {
    "engine": "jupyter",
    "markdown": "```@meta\nCurrentModule = CounterfactualExplanations \n```\n\n\n\n# Faithfulness and Plausibility\n\n\n```{=commonmark}\n!!! warning\n    The implementation of our faithfulness and plausibility metrics is based on our AAAI 2024 [paper](https://arxiv.org/abs/2312.10648). There is no consensus on the best way to measure faithfulness and plausibility and we are still conducting research on this. This tutorial is therefore also a work in progress. Current limitations are discussed below.\n```\n\n\nWe begin by loading some dependencies:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\n# Packages\nusing CounterfactualExplanations\nusing CounterfactualExplanations.Evaluation\nusing CounterfactualExplanations.Convergence\nusing CounterfactualExplanations.Models\nusing Flux\nusing JointEnergyModels\nusing MLJFlux\nusing EnergySamplers: PMC, SGLD, ImproperSGLD\nusing TaijaData\n```\n:::\n\n\n## Sample-Based Metrics\n\nIn @altmeyer2024faithful, we defined two sample-based metrics for plausibility and faithfulness. The metrics rely on the premise of comparing the counterfactual to samples drawn from some target distribution. To assess plausibility, we compare the counterfactual to samples drawn from the training data that fall into the target class. To assess faithfulness, we compare the counterfactual to samples drawn from the model posterior conditional through Stochastic Gradient Langevin Dynamics (SGLD). For details specific to posterior sampling, please consult our documentation Taija's [EnergySamplers.jl](https://juliatrustworthyai.github.io/EnergySamplers.jl/v1.0/). For broader details on this topic, please consult @altmeyer2024faithful. \n\n### Simple Example\n\nBelow we generate a simple synthetic dataset with two output classes, both Gaussian clusters with different centers. We then train a joint energy-based model (JEM) using Taija's [JointEnergyModels.jl](https://github.com/JuliaTrustworthyAI/JointEnergyModels.jl) package to both discriminate between output classes and generate inputs.\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nn_obs = 1000\nX, y = TaijaData.load_blobs(n_obs; cluster_std=0.1, center_box=(-1. => 1.))\ndata = CounterfactualData(X, y)\n\nn_hidden = 16\n_batch_size = Int(round(n_obs/10))\nepochs = 100\nM = Models.fit_model(\n    data,:JEM;\n    builder=MLJFlux.MLP(\n        hidden=(n_hidden, n_hidden, n_hidden), \n        σ=Flux.swish\n    ),\n    batch_size=_batch_size,\n    finaliser=Flux.softmax,\n    loss=Flux.Losses.crossentropy,\n    jem_training_params=(\n        α=[1.0,1.0,1e-1],\n        verbosity=10,\n    ),\n    epochs=epochs,\n    sampling_steps=30,\n)\n```\n:::\n\n\nNext, we generate counterfactuals for a randomly drawn sampler using two different generators: firstly, the [`GenericGenerator`](@ref) and, secondly, the [`ECCoGenerator`](@ref). The latter was proposed in @altmeyer2024faithful to generate faithful counterfactuals by constraining their energy with respect to the model. In both cases, we generate multiple counterfactuals for the same factual. Each time the search is initialized by adding a small random perturbation to the features following [@slack2021counterfactual]. For both generators, we then compute the average plausibility and faithfulness of the generated counterfactuals as defined above and plot the counterfactual paths in the figure below. The estimated values for the plausibility and faithfulness are shown in the plot titles and indicate that the [`ECCoGenerator`](@ref) performs better in both regards.\n\nTo better understand why the [`ECCoGenerator`](@ref) generates more faithful counterfactuals, we have also plotted samples drawn from the model posterior $p_{\\theta}(X|y=1)$ in green: these largely overlap with training data in the target distribution, which indicates that the JEM has succeeded on both tasks---discriminating and generating---for this simple data set. The energy constraint of the [`ECCoGenerator`](@ref) ensures that counterfactuals remain anchored by the learned model posterior conditional distribution. As demonstrated in @altmeyer2024faithful, faithful counterfactuals will also be plausible if the underlying model has learned plausible explanations for the data as in this case. For the [`GenericGenerator`](@ref), counterfactuals end up outside of that target distribution, because the distance penalty pulls counterfactuals back to their original starting values.\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing Measures\n\n# Select a factual instance:\ntarget = 1\nfactual = 2\nchosen = rand(findall(predict_label(M, data) .== factual))\nx = select_factual(data, chosen)\n\n# Search parameters:\nopt = Adam(0.005)\nconv = GeneratorConditionsConvergence()\n\n# Generic Generator:\nλ₁ = 0.1\ngenerator = GenericGenerator(opt=opt, λ=λ₁)\nce = generate_counterfactual(x, target, data, M, generator; convergence=conv, num_counterfactuals=5)\nfaith = Evaluation.faithfulness(ce)\nplaus = Evaluation.plausibility(ce)\np1 = plot(ce; zoom=-1, target=target)\nX̂ = ce.search[:energy_sampler][ce.target].posterior\ntitle = \"Generic Generator\\nplaus.: $(round(plaus, digits=2)); faith.: $(round(faith, digits=2))\"\nscatter!(X̂[1, :], X̂[2, :]; label=\"X|y=$target\", shape=:star5, ms=10, title=title, color=3, alpha=0.1)\nscatter!(ce.x′[1,:], ce.x′[2,:]; label=\"Counterfactual\", shape=:star1, ms=20, color=4)\n\n# Search:\nλ₂ = 1.0\ngenerator = ECCoGenerator(opt=opt; λ=[λ₁, λ₂])\nce = generate_counterfactual(x, target, data, M, generator; convergence=conv, num_counterfactuals=5)\nfaith = Evaluation.faithfulness(ce)\nplaus = Evaluation.plausibility(ce)\np2 = plot(ce; zoom=-1, target=target)\nX̂ = ce.search[:energy_sampler][ce.target].posterior\ntitle = \"ECCo Generator\\nplaus.: $(round(plaus, digits=2)); faith.: $(round(faith, digits=2))\"\nscatter!(X̂[1, :], X̂[2, :]; label=\"X|y=$target\", shape=:star5, ms=10, title=title, color=3, alpha=0.1)\nscatter!(ce.x′[1,:], ce.x′[2,:]; label=\"Counterfactual\", shape=:star1, ms=20, color=4)\n\nplot(p1, p2; size=(1000, 400), topmargin=5mm)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](faithfulness_files/figure-commonmark/cell-5-output-1.svg){}\n:::\n:::\n\n\n### Current Limitations\n\nBut things do not always turn out this well. Our next example demonstrates an important shortcoming of the framework proposed in @altmeyer2024faithful. Instead of training a JEM, we now train a simpler, purely discriminative model:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nn_obs = 1000\nX, y = TaijaData.load_blobs(n_obs; cluster_std=0.1, center_box=(-1. => 1.))\ndata = CounterfactualData(X, y)\nflux_training_params.n_epochs = 1\nM = Models.fit_model(data,:DeepEnsemble)\nCounterfactualExplanations.reset!(flux_training_params)\n```\n:::\n\n\nNext, we repeat the same process above for generating counterfactuals. This time we can observe in the figure below that the [`GenericGenerator`](@ref) produces much more plausible though apparently less faithful counterfactuals than the [`ECCoGenerator`](@ref). Looking at the top row only, it is not obvious why the counterfactual produced by the [`GenericGenerator`](@ref) should be considered as less faithful to the model: conditional samples drawn from $p_{\\theta}(X|y=1)$ through SGLD are just scattered all across the target domain on the expected side of the decision boundary. When zooming out (bottom row), it becomes clear that the learned posterior conditional is far away from the observed training data in the target class. Our definition and measure of faithfulness is in that sense very strict, quite possibly too strict in some cases. \n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\n# Select a factual instance:\ntarget = 2\nfactual = 1\nchosen = rand(findall(predict_label(M, data) .== factual))\nx = select_factual(data, chosen)\n\n# Search parameters:\nopt = Adam(0.1)\nconv = GeneratorConditionsConvergence()\n\n# Generic Generator:\ngenerator = GenericGenerator(opt=opt)\nce = generate_counterfactual(x, target, data, M, generator; convergence=conv, initialization=:identity)\nplaus = Evaluation.plausibility(ce)\nfaith = Evaluation.faithfulness(ce)\nX̂ = ce.search[:energy_sampler][ce.target].posterior\ntitle = \"Generic Generator\\nplaus.: $(round(plaus, digits=2)); faith.: $(round(faith, digits=2))\"\np1 = plot(ce, zoom=-1, target=target)\nscatter!(X̂[1, :], X̂[2, :]; label=\"X|y=$target\", shape=:star5, ms=10, title=title, color=3, alpha=0.2)\nscatter!(ce.x′[1,:], ce.x′[2,:]; label=\"Counterfactual\", shape=:star1, ms=20, color=4)\n_lim = maximum(abs.(X̂))\nxlims, ylims = (-_lim, _lim), (-_lim, _lim)\np3 = plot(ce; xlims=xlims, ylims=ylims, target=target)\nscatter!(X̂[1, :], X̂[2, :]; label=\"X|y=$target\", shape=:star5, ms=10, title=title, color=3, alpha=0.2)\nscatter!(ce.x′[1,:], ce.x′[2,:]; label=\"Counterfactual\", shape=:star1, ms=20, color=4)\n\n# Search:\ngenerator = ECCoGenerator(opt=opt; λ=[0.1, 1.0])\nce = generate_counterfactual(x, target, data, M, generator; convergence=conv, initialization=:identity)\nplaus = Evaluation.plausibility(ce)\nfaith = Evaluation.faithfulness(ce)\nX̂ = ce.search[:energy_sampler][ce.target].posterior\ntitle = \"ECCo Generator\\nplaus.: $(round(plaus, digits=2)); faith.: $(round(faith, digits=2))\"\np2 = plot(ce, zoom=-1, target=target)\nscatter!(X̂[1, :], X̂[2, :]; label=\"X|y=$target\", shape=:star5, ms=10, title=title, color=3, alpha=0.2)\nscatter!(ce.x′[1,:], ce.x′[2,:]; label=\"Counterfactual\", shape=:star1, ms=20, color=4)\n_lim = maximum(abs.(X̂))\nxlims, ylims = (-_lim, _lim), (-_lim, _lim)\np4 = plot(ce; xlims=xlims, ylims=ylims, target=target)\nscatter!(X̂[1, :], X̂[2, :]; label=\"X|y=$target\", shape=:star5, ms=10, title=title, color=3, alpha=0.2)\nscatter!(ce.x′[1,:], ce.x′[2,:]; label=\"Counterfactual\", shape=:star1, ms=20, color=4)\n\nplot(p1, p2, p3, p4; size=(1000, 800), topmargin=5mm)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n![](faithfulness_files/figure-commonmark/cell-7-output-1.svg){}\n:::\n:::\n\n\nLooking at a different domain like images demonstrates another limitation of the sample-based metrics. Below we generate counterfactuals for turning an 8 into a 3 using our two generators from above for a simple MNIST [@lecun1998mnist] classifier. Looking at the figure below, arguably the [`ECCoGenerator`](@ref) generates a more plausible counterfactual in this case. Unfortunately, according to the sample-based plausibility metric, this is not the case. \n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\n_nrow = 3\n\nRandom.seed!(42)\nX, y = TaijaData.load_mnist()\ndata = CounterfactualData(X, y)\n\nusing CounterfactualExplanations.Models: load_mnist_model\nusing CounterfactualExplanations: JEM\nM = load_mnist_model(MLP())\n\n# Select a factual instance:\ntarget = 3\nfactual = 8\nchosen = rand(findall(predict_label(M, data) .== factual))\nx = select_factual(data, chosen)\n\n# Search parameters:\nopt = Adam(0.1)\nconv = GeneratorConditionsConvergence()\nλ₁ = 0.0\nλ₂ = 0.5\n\n# Factual:\nfactual = convert2image(MNIST, reshape(x, 28, 28))\np1 = plot(factual; title=\"\\nFactual\", axis=([], false))\n\n# Generic Generator:\ngenerator = GenericGenerator(opt=opt; λ=λ₁)\nce = generate_counterfactual(x, target, data, M, generator; convergence=conv, initialization=:identity)\nfaith = Evaluation.faithfulness(ce; nsamples=_nrow^2, niter_final=10000)\nplaus = Evaluation.plausibility(ce)\nimg = convert2image(MNIST, reshape(ce.x′, 28, 28))\ntitle = \"Generic Generator\\nplaus.: $(round(plaus, digits=2))\\nfaith.: $(round(faith, digits=2))\"\np2 = plot(img, title=title, axis=([], false))\n\n# Search:\ngenerator = ECCoGenerator(opt=opt; λ=[λ₁, λ₂])\nce = generate_counterfactual(x, target, data, M, generator; convergence=conv, initialization=:identity)\nfaith = Evaluation.faithfulness(ce; nsamples=_nrow^2, niter_final=10000)\nplaus = Evaluation.plausibility(ce)\nimg = convert2image(MNIST, reshape(ce.x′, 28, 28))\ntitle = \"ECCo Generator\\nplaus.: $(round(plaus, digits=2))\\nfaith.: $(round(faith, digits=2))\"\np3 = plot(img, title=title, axis=([], false))\n\nplot(p1, p2, p3; size=(600, 200), layout=(1, 3), topmargin=15mm)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n![](faithfulness_files/figure-commonmark/cell-8-output-1.svg){}\n:::\n:::\n\n\n## References\n\n",
    "supporting": [
      "faithfulness_files"
    ],
    "filters": []
  }
}