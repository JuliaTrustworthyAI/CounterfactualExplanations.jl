{
  "hash": "dea0ec38604de599c232e77d8e7a2a9f",
  "result": {
    "engine": "jupyter",
    "markdown": "```@meta\nCurrentModule = CounterfactualExplanations \n```\n\n\n\n# How to add Custom Generators\n\nAs we will see in this short tutorial, building custom counterfactual generators is straightforward. We hope that this will facilitate contributions through the community.\n\n## Generic generator with dropout\n\nTo illustrate how custom generators can be implemented we will consider a simple example of a generator that extends the functionality of our `GenericGenerator`. We have noted elsewhere that the effectiveness of counterfactual explanations depends to some degree on the quality of the fitted model. Another, perhaps trivial, thing to note is that counterfactual explanations are not unique: there are potentially many valid counterfactual paths. One interesting (or silly) idea following these two observations might be to introduce some form of regularization in the counterfactual search. For example, we could use dropout to randomly switch features on and off in each iteration. Without dwelling further on the usefulness of this idea, let us see how it can be implemented.\n\nThe first code chunk below implements two important steps: 1) create an abstract subtype of the `AbstractGradientBasedGenerator` and 2) create a constructor similar to the `GenericConstructor`, but with one additional field for the probability of dropout.\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\n# Abstract suptype:\nabstract type AbstractDropoutGenerator <: AbstractGradientBasedGenerator end\n\n# Constructor:\nstruct DropoutGenerator <: AbstractDropoutGenerator\n    loss::Function # loss function\n    penalty::Function\n    λ::AbstractFloat # strength of penalty\n    latent_space::Bool\n    opt::Any # optimizer\n    generative_model_params::NamedTuple\n    p_dropout::AbstractFloat # dropout rate\nend\n\n# Instantiate:\ngenerator = DropoutGenerator(\n    Flux.logitbinarycrossentropy,\n    CounterfactualExplanations.Objectives.distance_l1,\n    0.1,\n    false,\n    Flux.Optimise.Descent(0.1),\n    (;),\n    0.5,\n)\n```\n:::\n\n\nNext, we define how feature perturbations are generated for our dropout generator: in particular, we extend the relevant function through a method that implemented the dropout logic.\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nusing CounterfactualExplanations.Generators\nusing StatsBase\nfunction Generators.generate_perturbations(\n    generator::AbstractDropoutGenerator, \n    ce::CounterfactualExplanation\n)\n    s′ = deepcopy(ce.s′)\n    new_s′ = Generators.propose_state(generator, ce)\n    Δs′ = new_s′ - s′ # gradient step\n\n    # Dropout:\n    set_to_zero = sample(\n        1:length(Δs′),\n        Int(round(generator.p_dropout*length(Δs′))),\n        replace=false\n    )\n    Δs′[set_to_zero] .= 0\n    return Δs′\nend\n```\n:::\n\n\nFinally, we proceed to generate counterfactuals in the same way we always do:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\n# Data and Classifier:\nM = fit_model(counterfactual_data, :DeepEnsemble)\n\n# Factual and Target:\nyhat = predict_label(M, counterfactual_data)\ntarget = 2    # target label\ncandidates = findall(vec(yhat) .!= target)\nchosen = rand(candidates)\nx = select_factual(counterfactual_data, chosen)\n\n# Counterfactual search:\nce = generate_counterfactual(\n    x, target, counterfactual_data, M, generator;\n    num_counterfactuals=5)\n\nplot(ce)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](custom_generators_files/figure-commonmark/cell-5-output-1.svg){}\n:::\n:::\n\n\n",
    "supporting": [
      "custom_generators_files"
    ],
    "filters": []
  }
}