{
  "hash": "58fa13394376d9991181ef8897f84df0",
  "result": {
    "engine": "jupyter",
    "markdown": "```@meta\nCurrentModule = CounterfactualExplanations \n```\n\n\n\n# How to add Custom Models\n\nAdding custom models is possible and relatively straightforward, as we will demonstrate in this guide.\n\n## Custom Models\n\nApart from the default models you can use any arbitrary (differentiable) model and generate recourse in the same way as before. Only two steps are necessary to make your own Julia model compatible with this package:\n\n1. The model needs to be declared as a subtype of `<:CounterfactualExplanations.Models.AbstractFittedModel`.\n2. You need to extend the functions `CounterfactualExplanations.Models.logits` and `CounterfactualExplanations.Models.probs` for your custom model.\n\n### How `FluxModel` was added\n\nTo demonstrate how this can be done in practice, we will reiterate here how native support for [`Flux.jl`](https://fluxml.ai/) models was enabled [@innes2018flux]. Once again we use synthetic data for an illustrative example. The code below loads the data and builds a simple model architecture that can be used for a multi-class prediction task. Note how outputs from the final layer are not passed through a softmax activation function, since the counterfactual loss is evaluated with respect to logits. The model is trained with dropout.\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\n# Data:\nN = 200\ndata = TaijaData.load_blobs(N; centers=4, cluster_std=0.5)\ncounterfactual_data = DataPreprocessing.CounterfactualData(data...)\ny = counterfactual_data.y\nX = counterfactual_data.X\n\n# Flux model setup: \nusing Flux\ndata = Flux.DataLoader((X,y), batchsize=1)\nn_hidden = 32\noutput_dim = size(y,1)\ninput_dim = 2\nactivation = Ïƒ\nmodel = Chain(\n    Dense(input_dim, n_hidden, activation),\n    Dropout(0.1),\n    Dense(n_hidden, output_dim)\n)  \nloss(x, y) = Flux.Losses.logitcrossentropy(model(x), y)\n\n# Flux model training:\nusing Flux.Optimise: update!, Adam\nopt = Adam()\nepochs = 50\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(Flux.params(model)) do\n      l = loss(d...)\n    end\n    update!(opt, Flux.params(model), gs)\n  end\nend\n```\n:::\n\n\nThe code below implements the two steps that were necessary to make Flux models compatible with the package. We first declare our new struct as a subtype of `<:AbstractDifferentiableModel`, which itself is an abstract subtype of `<:AbstractFittedModel`. Computing logits amounts to just calling the model on inputs. Predicted probabilities for labels can in this case be computed by passing predicted logits through the softmax function. Finally, we just instantiate our model in the same way as always.\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\n# Step 1)\nstruct MyFluxModel <: AbstractDifferentiableModel\n    model::Any\n    likelihood::Symbol\nend\n\n# Step 2)\n# import functions in order to extend\nimport CounterfactualExplanations.Models: logits\nimport CounterfactualExplanations.Models: probs \nlogits(M::MyFluxModel, X::AbstractArray) = M.model(X)\nprobs(M::MyFluxModel, X::AbstractArray) = softmax(logits(M, X))\nM = MyFluxModel(model, :classification_multi)\n```\n:::\n\n\nThe code below implements the counterfactual search and plots the results:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nfactual_label = 4\ntarget = 2\nchosen = rand(findall(predict_label(M, counterfactual_data) .== factual_label))\nx = select_factual(counterfactual_data, chosen)  \n\n# Counterfactual search:\ngenerator = GenericGenerator()\nce = generate_counterfactual(x, target, counterfactual_data, M, generator)\nplot(ce)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](custom_models_files/figure-commonmark/cell-5-output-1.svg){}\n:::\n:::\n\n\n## References\n\n",
    "supporting": [
      "custom_models_files"
    ],
    "filters": []
  }
}