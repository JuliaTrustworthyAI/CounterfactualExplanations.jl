<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>🏠 Home · CounterfactualExplanations.jl</title><meta name="title" content="🏠 Home · CounterfactualExplanations.jl"/><meta property="og:title" content="🏠 Home · CounterfactualExplanations.jl"/><meta property="twitter:title" content="🏠 Home · CounterfactualExplanations.jl"/><meta name="description" content="Documentation for CounterfactualExplanations.jl."/><meta property="og:description" content="Documentation for CounterfactualExplanations.jl."/><meta property="twitter:description" content="Documentation for CounterfactualExplanations.jl."/><meta property="og:url" content="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/"/><meta property="twitter:url" content="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/"/><link rel="canonical" href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img src="assets/logo.svg" alt="CounterfactualExplanations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href>CounterfactualExplanations.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>🏠 Home</a><ul class="internal"><li><a class="tocitem" href="#Installation"><span>🚩 Installation</span></a></li><li><a class="tocitem" href="#Background-and-Motivation"><span>🤔 Background and Motivation</span></a></li><li><a class="tocitem" href="#Enter:-Counterfactual-Explanations"><span>🔮 Enter: Counterfactual Explanations</span></a></li><li><a class="tocitem" href="#Usage-example"><span>🔍 Usage example</span></a></li><li><a class="tocitem" href="#Implemented-Counterfactual-Generators"><span>☑️ Implemented Counterfactual Generators</span></a></li><li><a class="tocitem" href="#Goals-and-limitations"><span>🎯 Goals and limitations</span></a></li><li><a class="tocitem" href="#Contribute"><span>🛠 Contribute</span></a></li><li><a class="tocitem" href="#Citation"><span>🎓 Citation</span></a></li><li><a class="tocitem" href="#References"><span>📚 References</span></a></li></ul></li><li><span class="tocitem">🫣 Tutorials</span><ul><li><a class="tocitem" href="tutorials/">Overview</a></li><li><a class="tocitem" href="tutorials/simple_example/">Simple Example</a></li><li><a class="tocitem" href="tutorials/whistle_stop/">Whiste-Stop Tour</a></li><li><a class="tocitem" href="tutorials/data_preprocessing/">Handling Data</a></li><li><a class="tocitem" href="tutorials/data_catalogue/">Data Catalogue</a></li><li><a class="tocitem" href="tutorials/models/">Handling Models</a></li><li><a class="tocitem" href="tutorials/model_catalogue/">Model Catalogue</a></li><li><a class="tocitem" href="tutorials/generators/">Handling Generators</a></li><li><a class="tocitem" href="tutorials/evaluation/">Evaluating Explanations</a></li><li><a class="tocitem" href="tutorials/benchmarking/">Benchmarking Explanations</a></li><li><a class="tocitem" href="tutorials/parallelization/">Parallelization</a></li><li><a class="tocitem" href="tutorials/convergence/">Convergence</a></li></ul></li><li><span class="tocitem">🤓 Explanation</span><ul><li><a class="tocitem" href="explanation/">Overview</a></li><li><a class="tocitem" href="explanation/architecture/">Package Architecture</a></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Generators</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="explanation/generators/overview/">Overview</a></li><li><a class="tocitem" href="explanation/generators/generic/">Generic</a></li><li><a class="tocitem" href="explanation/generators/clap_roar/">ClaPROAR</a></li><li><a class="tocitem" href="explanation/generators/clue/">CLUE</a></li><li><a class="tocitem" href="explanation/generators/dice/">DiCE</a></li><li><a class="tocitem" href="explanation/generators/feature_tweak/">FeatureTweak</a></li><li><a class="tocitem" href="explanation/generators/gravitational/">Gravitational</a></li><li><a class="tocitem" href="explanation/generators/greedy/">Greedy</a></li><li><a class="tocitem" href="explanation/generators/growing_spheres/">GrowingSpheres</a></li><li><a class="tocitem" href="explanation/generators/probe/">PROBE</a></li><li><a class="tocitem" href="explanation/generators/revise/">REVISE</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-4" type="checkbox"/><label class="tocitem" for="menuitem-3-4"><span class="docs-label">Optimisers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="explanation/optimisers/overview/">Overview</a></li><li><a class="tocitem" href="explanation/optimisers/jsma/">JSMA</a></li></ul></li><li><a class="tocitem" href="explanation/categorical/">Categorical Features</a></li></ul></li><li><span class="tocitem">🫡 How-To ...</span><ul><li><a class="tocitem" href="how_to_guides/">Overview</a></li><li><a class="tocitem" href="how_to_guides/custom_generators/">... add custom generators</a></li><li><a class="tocitem" href="how_to_guides/custom_models/">... add custom models</a></li></ul></li><li><span class="tocitem">⛓️ Extensions</span><ul><li><a class="tocitem" href="extensions/">Overview</a></li><li><a class="tocitem" href="extensions/neurotree/">NeuroTrees</a></li><li><a class="tocitem" href="extensions/laplace_redux/">LaplaceRedux</a></li></ul></li><li><a class="tocitem" href="reference/">🧐 Reference</a></li><li><a class="tocitem" href="contribute/">🛠 Contribute</a></li><li><a class="tocitem" href="assets/resources/">📚 Additional Resources</a></li><li><a class="tocitem" href="release-notes/">Release Notes</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>🏠 Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>🏠 Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><p><img src="assets/wide_logo.png" alt/></p><p>Documentation for <a href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a>.</p><h1 id="CounterfactualExplanations"><a class="docs-heading-anchor" href="#CounterfactualExplanations">CounterfactualExplanations</a><a id="CounterfactualExplanations-1"></a><a class="docs-heading-anchor-permalink" href="#CounterfactualExplanations" title="Permalink"></a></h1><p><em>Counterfactual Explanations and Algorithmic Recourse in Julia.</em></p><p><a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable"><img src="https://img.shields.io/badge/docs-stable-blue.svg" alt="Stable"/></a> <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/dev"><img src="https://img.shields.io/badge/docs-dev-blue.svg" alt="Dev"/></a> <a href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/actions/workflows/CI.yml?query=branch%3Amain"><img src="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/actions/workflows/CI.yml/badge.svg?branch=main" alt="Build Status"/></a> <a href="https://codecov.io/gh/juliatrustworthyai/CounterfactualExplanations.jl"><img src="https://codecov.io/gh/juliatrustworthyai/CounterfactualExplanations.jl/branch/main/graph/badge.svg" alt="Coverage"/></a> <a href="https://github.com/invenia/BlueStyle"><img src="https://img.shields.io/badge/code%20style-blue-4495d1.svg" alt="Code Style: Blue"/></a> <a href="LICENSE"><img src="https://img.shields.io/github/license/juliatrustworthyai/CounterfactualExplanations.jl" alt="License"/></a> <a href="http://juliapkgstats.com/pkg/CounterfactualExplanations"><img src="https://img.shields.io/badge/dynamic/json?url=http%3A%2F%2Fjuliapkgstats.com%2Fapi%2Fv1%2Fmonthly_downloads%2FCounterfactualExplanations&amp;query=total_requests&amp;suffix=%2Fmonth&amp;label=Downloads" alt="Package Downloads"/></a> <a href="https://github.com/JuliaTesting/Aqua.jl"><img src="https://raw.githubusercontent.com/JuliaTesting/Aqua.jl/master/badge.svg" alt="Aqua QA"/></a></p><p><code>CounterfactualExplanations.jl</code> is a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box algorithms. Both CE and AR are related tools for explainable artificial intelligence (XAI). While the package is written purely in Julia, it can be used to explain machine learning algorithms developed and trained in other popular programming languages like Python and R. See below for a short introduction and other resources or dive straight into the <a href="https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/dev">docs</a>.</p><p>There is also a corresponding paper, <a href="https://proceedings.juliacon.org/papers/10.21105/jcon.00130"><em>Explaining Black-Box Models through Counterfactuals</em></a>, which has been published in JuliaCon Proceedings. Please consider citing the paper, if you use this package in your work:</p><p><a href="https://doi.org/10.21105/jcon.00130"><img src="https://proceedings.juliacon.org/papers/10.21105/jcon.00130/status.svg" alt="DOI"/></a> <a href="https://zenodo.org/badge/latestdoi/440782065"><img src="https://zenodo.org/badge/440782065.svg" alt="DOI"/></a></p><pre><code class="nohighlight hljs">@article{Altmeyer2023,
  doi = {10.21105/jcon.00130},
  url = {https://doi.org/10.21105/jcon.00130},
  year = {2023},
  publisher = {The Open Journal},
  volume = {1},
  number = {1},
  pages = {130},
  author = {Patrick Altmeyer and Arie van Deursen and Cynthia C. s. Liem},
  title = {Explaining Black-Box Models through Counterfactuals},
  journal = {Proceedings of the JuliaCon Conferences}
}</code></pre><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">🚩 Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>You can install the stable release from <a href="https://github.com/JuliaRegistries/General">Julia’s General Registry</a> as follows:</p><pre><code class="language-julia hljs">using Pkg
Pkg.add(&quot;CounterfactualExplanations&quot;)</code></pre><p><code>CounterfactualExplanations.jl</code> is under active development. To install the development version of the package you can run the following command:</p><pre><code class="language-julia hljs">using Pkg
Pkg.add(url=&quot;https://github.com/juliatrustworthyai/CounterfactualExplanations.jl&quot;)</code></pre><h2 id="Background-and-Motivation"><a class="docs-heading-anchor" href="#Background-and-Motivation">🤔 Background and Motivation</a><a id="Background-and-Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Background-and-Motivation" title="Permalink"></a></h2><p>Machine learning models like Deep Neural Networks have become so complex, opaque and underspecified in the data that they are generally considered Black Boxes. Nonetheless, such models often play a key role in data-driven decision-making systems. This creates the following problem: human operators in charge of such systems have to rely on them blindly, while those individuals subject to them generally have no way of challenging an undesirable outcome:</p><blockquote><p>“You cannot appeal to (algorithms). They do not listen. Nor do they bend.”</p><p>— Cathy O’Neil in <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction"><em>Weapons of Math Destruction</em></a>, 2016</p></blockquote><h2 id="Enter:-Counterfactual-Explanations"><a class="docs-heading-anchor" href="#Enter:-Counterfactual-Explanations">🔮 Enter: Counterfactual Explanations</a><a id="Enter:-Counterfactual-Explanations-1"></a><a class="docs-heading-anchor-permalink" href="#Enter:-Counterfactual-Explanations" title="Permalink"></a></h2><p>Counterfactual Explanations can help human stakeholders make sense of the systems they develop, use or endure: they explain how inputs into a system need to change for it to produce different decisions. Explainability benefits internal as well as external quality assurance.</p><p>Counterfactual Explanations have a few properties that are desirable in the context of Explainable Artificial Intelligence (XAI). These include:</p><ul><li>Full fidelity to the black-box model, since no proxy is involved.</li><li>No need for (reasonably) interpretable features as opposed to LIME and SHAP.</li><li>Clear link to Algorithmic Recourse and Causal Inference.</li><li>Less susceptible to adversarial attacks than LIME and SHAP.</li></ul><h3 id="Example:-Give-Me-Some-Credit"><a class="docs-heading-anchor" href="#Example:-Give-Me-Some-Credit">Example: Give Me Some Credit</a><a id="Example:-Give-Me-Some-Credit-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Give-Me-Some-Credit" title="Permalink"></a></h3><p>Consider the following real-world scenario: a retail bank is using a black-box model trained on their clients’ credit history to decide whether they will provide credit to new applicants. To simulate this scenario, we have pre-trained a binary classifier on the publicly available Give Me Some Credit dataset that ships with this package (Kaggle 2011).</p><p>The figure below shows counterfactuals for 10 randomly chosen individuals that would have been denied credit initially.</p><p><img src="index_files/figure-commonmark/cell-5-output-1.svg" alt/></p><h3 id="Example:-MNIST"><a class="docs-heading-anchor" href="#Example:-MNIST">Example: MNIST</a><a id="Example:-MNIST-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-MNIST" title="Permalink"></a></h3><p>The figure below shows a counterfactual generated for an image classifier trained on MNIST: in particular, it demonstrates which pixels need to change in order for the classifier to predict 3 instead of 8.</p><p>Since <code>v0.1.9</code> counterfactual generators are fully composable. Here we have composed a generator that combines ideas from Wachter, Mittelstadt, and Russell (2017) and Altmeyer et al. (2023):</p><pre><code class="language-julia hljs"># Compose generator:
using CounterfactualExplanations.Objectives: distance_mad, distance_from_target
generator = GradientBasedGenerator()
@chain generator begin
    @objective logitcrossentropy + 0.2distance_mad + 0.1distance_from_target
    @with_optimiser Adam(0.1)                  
end</code></pre><p><img src="index_files/figure-commonmark/cell-10-output-1.svg" alt/></p><h2 id="Usage-example"><a class="docs-heading-anchor" href="#Usage-example">🔍 Usage example</a><a id="Usage-example-1"></a><a class="docs-heading-anchor-permalink" href="#Usage-example" title="Permalink"></a></h2><p>Generating counterfactuals will typically look like follows. Below we first fit a simple model to a synthetic dataset with linearly separable features and then draw a random sample:</p><pre><code class="language-julia hljs"># Data and Classifier:
counterfactual_data = CounterfactualData(load_linearly_separable()...)
M = fit_model(counterfactual_data, :Linear)

# Select random sample:
target = 2
factual = 1
chosen = rand(findall(predict_label(M, counterfactual_data) .== factual))
x = select_factual(counterfactual_data, chosen)</code></pre><p>To this end, we specify a counterfactual generator of our choice:</p><pre><code class="language-julia hljs"># Counterfactual search:
generator = DiCEGenerator(λ=[0.1,0.3])</code></pre><p>Here, we have chosen to use the <code>GradientBasedGenerator</code> to move the individual from its factual label 1 to the target label 2.</p><p>With all of our ingredients specified, we finally generate counterfactuals using a simple API call:</p><pre><code class="language-julia hljs">conv = conv = CounterfactualExplanations.Convergence.GeneratorConditionsConvergence()
ce = generate_counterfactual(
  x, target, counterfactual_data, M, generator; 
  num_counterfactuals=3, convergence=conv,
)</code></pre><p>The plot below shows the resulting counterfactual path:</p><p><img src="index_files/figure-commonmark/cell-15-output-1.svg" alt/></p><h2 id="Implemented-Counterfactual-Generators"><a class="docs-heading-anchor" href="#Implemented-Counterfactual-Generators">☑️ Implemented Counterfactual Generators</a><a id="Implemented-Counterfactual-Generators-1"></a><a class="docs-heading-anchor-permalink" href="#Implemented-Counterfactual-Generators" title="Permalink"></a></h2><p>Currently, the following counterfactual generators are implemented:</p><ul><li>ClaPROAR (Altmeyer et al. 2023)</li><li>CLUE (Antorán et al. 2020)</li><li>DiCE (Mothilal, Sharma, and Tan 2020)</li><li>FeatureTweak (Tolomei et al. 2017)</li><li>Generic</li><li>GravitationalGenerator (Altmeyer et al. 2023)</li><li>Greedy (Schut et al. 2021)</li><li>GrowingSpheres (Laugel et al. 2017)</li><li>PROBE (Pawelczyk et al. 2023)</li><li>REVISE (Joshi et al. 2019)</li><li>Wachter (Wachter, Mittelstadt, and Russell 2017)</li></ul><h2 id="Goals-and-limitations"><a class="docs-heading-anchor" href="#Goals-and-limitations">🎯 Goals and limitations</a><a id="Goals-and-limitations-1"></a><a class="docs-heading-anchor-permalink" href="#Goals-and-limitations" title="Permalink"></a></h2><p>The goal of this library is to contribute to efforts towards trustworthy machine learning in Julia. The Julia language has an edge when it comes to trustworthiness: it is very transparent. Packages like this one are generally written in pure Julia, which makes it easy for users and developers to understand and contribute to open-source code. Eventually, this project aims to offer a one-stop-shop of counterfactual explanations.</p><p>Our ambition is to enhance the package through the following features:</p><ol><li>Support for all supervised machine learning models trained in <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/"><code>MLJ.jl</code></a>.</li><li>Support for regression models.</li></ol><h2 id="Contribute"><a class="docs-heading-anchor" href="#Contribute">🛠 Contribute</a><a id="Contribute-1"></a><a class="docs-heading-anchor-permalink" href="#Contribute" title="Permalink"></a></h2><p>Contributions of any kind are very much welcome! Take a look at the <a href="https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/issues">issue</a> to see what things we are currently working on. If you have an idea for a new feature or want to report a bug, please open a new issue.</p><h3 id="Development"><a class="docs-heading-anchor" href="#Development">Development</a><a id="Development-1"></a><a class="docs-heading-anchor-permalink" href="#Development" title="Permalink"></a></h3><p>If your looking to contribute code, it may be helpful to check out the <a href="explanation/index.qmd">Explanation</a> section of the docs.</p><h4 id="Testing"><a class="docs-heading-anchor" href="#Testing">Testing</a><a id="Testing-1"></a><a class="docs-heading-anchor-permalink" href="#Testing" title="Permalink"></a></h4><p>Please always make sure to add tests for any new features or changes.</p><h4 id="Documentation"><a class="docs-heading-anchor" href="#Documentation">Documentation</a><a id="Documentation-1"></a><a class="docs-heading-anchor-permalink" href="#Documentation" title="Permalink"></a></h4><p>If you add new features or change existing ones, please make sure to update the documentation accordingly. The documentation is written in <a href="https://juliadocs.github.io/Documenter.jl/stable/">Documenter.jl</a> and is located in the <code>docs/src</code> folder.</p><h4 id="Log-Changes"><a class="docs-heading-anchor" href="#Log-Changes">Log Changes</a><a id="Log-Changes-1"></a><a class="docs-heading-anchor-permalink" href="#Log-Changes" title="Permalink"></a></h4><p>As of version <code>1.1.1</code>, we have tried to be more stringent about logging changes. Please make sure to add a note to the <a href="CHANGELOG/">CHANGELOG.md</a> file for any changes you make. It is sufficient to add a note under the <code>Unreleased</code> section.</p><h3 id="General-Pointers"><a class="docs-heading-anchor" href="#General-Pointers">General Pointers</a><a id="General-Pointers-1"></a><a class="docs-heading-anchor-permalink" href="#General-Pointers" title="Permalink"></a></h3><p>There are also some general pointers for people looking to contribute to any of our Taija packages <a href="https://github.com/JuliaTrustworthyAI#general-pointers-for-contributors">here</a>.</p><p>Please follow the <a href="https://github.com/SciML/ColPrac">SciML ColPrac guide</a>.</p><h2 id="Citation"><a class="docs-heading-anchor" href="#Citation">🎓 Citation</a><a id="Citation-1"></a><a class="docs-heading-anchor-permalink" href="#Citation" title="Permalink"></a></h2><p>If you want to use this codebase, please consider citing the corresponding paper:</p><pre><code class="nohighlight hljs">@article{Altmeyer2023,
  doi = {10.21105/jcon.00130},
  url = {https://doi.org/10.21105/jcon.00130},
  year = {2023},
  publisher = {The Open Journal},
  volume = {1},
  number = {1},
  pages = {130},
  author = {Patrick Altmeyer and Arie van Deursen and Cynthia C. s. Liem},
  title = {Explaining Black-Box Models through Counterfactuals},
  journal = {Proceedings of the JuliaCon Conferences}
}</code></pre><h2 id="References"><a class="docs-heading-anchor" href="#References">📚 References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><p>Altmeyer, Patrick, Giovan Angela, Aleksander Buszydlik, Karol Dobiczek, Arie van Deursen, and Cynthia CS Liem. 2023. “Endogenous Macrodynamics in Algorithmic Recourse.” In <em>2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)</em>, 418–31. IEEE.</p><p>Antorán, Javier, Umang Bhatt, Tameem Adel, Adrian Weller, and José Miguel Hernández-Lobato. 2020. “Getting a Clue: A Method for Explaining Uncertainty Estimates.” <a href="https://arxiv.org/abs/2006.06848">https://arxiv.org/abs/2006.06848</a>.</p><p>Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. “Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.” <a href="https://arxiv.org/abs/1907.09615">https://arxiv.org/abs/1907.09615</a>.</p><p>Kaggle. 2011. “Give Me Some Credit, Improve on the State of the Art in Credit Scoring by Predicting the Probability That Somebody Will Experience Financial Distress in the Next Two Years.” https://www.kaggle.com/c/GiveMeSomeCredit; Kaggle. <a href="https://www.kaggle.com/c/GiveMeSomeCredit">https://www.kaggle.com/c/GiveMeSomeCredit</a>.</p><p>Laugel, Thibault, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. 2017. “Inverse Classification for Comparison-Based Interpretability in Machine Learning.” <a href="https://arxiv.org/abs/1712.08443">https://arxiv.org/abs/1712.08443</a>.</p><p>Mothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. “Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.” In <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 607–17. <a href="https://doi.org/10.1145/3351095.3372850">https://doi.org/10.1145/3351095.3372850</a>.</p><p>Pawelczyk, Martin, Teresa Datta, Johannes van-den-Heuvel, Gjergji Kasneci, and Himabindu Lakkaraju. 2023. “Probabilistically Robust Recourse: Navigating the Trade-Offs Between Costs and Robustness in Algorithmic Recourse.” <a href="https://arxiv.org/abs/2203.06768">https://arxiv.org/abs/2203.06768</a>.</p><p>Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. “Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.” In <em>International Conference on Artificial Intelligence and Statistics</em>, 1756–64. PMLR.</p><p>Tolomei, Gabriele, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas. 2017. “Interpretable Predictions of Tree-Based Ensembles via Actionable Feature Tweaking.” In <em>Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 465–74. <a href="https://doi.org/10.1145/3097983.3098039">https://doi.org/10.1145/3097983.3098039</a>.</p><p>Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” <em>Harv. JL &amp; Tech.</em> 31: 841. <a href="https://doi.org/10.2139/ssrn.3063289">https://doi.org/10.2139/ssrn.3063289</a>.</p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="tutorials/">Overview »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Sunday 19 May 2024 07:17">Sunday 19 May 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
